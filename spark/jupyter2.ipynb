{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5442d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9f63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all Required Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "import warnings\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import Window\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ab9c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/10 12:01:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Setting Up the Configurations for pySpark\n",
    "conf = (pyspark.SparkConf().set('spark.driver.memory', '1G')\\\n",
    "        .set('spark.executor.memory', '1G')\\\n",
    "        .set('spark.driver.maxResultSize', '1G')\\\n",
    "        .set('spark.dynamicAllocation.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocaton.maxExecutors', '4')\\\n",
    "        .set('spark.master','local[4]'))\\\n",
    "\n",
    "# Fetching or Creating a Session for this Activity\n",
    "spark = SparkSession.builder.config(conf = conf).appName(\"pysparklearning\").enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a5f09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.129.130.96:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pysparklearning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc80d7b7580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e02603",
   "metadata": {},
   "source": [
    "Table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09631407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station = spark.read.format(\"csv\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/sql/stationtable.csv\")\n",
    "city = spark.read.format(\"csv\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/sql/citytable.csv\")\n",
    "\n",
    "station.createOrReplaceTempView(\"station\")\n",
    "city.createOrReplaceTempView(\"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75ce93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42613c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b15c9f50",
   "metadata": {},
   "source": [
    "## Revising the Select Query 1\n",
    "**Query all columns for all American cities in the CITY table with populations larger than 100000. The CountryCode for America is USA.**\n",
    "\n",
    "     +---+----------------+-----------+-----------------+----------+\n",
    "     |ID |NAME            |COUNTRYCODE|DISTRICT         |POPULATION|\n",
    "     +---+----------------+-----------+-----------------+----------+\n",
    "     |6  |Rotterdam       |NLD        |Zuid-Holland     |12312     |\n",
    "     |19 |Zaanstad        |NLD        |Noord-Holland    |135621    |\n",
    "     |214|Porto Alegre    |BRA        |Rio Grande do Sul|1314032   |\n",
    "     |397|Lauro de Freitas|BRA        |Bahia            |109236    |\n",
    "     |547|Dobric          |BGR        |Varna            |100399    |\n",
    "     +---+----------------+-----------+-----------------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "809af58f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----------+----------+----------+\n",
      "|  ID|         NAME|COUNTRYCODE|  DISTRICT|POPULATION|\n",
      "+----+-------------+-----------+----------+----------+\n",
      "|3815|      El Paso|        USA|     Texas|    563662|\n",
      "|3878|   Scottsdale|        USA|   Arizona|    202705|\n",
      "|3965|       Corona|        USA|California|    124966|\n",
      "|3973|      Concord|        USA|California|    121780|\n",
      "|3977| Cedar Rapids|        USA|      Iowa|    120758|\n",
      "|3982|Coral Springs|        USA|   Florida|    117549|\n",
      "+----+-------------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "select * \n",
    "from city \n",
    "where COUNTRYCODE='USA' and POPULATION>100000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30ee2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----------+----------+----------+\n",
      "|  ID|         NAME|COUNTRYCODE|  DISTRICT|POPULATION|\n",
      "+----+-------------+-----------+----------+----------+\n",
      "|3815|      El Paso|        USA|     Texas|    563662|\n",
      "|3878|   Scottsdale|        USA|   Arizona|    202705|\n",
      "|3965|       Corona|        USA|California|    124966|\n",
      "|3973|      Concord|        USA|California|    121780|\n",
      "|3977| Cedar Rapids|        USA|      Iowa|    120758|\n",
      "|3982|Coral Springs|        USA|   Florida|    117549|\n",
      "+----+-------------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city.filter((col(\"COUNTRYCODE\")=='USA') & (col(\"POPULATION\")>100000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7e85e",
   "metadata": {},
   "source": [
    "## Revising the Select Query 2\n",
    "**Query the NAME field for all American cities in the CITY table with populations larger than 120000. The CountryCode for America is USA.**\n",
    "\n",
    "     +---+----------------+-----------+-----------------+----------+\n",
    "     |ID |NAME            |COUNTRYCODE|DISTRICT         |POPULATION|\n",
    "     +---+----------------+-----------+-----------------+----------+\n",
    "     |6  |Rotterdam       |NLD        |Zuid-Holland     |12312     |\n",
    "     |19 |Zaanstad        |NLD        |Noord-Holland    |135621    |\n",
    "     |214|Porto Alegre    |BRA        |Rio Grande do Sul|1314032   |\n",
    "     |397|Lauro de Freitas|BRA        |Bahia            |109236    |\n",
    "     |547|Dobric          |BGR        |Varna            |100399    |\n",
    "     +---+----------------+-----------+-----------------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f159a8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        NAME|\n",
      "+------------+\n",
      "|     El Paso|\n",
      "|  Scottsdale|\n",
      "|      Corona|\n",
      "|     Concord|\n",
      "|Cedar Rapids|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select NAME \n",
    "from citytable \n",
    "where COUNTRYCODE='USA' and POPULATION>120000\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08eb638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        NAME|\n",
      "+------------+\n",
      "|     El Paso|\n",
      "|  Scottsdale|\n",
      "|      Corona|\n",
      "|     Concord|\n",
      "|Cedar Rapids|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city.select(col(\"NAME\"))\\\n",
    "    .filter((col(\"COUNTRYCODE\")=='USA') & (col(\"POPULATION\")>120000)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea83c23",
   "metadata": {},
   "source": [
    "**Query all columns for a city in CITY with the ID 1661.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1098c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+--------+----------+\n",
      "|  ID|  NAME|COUNTRYCODE|DISTRICT|POPULATION|\n",
      "+----+------+-----------+--------+----------+\n",
      "|1661|Sayama|        JPN| Saitama|    162472|\n",
      "+----+------+-----------+--------+----------+\n",
      "\n",
      "+----+------+-----------+--------+----------+\n",
      "|  ID|  NAME|COUNTRYCODE|DISTRICT|POPULATION|\n",
      "+----+------+-----------+--------+----------+\n",
      "|1661|Sayama|        JPN| Saitama|    162472|\n",
      "+----+------+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using pysparkSQL\n",
    "spark.sql(\"\"\"\n",
    "select * from citytable where ID='1661'\n",
    "\"\"\").show(5)\n",
    "# using pyspark\n",
    "city.filter((col(\"ID\")=='1661')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db45d8",
   "metadata": {},
   "source": [
    "**Query a list of CITY names from STATION for cities that have an even ID number. Exclude duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62021af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----+-----+------+\n",
      "| Id|          City|State|Lat_N|Long_W|\n",
      "+---+--------------+-----+-----+------+\n",
      "|794|  Kissee Mills|   MO|  139|    73|\n",
      "|824|      Loma Mar|   CA|   48|   130|\n",
      "|478|        Tipton|   IN|   33|    97|\n",
      "|588|       Glencoe|   KY|   46|   136|\n",
      "|342|Chignik Lagoon|   AK|  103|   153|\n",
      "+---+--------------+-----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------+-----+-----+------+\n",
      "| Id|          City|State|Lat_N|Long_W|\n",
      "+---+--------------+-----+-----+------+\n",
      "|794|  Kissee Mills|   MO|  139|    73|\n",
      "|824|      Loma Mar|   CA|   48|   130|\n",
      "|478|        Tipton|   IN|   33|    97|\n",
      "|588|       Glencoe|   KY|   46|   136|\n",
      "|342|Chignik Lagoon|   AK|  103|   153|\n",
      "+---+--------------+-----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using pysparkSQL\n",
    "spark.sql(\"\"\"\n",
    "select * from stationtable where Id %2 ==0\n",
    "\"\"\").show(5)\n",
    "#using pyspark\n",
    "station.filter(col(\"Id\")%2==0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd32d6",
   "metadata": {},
   "source": [
    "**Find the difference between the total number of CITY entries in the table and the number of distinct CITY entries in the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56d0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|difference|\n",
      "+----------+\n",
      "|        13|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|difference|\n",
      "+----------+\n",
      "|        13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pysparksql\n",
    "spark.sql(\"\"\"\n",
    "select count(City) - count(distinct(City)) as difference\n",
    "from stationtable\n",
    "\"\"\").show(5)\n",
    "#using pyspark\n",
    "station.selectExpr(\"count(City) - count(distinct City) as difference\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6bd47a",
   "metadata": {},
   "source": [
    "**Query the two cities in STATION with the shortest and longest CITY names, as well as their respective lengths.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08fd0acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|city|c_len|\n",
      "+----+-----+\n",
      "| Amo|    3|\n",
      "+----+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|                city|c_len|\n",
      "+--------------------+-----+\n",
      "|Marine On  Saint ...|   23|\n",
      "+--------------------+-----+\n",
      "\n",
      "+----+-----+\n",
      "|city|c_len|\n",
      "+----+-----+\n",
      "| Amo|    3|\n",
      "+----+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|                city|c_len|\n",
      "+--------------------+-----+\n",
      "|Marine On  Saint ...|   23|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pysparksql\n",
    "spark.sql(\"\"\"\n",
    "select city, length(city) as c_len \n",
    "       from stationtable \n",
    "       order by c_len asc, city asc \n",
    "       limit 1\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "select city, length(city) as c_len\n",
    "       from stationtable order by c_len desc, city asc\n",
    "       limit 1\"\"\").show()\n",
    "#using pyspark\n",
    "station.select(col(\"city\"),length(col(\"city\")).alias(\"c_len\")) \\\n",
    "       .orderBy(col(\"c_len\").asc(),col(\"city\").asc())\\\n",
    "       .limit(1)\\\n",
    "       .show()\n",
    "station.select(col(\"city\"),length(col(\"city\")).alias(\"c_len\")) \\\n",
    "       .orderBy(col(\"c_len\").desc(),col(\"city\").asc())\\\n",
    "       .limit(1)\\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35e4d4",
   "metadata": {},
   "source": [
    "**Query the list of CITY names starting with vowels (a, e, i, o, u) from STATION. Exclude duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfd83419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         city|\n",
      "+-------------+\n",
      "|       Auburn|\n",
      "|   East China|\n",
      "|  Orange Park|\n",
      "|Andersonville|\n",
      "|    Elm Grove|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+\n",
      "|         city|\n",
      "+-------------+\n",
      "|       Auburn|\n",
      "|   East China|\n",
      "|  Orange Park|\n",
      "|Andersonville|\n",
      "|    Elm Grove|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pyspark sql\n",
    "spark.sql(\"\"\"\n",
    "       SELECT DISTINCT city FROM stationtable WHERE city REGEXP '^[AEIOUaeiou]';\n",
    "\"\"\").show(5)\n",
    "#using pyspark\n",
    "station.select(col(\"city\"))\\\n",
    "       .filter(col(\"city\").rlike (\"^[AEIOUaeiou].*\")) \\\n",
    "       .distinct()\\\n",
    "       .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4c29c",
   "metadata": {},
   "source": [
    "**Query the list of CITY names ending with vowels (a, e, i, o, u) from STATION. Exclude duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f497a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      city|\n",
      "+----------+\n",
      "|   Lismore|\n",
      "|  Samantha|\n",
      "|      Hope|\n",
      "| Wickliffe|\n",
      "|East China|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      city|\n",
      "+----------+\n",
      "|   Lismore|\n",
      "|  Samantha|\n",
      "|      Hope|\n",
      "| Wickliffe|\n",
      "|East China|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pyspark sql\n",
    "spark.sql(\"\"\"\n",
    "       SELECT DISTINCT city FROM stationtable WHERE city REGEXP '[AEIOUaeiou]$';\n",
    "\"\"\").show(5)\n",
    "#using pyspark\n",
    "station.select(col(\"city\"))\\\n",
    "       .filter(col(\"city\").rlike (\"[AEIOUaeiou]$.*\")) \\\n",
    "       .distinct()\\\n",
    "       .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02ed8a",
   "metadata": {},
   "source": [
    "**Query the list of CITY names from STATION that do not start with vowels. Exclude duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b225fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    city|\n",
      "+--------+\n",
      "|   Tyler|\n",
      "| Jemison|\n",
      "|  Grimes|\n",
      "| Lismore|\n",
      "|Samantha|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+\n",
      "|    city|\n",
      "+--------+\n",
      "|   Tyler|\n",
      "| Jemison|\n",
      "|  Grimes|\n",
      "| Lismore|\n",
      "|Samantha|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pyspark sql\n",
    "spark.sql(\"\"\"\n",
    "       SELECT DISTINCT city FROM stationtable WHERE city not REGEXP '^[AEIOUaeiou]';\n",
    "\"\"\").show(5)\n",
    "#using pyspark\n",
    "station.select(col(\"city\"))\\\n",
    "       .filter(~col(\"city\").rlike (\"^[AEIOUaeiou].*\")) \\\n",
    "       .distinct()\\\n",
    "       .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126121b",
   "metadata": {},
   "source": [
    "**Write an SQL query that reports the products that were only sold in the first quarter of 2019 (2019-01-01 to 2019-03-31).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd395f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n",
      "|product_id|product_name|unit_price|\n",
      "+----------+------------+----------+\n",
      "|         1|          S8|      1000|\n",
      "|         2|          G4|       800|\n",
      "|         3|      iPhone|      1400|\n",
      "+----------+------------+----------+\n",
      "\n",
      "+---------+----------+--------+----------+--------+-----+\n",
      "|seller_id|product_id|buyer_id| sale_date|quantity|price|\n",
      "+---------+----------+--------+----------+--------+-----+\n",
      "|        1|         1|       1|2019-01-21|       2| 2000|\n",
      "|        1|         2|       2|2019-02-17|       1|  800|\n",
      "|        2|         2|       3|2019-06-02|       1|  800|\n",
      "|        3|         3|       4|2019-05-13|       2| 2800|\n",
      "+---------+----------+--------+----------+--------+-----+\n",
      "\n",
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         1|          S8|\n",
      "+----------+------------+\n",
      "\n",
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         1|          S8|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'S8',1000),\n",
    "(2,'G4',800),\n",
    "(3,'iPhone',1400)]\n",
    "\n",
    "data2=[(1,1,1,'2019-01-21',2,2000),\n",
    "(1,2,2,'2019-02-17',1,800),\n",
    "(2,2,3,'2019-06-02',1,800),\n",
    "(3,3,4,'2019-05-13',2,2800)]\n",
    "\n",
    "schema1 = ['product_id','product_name','unit_price']\n",
    "schema2 = ['seller_id','product_id','buyer_id','sale_date','quantity','price']\n",
    "product = spark.createDataFrame(data=data1,schema=schema1)\n",
    "sales = spark.createDataFrame(data=data2,schema=schema2)\n",
    "product.show()\n",
    "sales.show()\n",
    "product.createOrReplaceTempView(\"producttable\")\n",
    "sales.createOrReplaceTempView(\"salestable\")\n",
    "\n",
    "#using pyspark\n",
    "invalid_sales = (sales\n",
    "                 .filter(~((col(\"sale_date\") >= \"2019-01-02\") & (col(\"sale_date\") <= \"2019-03-31\")))\n",
    "                 .select(\"product_id\")\n",
    "                 .distinct())\n",
    "result = (product.join(invalid_sales, on=\"product_id\", how=\"left_anti\")  # equivalent of NOT IN\n",
    "          .select(\"product_id\", \"product_name\")).show()\n",
    "#using pysparksql\n",
    "spark.sql(\"\"\"\n",
    "select product_id,product_name \n",
    "from producttable \n",
    "where product_id\n",
    "not in (select product_id\n",
    "        from salestable \n",
    "        where sale_date not between '2019-01-02' and '2019-03-31')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8c751",
   "metadata": {},
   "source": [
    "**Write an SQL query to find all the authors that viewed at least one of their own articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b447b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "+---------+\n",
      "|author_id|\n",
      "+---------+\n",
      "|        7|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views = [(1,3,5,'2019-08-01'),\n",
    "(1,3,6,'2019-08-02'),\n",
    "(2,7,7,'2019-08-01'),\n",
    "(2,7,6,'2019-08-02'),\n",
    "(4,7,1,'2019-07-22'), \n",
    "(3,4,4,'2019-07-21'),\n",
    "(3,4,4,'2019-07-21')]\n",
    "\n",
    "schema = ['article_id','author_id','viewer_id','viewer_date']\n",
    "viewers = spark.createDataFrame(data=views,schema=schema)\n",
    "viewers.createOrReplaceTempView(\"viewtable\")\n",
    " \n",
    "#using pysparksql\n",
    "spark.sql(\"\"\"\n",
    "select distinct author_id as id\n",
    "from viewtable\n",
    "where author_id=viewer_id\n",
    "\"\"\").show()\n",
    "#using pyspark\n",
    "viewers.filter(col(\"author_id\")==col(\"viewer_id\"))\\\n",
    "      .select(\"author_id\")\\\n",
    "      .distinct()\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a90708",
   "metadata": {},
   "source": [
    "**Write an SQL query to find the percentage of immediate orders in the Delivery table (rounded to 2 decimal places).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5053ed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------------------------+\n",
      "|delivery_id|customer_id|order_date|customer_pref_delivery_date|\n",
      "+-----------+-----------+----------+---------------------------+\n",
      "|          1|          1|2019-08-01|                 2019-08-02|\n",
      "|          2|          5|2019-08-02|                 2019-08-02|\n",
      "|          3|          1|2019-08-11|                 2019-08-11|\n",
      "|          4|          3|2019-08-24|                 2019-08-26|\n",
      "|          5|          4|2019-08-21|                 2019-08-22|\n",
      "|          6|          2|2019-08-11|                 2019-08-13|\n",
      "+-----------+-----------+----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order=[(1,1,'2019-08-01','2019-08-02'),(2,5,'2019-08-02','2019-08-02'),\n",
    "        (3,1,'2019-08-11','2019-08-11'),(4,3,'2019-08-24','2019-08-26'),\n",
    "        (5,4,'2019-08-21','2019-08-22'),(6,2,'2019-08-11','2019-08-13')]\n",
    "schema = ['delivery_id','customer_id','order_date','customer_pref_delivery_date']\n",
    "orders = spark.createDataFrame(data=order,schema=schema)\n",
    "orders.createOrReplaceTempView(\"ordertable\")\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbe0b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|               33.33|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|               33.33|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pyspark.sql\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ROUND(100 * SUM(CASE WHEN order_date = customer_pref_delivery_date THEN 1 ELSE 0 END) / COUNT(*), 2) \n",
    "    AS immediate_percentage\n",
    "FROM ordertable\n",
    "\"\"\").show()\n",
    "\n",
    "#using pyspark\n",
    "orders = orders.withColumn(\n",
    "    \"immediate_flag\",\n",
    "    when(col(\"order_date\") == col(\"customer_pref_delivery_date\"), 1).otherwise(0)\n",
    ")\n",
    "orders.agg_df = orders.agg(\n",
    "    (round((sum(\"immediate_flag\") / count(\"*\")) * 100, 2)).alias(\"immediate_percentage\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747a56e",
   "metadata": {},
   "source": [
    "**Write an SQL query to find the CTR (Click-Through Rate) of each Ad. Round to 2 decimals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "570eef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|ad_id|user_id| action|\n",
      "+-----+-------+-------+\n",
      "|    1|      1|Clicked|\n",
      "|    2|      2|Clicked|\n",
      "|    3|      3| Viewed|\n",
      "|    5|      5|Ignored|\n",
      "|    1|      7|Ignored|\n",
      "|    2|      7| Viewed|\n",
      "|    3|      5|Clicked|\n",
      "|    1|      4| Viewed|\n",
      "|    2|     11| Viewed|\n",
      "|    1|      2|Clicked|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctr = [(1,1,'Clicked'),(2,2,'Clicked'),(3,3,'Viewed'),(5,5,'Ignored'),\n",
    "       (1,7,'Ignored'),(2,7,'Viewed'),(3,5,'Clicked'),(1,4,'Viewed'),\n",
    "       (2,11,'Viewed'),(1,2,'Clicked')]\n",
    "schema = ['ad_id','user_id','action']\n",
    "ctrdf = spark.createDataFrame(data=ctr,schema=schema)\n",
    "ctrdf.createOrReplaceTempView(\"ctrtable\")\n",
    "ctrdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a83c9234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|ad_id|  ctr|\n",
      "+-----+-----+\n",
      "|    1|66.67|\n",
      "|    3| 50.0|\n",
      "|    2|33.33|\n",
      "|    5|  0.0|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select ad_id,\n",
    "ifnull(\n",
    "round( avg(\n",
    "            case\n",
    "                when action = \"Clicked\" then 1\n",
    "                when action = \"Viewed\" then 0\n",
    "                else null\n",
    "end ) * 100,\n",
    "2), 0)\n",
    "as ctr\n",
    "from ctrtable\n",
    "group by ad_id\n",
    "order by ctr desc, ad_id asc;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "081ed972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|ad_id|  ctr|\n",
      "+-----+-----+\n",
      "|    1|66.67|\n",
      "|    3| 50.0|\n",
      "|    2|33.33|\n",
      "|    5|  0.0|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = ctrdf.groupBy(\"ad_id\").agg(\n",
    "    F.round(\n",
    "        F.avg(\n",
    "            F.when(F.col(\"action\") == \"Clicked\", 1)\n",
    "             .when(F.col(\"action\") == \"Viewed\", 0)\n",
    "             .otherwise(None)\n",
    "        ) * 100, 2\n",
    "    ).alias(\"ctr\")\n",
    ")\n",
    "\n",
    "# Replace null with 0\n",
    "df = df.withColumn(\"ctr\", F.when(F.col(\"ctr\").isNull(), F.lit(0)).otherwise(F.col(\"ctr\")))\n",
    "\n",
    "# Order by ctr desc, ad_id asc\n",
    "df.orderBy(F.col(\"ctr\").desc(), F.col(\"ad_id\").asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137a7b",
   "metadata": {},
   "source": [
    "**Write an SQL query to find the team size of each employee.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d92c5cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|empid|teamid|\n",
      "+-----+------+\n",
      "|    1|     8|\n",
      "|    2|     8|\n",
      "|    3|     8|\n",
      "|    4|     7|\n",
      "|    5|     9|\n",
      "|    6|     9|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee = [(1,8),(2,8),(3,8),(4,7),(5,9),(6,9)]\n",
    "schema = ['empid','teamid']\n",
    "empdf = spark.createDataFrame(data=employee,schema=schema)\n",
    "empdf.createOrReplaceTempView(\"emptable\")\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc2be101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|empid|team_size|\n",
      "+-----+---------+\n",
      "|    1|        3|\n",
      "|    2|        3|\n",
      "|    3|        3|\n",
      "|    5|        2|\n",
      "|    6|        2|\n",
      "|    4|        1|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select empid,\n",
    "         count(*) over(partition by teamid) as team_size\n",
    "from emptable order by team_size desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ad6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ade2291f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|empid|team_size|\n",
      "+-----+---------+\n",
      "|    1|        3|\n",
      "|    2|        3|\n",
      "|    3|        3|\n",
      "|    5|        2|\n",
      "|    6|        2|\n",
      "|    4|        1|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"teamid\")\n",
    "empdf = empdf.withColumn(\"team_size\", F.count(\"*\").over(window))\n",
    "empdf.orderBy(F.col(\"team_size\").desc()).select(\"empid\", \"team_size\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5e219",
   "metadata": {},
   "source": [
    "**Write an SQL query to find the type of weather in each country for November 2019.**\n",
    "**The type of weather is:\n",
    " Cold if the average weather_state is less than or equal 15,\n",
    " Hot if the average weather_state is greater than or equal to 25,\n",
    "and\n",
    " Warm otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49e2b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country=[(2,'USA'),(3,'Australia'),(7,'Peru'),(5,'China'),(8,'Morocco'),(9,'Spain')]\n",
    "countryschema=['id','countryname']\n",
    "weather = [(2,15,'2019-11-01'),(2,12,'2019-10-28'),(2,12,'2019-10-27'),(3,-2,'2019-11-10'),\n",
    "           (3,0,'2019-11-11'),(3,3,'2019-11-12'),(5,16,'2019-11-07'),(5,18,'2019-11-09'),\n",
    "           (5,21,'2019-11-23'),(7,25,'2019-11-28'),(7,22,'2019-12-01'),(7,20,'2019-12-02'),\n",
    "           (8,25,'2019-11-05'),(8,27,'2019-11-15'),(8,31,'2019-11-25'),(9,7,'2019-10-23'),(9,3,'2019-12-23')]\n",
    "weatherschema=['id','weatherstate','day']\n",
    "countrydf = spark.createDataFrame(data=country,schema=countryschema)\n",
    "weatherdf = spark.createDataFrame(data=weather,schema=weatherschema)\n",
    "countrydf.createOrReplaceTempView(\"country\")\n",
    "weatherdf.createOrReplaceTempView(\"weather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3da63d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|countryname|\n",
      "+---+-----------+\n",
      "|  2|        USA|\n",
      "|  3|  Australia|\n",
      "+---+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|weatherstate|       day|\n",
      "+---+------------+----------+\n",
      "|  2|          15|2019-11-01|\n",
      "|  2|          12|2019-10-28|\n",
      "|  2|          12|2019-10-27|\n",
      "|  3|          -2|2019-11-10|\n",
      "|  3|           0|2019-11-11|\n",
      "|  3|           3|2019-11-12|\n",
      "|  5|          16|2019-11-07|\n",
      "|  5|          18|2019-11-09|\n",
      "|  5|          21|2019-11-23|\n",
      "|  7|          25|2019-11-28|\n",
      "|  7|          22|2019-12-01|\n",
      "|  7|          20|2019-12-02|\n",
      "|  8|          25|2019-11-05|\n",
      "|  8|          27|2019-11-15|\n",
      "|  8|          31|2019-11-25|\n",
      "|  9|           7|2019-10-23|\n",
      "|  9|           3|2019-12-23|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countrydf.show(2)\n",
    "weatherdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19dea2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|countryname|weathertype|\n",
      "+-----------+-----------+\n",
      "|        USA|       cold|\n",
      "|  Australia|       cold|\n",
      "|      China|       warm|\n",
      "|       Peru|        hot|\n",
      "|    Morocco|        hot|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select c.countryname,\n",
    "           case \n",
    "               when avg(w.weatherstate) <= 15 then 'cold'\n",
    "               when avg(w.weatherstate) >= 25 then 'hot'\n",
    "               else 'warm'\n",
    "           end as weathertype\n",
    "    from country as c\n",
    "    inner join weather w \n",
    "        on c.id = w.id\n",
    "    where w.day between '2019-11-01' and '2019-11-30'\n",
    "    group by c.id, c.countryname\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee699719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|countryname|weathertype|\n",
      "+-----------+-----------+\n",
      "|        USA|       cold|\n",
      "|       Peru|       warm|\n",
      "|  Australia|       cold|\n",
      "|      China|       warm|\n",
      "|      Spain|       cold|\n",
      "|    Morocco|        hot|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherdf2 = weatherdf.groupBy(\"id\").agg(F.avg(\"weatherstate\").alias(\"avg_state\"))\\\n",
    "         .withColumn(\"weathertype\", when ((col(\"avg_state\"))<=15,'cold')\\\n",
    "                     .when ((col(\"avg_state\"))>=25,'hot')\n",
    "                     .otherwise('warm'))\n",
    "result = countrydf.join(weatherdf2,weatherdf[\"id\"]==countrydf[\"id\"],'inner')\n",
    "result2 = result.select(\"countryname\",\"weathertype\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b45f1",
   "metadata": {},
   "source": [
    "**Write an SQL query to find the average selling price for each product, rounded to 2 decimal places.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3e4e74b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+\n",
      "|product_id|start_date|  end_date|price|\n",
      "+----------+----------+----------+-----+\n",
      "|         1|2019-02-17|2019-02-28|    5|\n",
      "|         1|2019-03-01|2019-03-22|   20|\n",
      "|         2|2019-02-01|2019-02-20|   15|\n",
      "|         2|2019-02-21|2019-03-31|   30|\n",
      "+----------+----------+----------+-----+\n",
      "\n",
      "+---+-------------+-----+\n",
      "| id|purchase_date|units|\n",
      "+---+-------------+-----+\n",
      "|  1|   2019-02-25|  100|\n",
      "|  1|   2019-03-01|   15|\n",
      "|  2|   2019-02-10|  200|\n",
      "|  2|   2019-03-22|   30|\n",
      "+---+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values=[(1,'2019-02-17','2019-02-28',5),(1,'2019-03-01','2019-03-22',20),\n",
    "(2,'2019-02-01','2019-02-20',15),(2,'2019-02-21','2019-03-31',30)]\n",
    "valuesschema=['product_id','start_date','end_date','price']\n",
    "units =[(1,'2019-02-25',100),(1,'2019-03-01',15),(2,'2019-02-10',200),(2,'2019-03-22',30)]\n",
    "pricesschema=['id','purchase_date','units']\n",
    "\n",
    "prices = spark.createDataFrame(data=values,schema=valuesschema)\n",
    "unitssold = spark.createDataFrame(data=units,schema=pricesschema)\n",
    "prices.createOrReplaceTempView(\"prices\")\n",
    "unitssold.createOrReplaceTempView(\"unitssold\")\n",
    "prices.show()\n",
    "unitssold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "319d99d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|average_price|\n",
      "+---+-------------+\n",
      "|  1|         6.96|\n",
      "|  2|        16.96|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.id\n",
    "      , round(SUM(a.units * b.price) / SUM(a.units), 2) AS\n",
    "average_price\n",
    "FROM unitssold a\n",
    "      JOIN prices b\n",
    "      ON (a.id = b.product_id\n",
    "           AND a.purchase_date >= b.start_date\n",
    "           AND a.purchase_date <= b.end_date)\n",
    "GROUP BY a.id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a24f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+----------+----------+----------+-----+-------+\n",
      "| id|purchase_date|units|product_id|start_date|  end_date|price|revenue|\n",
      "+---+-------------+-----+----------+----------+----------+-----+-------+\n",
      "|  1|   2019-02-25|  100|         1|2019-02-17|2019-02-28|    5|    500|\n",
      "|  1|   2019-03-01|   15|         1|2019-03-01|2019-03-22|   20|    300|\n",
      "|  2|   2019-02-10|  200|         2|2019-02-01|2019-02-20|   15|   3000|\n",
      "|  2|   2019-03-22|   30|         2|2019-02-21|2019-03-31|   30|    900|\n",
      "+---+-------------+-----+----------+----------+----------+-----+-------+\n",
      "\n",
      "+----------+-------------+-----------+\n",
      "|product_id|total_revenue|total_units|\n",
      "+----------+-------------+-----------+\n",
      "|         1|          800|        115|\n",
      "|         2|         3900|        230|\n",
      "+----------+-------------+-----------+\n",
      "\n",
      "+----------+-------------+-----------+-------------+\n",
      "|product_id|total_revenue|total_units|average_price|\n",
      "+----------+-------------+-----------+-------------+\n",
      "|         1|          800|        115|         6.96|\n",
      "|         2|         3900|        230|        16.96|\n",
      "+----------+-------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = unitssold.join(prices,\\\n",
    "                     (prices[\"product_id\"] == unitssold[\"id\"]) &\n",
    "                     (unitssold[\"purchase_date\"] >= prices[\"start_date\"]) &\n",
    "                     (unitssold[\"purchase_date\"] <= prices[\"end_date\"])\n",
    "                     ,'inner')\\\n",
    "                \n",
    "step1 = result.withColumn(\"revenue\", col(\"price\") * col(\"units\"))\n",
    "step1.show()\n",
    "step2 = step1.groupBy(\"product_id\") \\\n",
    "              .agg(\n",
    "                  sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                  sum(\"units\").alias(\"total_units\")\n",
    "              )\n",
    "step2.show()\n",
    "\n",
    "step3= step2.withColumn(\"average_price\",\n",
    "                         round(col(\"total_revenue\")/col(\"total_units\"), 2))\n",
    "\n",
    "step3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08bbf2",
   "metadata": {},
   "source": [
    "**Write an SQL query to report the first login date for each player.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa6519d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+-----------+\n",
      "|playerid|deviceid| eventdate|gamesplayed|\n",
      "+--------+--------+----------+-----------+\n",
      "|       1|       2|2016-03-01|          5|\n",
      "|       1|       2|2016-05-02|          6|\n",
      "|       2|       3|2017-06-25|          1|\n",
      "|       3|       1|2016-03-02|          0|\n",
      "|       3|       4|2018-07-03|          5|\n",
      "+--------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity = [(1,2,'2016-03-01',5),(1,2,'2016-05-02',6),(2,3,'2017-06-25',1),\n",
    "(3,1,'2016-03-02',0),(3,4,'2018-07-03',5)]\n",
    "schema=['playerid','deviceid','eventdate','gamesplayed']\n",
    "\n",
    "activitydf = spark.createDataFrame(data=activity,schema=schema)\n",
    "activitydf.createOrReplaceTempView(\"activitytable\")\n",
    "activitydf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a9878ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|playerid|first_login|\n",
      "+--------+-----------+\n",
      "|       1| 2016-03-01|\n",
      "|       2| 2017-06-25|\n",
      "|       3| 2016-03-02|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select player_id,event_date as first_login,\n",
    "#        row_number() over(partition by player_id) as row_num\n",
    "# from Activity;\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT tmp.playerid,\n",
    "       tmp.eventdate AS first_login\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           row_number() OVER (PARTITION BY playerid ORDER BY eventdate) AS row_num\n",
    "    FROM activitytable\n",
    ") tmp\n",
    "WHERE tmp.row_num = 1;\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32b1ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|playerid|first_login|\n",
      "+--------+-----------+\n",
      "|       1| 2016-03-01|\n",
      "|       2| 2017-06-25|\n",
      "|       3| 2016-03-02|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"playerid\").orderBy(\"eventdate\")\n",
    "df_with_rn = activitydf.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "first_login_df = df_with_rn.filter(col(\"row_num\") == 1) \\\n",
    "                           .select(\"playerid\", col(\"eventdate\").alias(\"first_login\"))\n",
    "\n",
    "first_login_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c812bc",
   "metadata": {},
   "source": [
    "**Write an SQL query to report the device that is first logged in for each player.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04514e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|playerid|deviceid|\n",
      "+--------+--------+\n",
      "|       1|       2|\n",
      "|       2|       3|\n",
      "|       3|       1|\n",
      "+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 18:06:01 ERROR UserGroupInformation: TGT is expired. Aborting renew thread for rrastats/srdcrrahdtf01.ril.com@RABFPRD.COM.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT tmp.playerid,\n",
    "       tmp.deviceid\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           row_number() OVER (PARTITION BY playerid ORDER BY eventdate) AS row_num\n",
    "    FROM activitytable\n",
    ") tmp\n",
    "WHERE tmp.row_num = 1;\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a082c22",
   "metadata": {},
   "source": [
    "**Write an SQL query to get the names of products that have at least 100 units ordered in February 2020 and their amount.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0690c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00224594",
   "metadata": {},
   "source": [
    "### Q27: Write an SQL query to find the users who have valid emails. (Valid = prefix rules + @leetcode.com domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e3f103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ae00d",
   "metadata": {},
   "source": [
    "### Q28: Write an SQL query to report the customers who have spent at least $100 in each month of June and July 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3ea9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab00ec",
   "metadata": {},
   "source": [
    "### Q29: Write an SQL query to report the distinct titles of the kid-friendly movies streamed in June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17163c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ec173",
   "metadata": {},
   "source": [
    "### Q30: Write an SQL query to find the NPV of each query from the Queries table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87187276",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ab801",
   "metadata": {},
   "source": [
    "### Q31: Write an SQL query to find the unique ID of each employee. If not present, return NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43c1cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b289e",
   "metadata": {},
   "source": [
    "### Q32: Write an SQL query to report the distance travelled by each user. Order by travelled_distance desc, name asc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b715981",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247b184",
   "metadata": {},
   "source": [
    "### Q33: Write an SQL query to get the names of products that have at least 100 units ordered in February 2020 and their amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a89c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ecbd5",
   "metadata": {},
   "source": [
    "### Q34: Write an SQL query to: (1) Find the user with most rated movies; (2) Find the movie with highest avg rating in Feb 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "389964b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba914d",
   "metadata": {},
   "source": [
    "### Q35: Write an SQL query to show the unique ID of each user. If no unique ID, return NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9330f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74069994",
   "metadata": {},
   "source": [
    "### Q36: Write an SQL query to find students enrolled in departments that no longer exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "846ef715",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401eb0fb",
   "metadata": {},
   "source": [
    "### Q37: Write an SQL query to report the number of calls and total duration between each pair of distinct persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1e952d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa3663",
   "metadata": {},
   "source": [
    "### Q38: Write an SQL query to find the average selling price for each product, rounded to 2 decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0e99e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c9cd6",
   "metadata": {},
   "source": [
    "### Q39: Write an SQL query to report the number of cubic feet of volume the inventory occupies in each warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94e8f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea90fd",
   "metadata": {},
   "source": [
    "### Q40: Write an SQL query to report the difference between apples and oranges sold each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f0e6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be1a63",
   "metadata": {},
   "source": [
    "### Q41: Write an SQL query to report the fraction of players that logged in again the day after their first login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b359ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f6c82",
   "metadata": {},
   "source": [
    "### Q42: Write an SQL query to report the managers with at least five direct reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cb87959",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8fb9a1",
   "metadata": {},
   "source": [
    "### Q43: Write an SQL query to report each department and number of students. Include empty depts. Order by count desc, name asc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "953106d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027833b",
   "metadata": {},
   "source": [
    "### Q44: Write an SQL query to report the customers who bought all products in the Product table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc48885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66897938",
   "metadata": {},
   "source": [
    "### Q45: Write an SQL query to report the most experienced employees in each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f878ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8290a",
   "metadata": {},
   "source": [
    "### Q46: Write an SQL query to report the books that sold less than 10 copies in the last year (excluding books available <1 month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cc1fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb929cf1",
   "metadata": {},
   "source": [
    "### Q47: Write an SQL query to find the highest grade with its corresponding course for each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15ed5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a0bf7",
   "metadata": {},
   "source": [
    "### Q48: Write an SQL query to find the winner in each group of players (max total points, tiebreak = lowest player_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fd7002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52773a16",
   "metadata": {},
   "source": [
    "### Q49: Write an SQL query to report the football match results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a3ed8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb31ac",
   "metadata": {},
   "source": [
    "### Q50: Write an SQL query to report the final winners of football matches based on scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48a647cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your SQL query here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df8e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3e285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark_env)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
