# Apache Ignite

Apache Ignite provides a convenient and easy-to-use interface for developers to work with large-scale data sets in real time and other aspects of in-memory computing.

## Features

Apache Ignite has the following features:

1. Data grid
2. Compute grid
3. Service grid
4. Bigdata accelerator
5. Streaming grid

The following figure illustrates the basic features of Apache Ignite:

## Primary Capabilities

- Elasticity: An Apache Ignite cluster can grow horizontally by adding new nodes.
- Persistence: Apache Ignite data grid can persist cache entries in RDBMS, even in NoSQL like MongoDB or Cassandra.
- Cache as a Service (CaaS): Apache Ignite supports Cache-as-a-Service across the organization which allows multiple applications from different departments to access managed in-memory cache instead of slow disk base databases.
- 2ⁿ Level Cache: Apache Ignite is the perfect caching tier to use as a 2ⁿ level cache in Hibernate and MyBatis.
- High-performance Hadoop accelerator: Apache Ignite can replace Hadoop task tracker and job tracker and HDFS to increase the performance of big data analysis.
- Share state in-memory across Spark applications: Ignite RDD allows easily sharing of state in-memory between different Spark jobs or applications. With Ignite in-memory shared RDD's, any Spark application can put data into Ignite cache which will be accessible by another Spark application later.
- Distributed computing: Apache Ignite provides a set of simple APIs that allows a user to distribute computation and data processing across multiple nodes in the cluster to gain high performance. Apache Ignite distributed services is very useful to develop and execute microservice like architecture.
- Streaming: Apache Ignite allows processing continuous never-ending streams of data in scalable and fault-tolerant fashion in-memory, rather than analyzing the data after it has been stored in the database.


## Architecture Deep Dive

### Caching Topology

Ignite provides three different approaches to caching topology: Partitioned, Replicated and Local. A cache mode is configured for each cache individually. Every caching topology has its own goal with pros and cons. The default cache topology is partitioned, without any backup option.

1. Partitioned: The goal of this topology is to get extreme scalability. In this mode, the Ignite cluster transparently partitions the cached data to distribute the load across an entire cluster evenly. By partitioning the data evenly, the size of the cache and the processing power grows linearly with the size of the cluster. The responsibility for managing the data is automatically shared across the cluster. Every node or server in the cluster contains its primary data with a backup copy if defined.
   - With partitioned cache topology, DML operations on the cache are extremely fast, because only one primary node (optionally 1 or more backup node) needs to be updated for every key. For high availability, a backup copy of the cache entry should be configured. The backup copy is the redundant copy of one or more primary copies, which will live in another node.

2. Replicated: The goal of this approach is to get extreme performance. With this approach, cache data is replicated to all members of the cluster. Since the data is replicated to each cluster node, it is available for use without any waiting. This provides highest possible speed for read-access; each member accesses the data from its own memory. The downside is that frequent writes are very expensive. Updating a replicated cache requires pushing the new version to all other cluster members. This will limit the scalability if there are a high frequency of updates.

3. Local: This is a very primitive version of cache mode; with this approach, no data is distributed to other nodes in the cluster. As far as the Local cache does not have any replication or partitioning process, data fetching is very inexpensive and fast. It provides zero latency access to recently and frequently used data. The local cache is mostly used in read-only operations. It also works very well for read/write-through behavior, where data is loaded from the data sources on cache misses. Unlike a distributed cache, local cache still has all the features of distributed cache; it provides query caching, automatic data eviction and much more.

### Caching Strategy

With the explosion of high transactions web applications and mobile apps, data storage has become the main bottleneck of performance. In most cases, persistence stores such as relational databases cannot scale out perfectly by adding more servers. In this circumstance, in-memory distributed cache offers an excellent solution to data storage bottleneck. It extends multiple servers (called a grid) to pool their memory together and keep the cache synchronized across all servers. There are two main strategies to use in a distributed in-memory cache.

1. Cache-aside: In this approach, an application is responsible for reading and writing from the persistence store. The cache doesn't interact with the database at all. This is called cache-aside. The cache behaves as a fast scaling in-memory data store. The application checks the cache for data before querying the data store. Also, the application updates the cache after making any changes to the persistence store. However, even though cache-aside is very fast, there are quite a few disadvantages with this strategy. Application code can become complex and may lead to code duplication if multiple applications deal with the same data store. When there are cache data misses, the application will query the data store, update the caches and continue processing. This can result in multiple data store visits if different application threads perform this processing at the same time.
2. Read-through and Write-through: This is where application treats in-memory cache as the main data store, and reads data from it and writes data to it. In-memory cache is responsible for propagating the query to the data store on cache misses. Also, the data will be updated automatically whenever it is updated in the cache. All read-through and write-through operations will participate in the overall cache transaction and will be committed or rolled back as a whole. Read-through and write-through have numerous advantages over cache-aside. First of all, it simplifies application code. Read-through allows the cache to reload objects from the database when it expires automatically. This means that your application does not have to hit the database in peak hours because the latest data is always in the cache.
3. Write behind: It is also possible to use write-behind to get better write performance. Write-behind lets your application quickly update the cache and return. It then aggregates the updates and asynchronously flushes them to persistence store as a bulk operation. Also with Write-behind, you can specify throttling limits, so the database writes are not performed as fast as the cache updates and therefore the pressure on the database is lower. Additionally, you can schedule the database writes to occur during off-peak hours, which can minimize the pressure on the Database.



### CAP Theorem and Where Does Ignite Stand

- CA system: In this approach, you sacrifice partition tolerance for getting consistency and availability. Your database system offers transactions, and the system is highly available. Most of the relational databases are classified as CA systems. This system has serious problems with scaling.
- CP system: the opposite of the CA system. In CP system availability is sacrificed for consistency and partition-tolerance. In the event of the node failure, some data will be lost.
- AP system: This system is always available and partitioned. Also this system scales easily by adding nodes to the cluster. Cassandra is a good example of this type of system.

Now, we can return back to our question, where does Ignite stand in the CAP theorem? At first glance, Ignite can be classified by CP, because Ignite is fully ACID compliant distributed transactions with partitioned tolerance. But this is half part of the history. Apache Ignite can also be considered an AP system. But why does Ignite have two different classifications? Because it has two different transactional modes for cache operations, transactional and atomic. In transactional mode, you can group multiple DML operations in one transaction and make a commit into the cache. In this scenario, Ignite will lock data on access by a pessimistic lock. If you configure backup copy for the cache, Ignite will use 2p commit protocol for its transaction. On the other hand, in atomic mode Ignite supports multiple atomic operations, one at a time. In the atomic mode, each DML operation will either succeed or fail and neither Read nor Write operation will lock the data at all. This mode gives a higher performance than the transactional mode. When you make a write in Ignite cache, for every piece of data there will be a master copy in primary node and a backup copy (if defined). When you read data from Ignite grid, you always read from the Primary node, unless the Primary node is down, at which time data will be read from the backup. From this point of view, you gain the system availability and the partition-tolerance of the entire system as an AP system. In the atomic mode, Ignite is very similar to Apache Cassandra. However, real world systems rarely fall neatly into all of these above categories, so it's more helpful to view CAP as a continuum. Most systems will make some effort to be consistent, available, and partition tolerant, and many can be tuned depending on what's most important.



### Zero SPOF

In any distributed system, node failure should be expected, particularly as the size of the cluster grows. The Zero Single Point of Failure (SPOF) design pattern ensures that no single part of a system can stop the entire cluster or system from working. Sometimes, the system using master-slave replication or the mixed master-master system falls into this category. Prior to Hadoop 2.0.0, the Hadoop NameNode was an SPOF in an HDFS cluster. Netflix has calculated the revenue loss for each ms of downtime or latency, and it is not small at all. Most businesses do not want single points of failure for the obvious reason. Apache Ignite, as a horizontally scalable distributed system, is designed in such way that all nodes in the cluster are equal, you can read and write from any node in the cluster. There are no master-slave communications in the Ignite cluster. Data is backed up or replicated across the cluster so that failure of any node doesn't bring down the entire cluster or the application. This way Ignite provides a dynamic form of High Availability. Another benefit of this approach is the ease at which new nodes can be added. When new nodes join the cluster, they can take over a portion of data from the existing nodes. Because all nodes are the same, this communication can happen seamlessly in a running cluster.


### How SQL Queries Work in Ignite

In chapter one, we introduced Ignite SQL query feature very superficially. In chapter four, we will go into more details about Ignite SQL queries. It's interesting to know how a query processes under the hood of Ignite. There are two main approaches to process SQL queries in Ignite:

- In-memory Map-Reduce: If you are executing any SQL query against a Partitioned cache, Ignite under the hood splits the query into in-memory map queries and a single reduce query. The number of map queries depends on the size of the partitions and number the partitions in the cluster. Then all map queries are executed on all data nodes of the participating caches, providing results to the reducing node, which will, in turn, run the reduce query over these intermediate results. If you are not familiar with the Map-Reduce pattern, you can imagine it as a Java Fork-join process.
- H2 SQL engine: if you are executing SQL queries against Replicated or Local cache, Ignite knows that all data is available locally and runs a simple local SQL query in the H2 database engine. Note that, in replicated caches, every node contains a replica data for other nodes. H2 database is a free database written in Java and can work in an embedded mode. Depending on the configuration, every Ignite node can have an embedded H2 SQL engine.

### Performance Tuning SQL Queries

There are a few principles you should follow or consider when using SQL queries against Ignite cache:

- Carefully use the index, Indexes also consumes memory (on-heap/off-heap). Also, each index needs to be updated separately. If you have a huge update on a cache, index update can seriously decrease your application performance.
- Index only fields, that are participating in SQL WHERE clause.
- Do not overuse the non-collocated distributed joins approach in practice because the performance of this type of joins is worse than the performance of the affinity collocation-based joins due to the fact that there will be much more network round-trips and data movement between the nodes to fulfill a query.
- In SQL projection statement, select fields that you exactly needs. Extra fields often increase the data roundtrip over the network.
- If the query is using operator OR then it may use indexes in a way you not would expect. For example, for query `select name from Person where sex='M' and (age = 20 or age = 30)` index on field age will not be used even if it is obviously more selective than index on field sex and thus is preferable. To workaround this issue you have to rewrite the query with UNION ALL (notice that UNION without ALL will return DISTINCT rows, which will change query semantics and introduce additional performance penalty) like `select name from Person where sex='M' and age = 20 UNION ALL select name from Person where sex='M' and age = 30`. This way indexes will be used correctly.
- If query contains operator IN then it has two problems: it is impossible to provide variable list of parameters (you have to specify the exact list in query like `where id in (?, ?, ?)`), but you can not write it like `where id in ?` and pass array or collection) and this query will not use index. To work around both problems, you can rewrite the query in the following way: `select p.name from Person p join table(id bigint = ?) i on p.id = i.id`. Here you can provide object array (Object[]) of any length as a parameter, and the query will use an index on field id. Note that primitive arrays (int[], long[], etc..) can not be used with this syntax, you have to pass an array of boxed primitives.


### Expiration & Eviction of Cache Entries in Ignite

Apache Ignite caches are very powerful and can be configured and tuned to suit the needs of most applications. Apache Ignite provides two different approaches for data refreshing, which refers to an aspect of a copy of data (e.g,. entries in a cache) being up-to-date with the source version of the data. A stale copy of the data is considered to be out of use. Expiration and Evictions of cache entries is one of the key aspects of caching.

Expiration:

Usually, the purpose of a cache is to store short-lived data that needs to refresh regularly. You can use Apache Ignite expiry policy to store entry only for a certain period of time. Once expired, the entry is automatically removed from the cache. For instance, the cache could be configured to expire entries ten seconds after they are put in. Sometimes, it is called Time-to-live or TTL. Or to expire 20 seconds after the last time the entry was accessed or retrieve from the cache. Apache Ignite provides five differents predefined expire policy as follows:

1. CreatedExpiryPolicy - Defines the expiry of a Cache Entry based on when it was created. An update does not reset the expiry time
2. AccessedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was accessed. Accessed does not include a cache update.
3. ModifiedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was updated. Updating includes created and changing (updating) an entry.
4. TouchedExpiryPolicy - Defines the expiry of a Cache. Entry based on when it was last touched. A touch includes creation, update or, access.
5. EternalExpiryPolicy - Specifies that Cache Entries won't expire. This, however, doesn't mean they won't be evicted if an underlying implementation needs to free-up resources whereby it may choose to evict entries that are not due to expire.
6. CustomExpiryPolicy - Implements javax.cache.expiry interface, which defines functions to determine when cache entries will, expire based on creation, access and modification operations.

Eviction:

Usually, caches are unbounded, i.e. they grow indefinitely and it is up to the application to removed unneeded cache entries. A cache eviction algorithm is a way of deciding which entries to evict (removed) when the cache is full. However, when maximum on-heap memory is full, entries are evicted into the off-heap memory, if one is enabled. Some eviction policies support batch eviction and eviction by memory size limit. If a batch eviction is enabled then eviction starts when cache size (batchSize) is greater than the maximum cache size. In this cases, batchSize entries will be evicted. If eviction by memory size limit is enabled, then eviction starts when the size of cache entries in bytes becomes greater than the maximum memory size.

1. LRU: This eviction policy is based on LRU algorithm. The oldest element is the Less Recently Used (LRU) element gets evicted first. The last used timestamp is updated when an element is put into the cache, or an element is retrieved from the cache with a get call. This algorithm takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This eviction policy supports batch eviction and eviction by memory size limits. This eviction policy is suitable for most of all applications and recommended by Apache Ignite.
2. FIFO: Elements are evicted in the same order as they come in. When a put call is made for a new element (and assuming that the max limit is reached for the memory store), the element that was placed first (First-In) in the store is the candidate for eviction (First-Out). This algorithm is used if the use of an element makes it less likely to be used in the future. An example here would be an authentication cache. It takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This implementation is very efficient since it does not create any additional table-like data structures. The ordering information is maintained by attaching ordering metadata to cache entries. This eviction policy supports batch eviction and eviction by memory size limit.
3. Sorted: Sorted eviction policy is similar to FIFO eviction policy with the difference that the entries order is defined by default or user defined comparator and ensures that the minimal entry (i.e. the entry that has integer key with the smallest value) gets evicted first. Default comparator uses cache entries keys for comparison that imposes a requirement for keys to implementing Comparable interface. The user can provide own comparator implementation which can use keys, values or both for entries comparison. Supports batch eviction and eviction by memory size limit.
4. Random: This cache eviction policy selects random cache entry for eviction if cache size exceeds the getMaxSize parameter. This implementation is extremely light weight, lock-free, and does not create any data structures to maintain any order for eviction. Random eviction will provide the best performance over any key queue in which every key has the same probability of being accessed. This eviction policy implementation doesn't support near cache and doesn't work on client nodes. This eviction policy is mainly used for debugging and benchmarking purposes. 


### Discovery and communication mechanisms

Any distributed system that scales linearly has at least two mechanisms: one for communication with each other, and another to discover other nodes in the cluster. These two mechanisms or techniques are the backbone of any cluster that scale-out horizontally when needed. Also, they are responsible for forming the cluster, adding new nodes, handling failures or passing messages to the nodes.

Discovery mechanism features:

1. Connect a new node to the cluster topology.
2. Disconnect a node from the cluster.
3. Maintains the order in which nodes connected/disconnected to or from the cluster.
4. Ability to send user messages through the cluster.
5. Ability to set an authenticator that will validate the connected nodes


Ignite provides DiscoverySpi Java interface that allows discovering remote nodes in the cluster. Moreover, Ignite provides two specific DiscoverySpi implementations: TcpDiscoverySpi and ZookeeperDiscoverySpi for different scenarios.

1. TCP/IP discovery: is the default implementation of the Ignite DiscoverySpi interface that allows all nodes (with enabling multicast) to discover each other inside the same network. This specific implementation uses TCP/IP for node discovery, designed, and optimized for 10s and 100-300 of nodes deployment. When using this implementation Ignite forms a ring- shaped topology. So, almost all network exchanges (except few cases) is done through it. TcpDiscoverySpi uses IP finder (TcpDiscoveryIpFinder) to share and store information about nodes IP addresses. At startup, TcpDiscoverySpi tries to send messages to random IP address taken from the TcpDiscoveryIpFinder about self-start.TcpDiscoverySpi uses local port range to discover the nodes by default. The default local port range is 100. It is not necessary to open the entire range of discovery port in the range from 47500 to 47600 in each member of the Ignite cluster.Note that you are only required to provide at least one IP address of a remote node with the TcpDiscoveryVmIpFinder, but usually, it is advisable to provide 2 or 3 addresses of grid nodes that you plan to start at some point of time in the future. Ignite will automatically discover all other grid nodes once a connection to any of the provided IP addresses is established. The TcpDiscoveryVmIpFinder uses in non-shared mode by default. The list of IP addresses should contain an address of the local node as well if you plan to start a server node in this mode. It will let the node not to wait while other nodes join the cluster but instead it becomes the first cluster node and operate usually. Otherwise, you might get into the situation like that, where one node waits for the other nodes while joining to the cluster. However, you can use the combination of both Multicast and Static IP based discovery together.The Apache Ignite TCP/IP Discovery SPI has a few major drawbacks. The transmission time of the messages between nodes is directly proportional to the number of the nodes in the cluster. It means that an Ignite cluster with more than 100 nodes may take a few more seconds for a system message to traverse through the cluster events such as joining of a new node or detecting a split-brain situation can take a while, which can affect the overall performance of the cluster. Therefore, this implementation is not an optimal solution for a large cluster
2. ZooKeeper discovery: It’s proposed to move from the ring topology to the star topology to overcome the above disadvantages, in the center of which the Zookeeper⁵⁹ service is used. At the same time, the Zookeeper cluster appears as the connection and synchronization point for the Ignite cluster. Such an implementation allows scaling Ignite cluster to 100s and 1000s of nodes preserving linear scalability and performance.The basic idea behind the discovery service through Zookeeper is that for each node to be able to identify its current state and store that information into the centralized place. Primary usages of such storage are to provide as a minimum, IP and Port number of the node to all interested nodes that might need to communicate with it. This data is often extended with other types of the data such as sequence order of a node that how it connected to the cluster. ZooKeeper cluster is used as primary storage of information for
the current topology states. It also stores attributes of the nodes (user-defined attributes),the order that a node connected to the cluster, queues for storing user-defined events. All the messages about topology exchanges through Zookeeper, nodes are not communicated directly with each other. So, ZooKeeper cluster is used as primary storage of information for the current topology states in this implementation. Also, it stores attributes of the nodes (user-defined attributes), the order that a node connected to the cluster, queues for storing user-defined events. All the messages about topology exchanges through Zookeeper, nodes are not communicated directly with each other.Another essential functionality of ZooKeeper is the mechanism of notifying clients about changes. ZooKeeper allows you to store arbitrary, client information directly in the service. The recorded information can be accessed by all ZooKeeper clients once it save. ZooKeeper also provides opportunities to handle the split-brain scenario and network segmentation.

Communication:
Besides to discover nodes in a cluster, there are still needs for some direct communication between nodes for sending and receiving messages such as task execution, monitoring partition exchanges, etc. Ignite provides a CommunicationSpi which is responsible for peer- to-peer communication and data exchanges between nodes. Ignite CommunicationSpi is one of the most crucial SPI in Ignite. It is used heavily throughout the system and provides means for all data exchanges between nodes, such as internal implementation details and user-driven messages.Ignite comes with a built-in CommunicationSpi implementation: TcpCommunicationSpi, which uses TCP/IP protocols and Java NIO to communicate with other nodes.TcpCommunicationSpi uses IP address and port of the local node attributes to communicate
with other nodes. At startup, this SPI tries to start listening to a local port specified by the configuration. SPI will automatically increment the port number until it can successfully bind for listening if the local port is occupied. TcpCommunicationSpi caches connections to remote nodes, so it does not have to reconnect every time a message is sent. Idle connections are kept active for 10 minutes by default, and they are closed. You can configure the idle connection timeout by the setIdleConnectionTimeout parameter.


### Client Connectors Variety
- [Ignite Client connectors](https://medium.com/swlh/apache-ignite-client-connectors-variety-41aed7c12361#:~:text=If%20you%20have%20worked%20with,might%20get%20a%20little%20confusing)