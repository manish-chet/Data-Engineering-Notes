
### Expiration & Eviction of Cache Entries in Ignite

Apache Ignite caches are very powerful and can be configured and tuned to suit the needs of most applications. Apache Ignite provides two different approaches for data refreshing, which refers to an aspect of a copy of data (e.g,. entries in a cache) being up-to-date with the source version of the data. A stale copy of the data is considered to be out of use. Expiration and Evictions of cache entries is one of the key aspects of caching.

Expiration:

Usually, the purpose of a cache is to store short-lived data that needs to refresh regularly. You can use Apache Ignite expiry policy to store entry only for a certain period of time. Once expired, the entry is automatically removed from the cache. For instance, the cache could be configured to expire entries ten seconds after they are put in. Sometimes, it is called Time-to-live or TTL. Or to expire 20 seconds after the last time the entry was accessed or retrieve from the cache. Apache Ignite provides five differents predefined expire policy as follows:

1. CreatedExpiryPolicy - Defines the expiry of a Cache Entry based on when it was created. An update does not reset the expiry time
2. AccessedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was accessed. Accessed does not include a cache update.
3. ModifiedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was updated. Updating includes created and changing (updating) an entry.
4. TouchedExpiryPolicy - Defines the expiry of a Cache. Entry based on when it was last touched. A touch includes creation, update or, access.
5. EternalExpiryPolicy - Specifies that Cache Entries won't expire. This, however, doesn't mean they won't be evicted if an underlying implementation needs to free-up resources whereby it may choose to evict entries that are not due to expire.
6. CustomExpiryPolicy - Implements javax.cache.expiry interface, which defines functions to determine when cache entries will, expire based on creation, access and modification operations.

Eviction:

Usually, caches are unbounded, i.e. they grow indefinitely and it is up to the application to removed unneeded cache entries. A cache eviction algorithm is a way of deciding which entries to evict (removed) when the cache is full. However, when maximum on-heap memory is full, entries are evicted into the off-heap memory, if one is enabled. Some eviction policies support batch eviction and eviction by memory size limit. If a batch eviction is enabled then eviction starts when cache size (batchSize) is greater than the maximum cache size. In this cases, batchSize entries will be evicted. If eviction by memory size limit is enabled, then eviction starts when the size of cache entries in bytes becomes greater than the maximum memory size.

1. LRU: This eviction policy is based on LRU algorithm. The oldest element is the Less Recently Used (LRU) element gets evicted first. The last used timestamp is updated when an element is put into the cache, or an element is retrieved from the cache with a get call. This algorithm takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This eviction policy supports batch eviction and eviction by memory size limits. This eviction policy is suitable for most of all applications and recommended by Apache Ignite.
2. FIFO: Elements are evicted in the same order as they come in. When a put call is made for a new element (and assuming that the max limit is reached for the memory store), the element that was placed first (First-In) in the store is the candidate for eviction (First-Out). This algorithm is used if the use of an element makes it less likely to be used in the future. An example here would be an authentication cache. It takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This implementation is very efficient since it does not create any additional table-like data structures. The ordering information is maintained by attaching ordering metadata to cache entries. This eviction policy supports batch eviction and eviction by memory size limit.
3. Sorted: Sorted eviction policy is similar to FIFO eviction policy with the difference that the entries order is defined by default or user defined comparator and ensures that the minimal entry (i.e. the entry that has integer key with the smallest value) gets evicted first. Default comparator uses cache entries keys for comparison that imposes a requirement for keys to implementing Comparable interface. The user can provide own comparator implementation which can use keys, values or both for entries comparison. Supports batch eviction and eviction by memory size limit.
4. Random: This cache eviction policy selects random cache entry for eviction if cache size exceeds the getMaxSize parameter. This implementation is extremely light weight, lock-free, and does not create any data structures to maintain any order for eviction. Random eviction will provide the best performance over any key queue in which every key has the same probability of being accessed. This eviction policy implementation doesn't support near cache and doesn't work on client nodes. This eviction policy is mainly used for debugging and benchmarking purposes. 