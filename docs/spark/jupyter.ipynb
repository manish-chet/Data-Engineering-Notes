{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d431b54d",
   "metadata": {},
   "source": [
    "## **Import all the functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6edaaa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#Importing all Required Packages\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "import warnings\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import Window\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b355a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# Setting Up the Configurations for pySpark\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "conf = (pyspark.SparkConf().set('spark.driver.memory', '10G')\\\n",
    "        .set('spark.executor.memory', '10G')\\\n",
    "        .set('spark.driver.maxResultSize', '10G')\\\n",
    "        .set('spark.dynamicAllocation.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocaton.maxExecutors', '4')\\\n",
    "        .set('spark.master','local[4]'))\\\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# Fetching or Creating a Session for this Activity\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder.config(conf = conf).appName(\"pyspark2\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe73fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.129.130.96:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f720515f520>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321042e",
   "metadata": {},
   "source": [
    "## **Spark read modes**\n",
    "\n",
    "When reading data (e.g., from CSV, JSON, etc.) in Apache Spark using DataFrameReader, you can specify how Spark should handle malformed or corrupted records using the mode option. \n",
    "\n",
    "Below are the available modes:\n",
    "\n",
    "\n",
    "| Mode           | Description                                                                 |\n",
    "|----------------|-----------------------------------------------------------------------------|\n",
    "| failFast   | Terminates the query immediately if any malformed record is encountered. This is useful when data integrity is critical. |\n",
    "| dropMalformed | Drops all rows containing malformed records. This can be useful when you prefer to skip bad data instead of failing the entire job. |\n",
    "| permissive (default) | Tries to parse all records. If a record is corrupted or missing fields, Spark sets `null` values for corrupted fields and puts malformed data into a special column named `_corrupt_record`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83c4ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 12:17:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/06 12:17:42 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|                 _c0|                _c1|  _c2|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#Read dataframe with header false\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "flight_df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"inferschema\",\"false\")\\\n",
    "                .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/flightdata.csv\") \n",
    "flight_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3d8a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#Read dataframe with header true\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "flight_df2 = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"false\")\\\n",
    "                .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/flightdata.csv\") \n",
    "flight_df2.show()\n",
    "flight_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a764ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#Read dataframe with inferschema true\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "flight_df3 = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/flightdata.csv\") \n",
    "flight_df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da721124",
   "metadata": {},
   "source": [
    "## **Schema in spark**\n",
    "\n",
    "There are two primary methods: using StructType and StructField classes, and using a DDL (Data Definition Language) string.\n",
    "\n",
    "These are classes in Spark used to define the schema structure.\n",
    "\n",
    "StructField represents a single column within a DataFrame. It holds information such as the column's name, its data type (e.g., String, Integer, Timestamp), and whether it can contain null values (nullable: True/False). If nullable is set to False, the column cannot contain NULL values, and an error will be thrown if it does.\n",
    "\n",
    "StructType defines the overall structure of a DataFrame. It is essentially a list or collection of StructField objects. When you combine ID, Name, and Age fields, for example, they define the structure of a record, and a collection of such records forms a DataFrame.\n",
    "\n",
    "What happens if you set header=False when your data actually has a header? If you disable the header option (header=False) but your CSV file contains a header row, Spark will treat that header row as regular data. If this header row's values do not match the data types defined in your manual schema (e.g., a string \"Count\" being read into an Integer column), it can lead to null values in that column if the read mode is set to permissive, or an error if the mode is failfast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757c9074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "# Define the schema using StructType and StructField\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "my_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),  \n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),  \n",
    "    StructField(\"count\", IntegerType(), True)  \n",
    "])\n",
    "my_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87eab594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#Read dataframe with inferschema true\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "flight_df4 = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"false\")\\\n",
    "                .option(\"skipRows\",1)\\\n",
    "                .schema(my_schema)\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/flightdata.csv\") \n",
    "\n",
    "flight_df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0592c3d",
   "metadata": {},
   "source": [
    "## **Handling corrupted records**\n",
    "\n",
    "\n",
    "When reading data, Spark offers different modes to handle corrupted records, which influence how the DataFrame is populated.\n",
    "\n",
    "In permissive mode, all records are allowed to enter the DataFrame. If a record is corrupted, Spark sets the malformed values to null and does not throw an error. For the example data with five total records (two corrupted), permissive mode will result in five records in the DataFrame, with nulls where data is bad.\n",
    "\n",
    "In dropMalformed mode, Spark discards any record it identifies as corrupted. Given the example data has two corrupted records out of five, this mode will produce a DataFrame with three records\n",
    "\n",
    "In failfast mode, Spark immediately throws an error and stops the job as soon as it encounters the first corrupted record. This mode will result in zero records in the DataFrame because the job will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53849538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employee = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/employee.csv\") \n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3bee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+\n",
      "| id|  name|age|salary|     address| nominee|\n",
      "+---+------+---+------+------------+--------+\n",
      "|  1|Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  5|Vikash| 31|300000|        NULL|nominee5|\n",
      "+---+------+---+------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employee1 = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"dropmalformed\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/employee.csv\") \n",
    "\n",
    "employee1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f7eb8",
   "metadata": {},
   "source": [
    "## **Print bad records**\n",
    "\n",
    "\n",
    "To specifically identify and view the corrupted records, you need to define a manual schema that includes a special column named _corrupt_record. This column will capture the raw content of the corrupted record.\n",
    "\n",
    "Where to store bad record For scenarios with a large volume of corrupted records (e.g., thousands), printing them is not practical. Spark provides the badRecordsPath option to store all corrupted records in a specified location. These records are saved in JSON format at the designated path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb79f7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('salary', IntegerType(), True), StructField('address', StringType(), True), StructField('nominee', StringType(), True), StructField('_corrupt_record', StringType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empschema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),  \n",
    "    StructField(\"name\", StringType(), True),  \n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True), \n",
    "    StructField(\"address\", StringType(), True), \n",
    "    StructField(\"nominee\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True),\n",
    "])\n",
    "empschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e63de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "| id|    name|age|salary|     address| nominee|     _corrupt_record|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|                NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|                NULL|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|id |name    |age|salary|address     |nominee |_corrupt_record                            |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|1  |Manish  |26 |75000 |bihar       |nominee1|NULL                                       |\n",
      "|2  |Nikita  |23 |100000|uttarpradesh|nominee2|NULL                                       |\n",
      "|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n",
      "|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n",
      "|5  |Vikash  |31 |300000|NULL        |nominee5|NULL                                       |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employee3 = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .schema(empschema)\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/employee.csv\") \n",
    "\n",
    "employee3.show()\n",
    "employee3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1688a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "| id|    name|age|salary|     address| nominee|     _corrupt_record|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|                NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|                NULL|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|id |name    |age|salary|address     |nominee |_corrupt_record                            |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|1  |Manish  |26 |75000 |bihar       |nominee1|NULL                                       |\n",
      "|2  |Nikita  |23 |100000|uttarpradesh|nominee2|NULL                                       |\n",
      "|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n",
      "|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n",
      "|5  |Vikash  |31 |300000|NULL        |nominee5|NULL                                       |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employee3 = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .schema(empschema)\\\n",
    "                .option(\"badRecordsPath\",\"/sandbox/DataEngineering/manish/pyspark/badrecords\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/employee.csv\") \n",
    "\n",
    "employee3.show()\n",
    "employee3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f8a6f",
   "metadata": {},
   "source": [
    "## **Read json in spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f6c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json = spark.read.format(\"json\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/line_delimeted.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a21cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+\n",
      "|age|gender|    name|salary|\n",
      "+---+------+--------+------+\n",
      "| 20|  NULL|  Manish| 20000|\n",
      "| 25|  NULL|  Nikita| 21000|\n",
      "| 16|  NULL|  Pritam| 22000|\n",
      "| 35|  NULL|Prantosh| 25000|\n",
      "| 67|     M|  Vikash| 40000|\n",
      "+---+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json2 = spark.read.format(\"json\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/single_file.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "395d2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|age|  name|salary|\n",
      "+---+------+------+\n",
      "| 20|Manish| 20000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json3 = spark.read.format(\"json\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .option(\"multiline\",\"True\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/multiline.json\").show()\n",
    "\n",
    "json4 = spark.read.format(\"json\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .option(\"multiline\",\"True\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/corrupted_json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318c51f",
   "metadata": {},
   "source": [
    "## **Write dataframe**\n",
    "\n",
    "When working with Spark, after you have read data into a DataFrame and performed transformations, it is crucial to write the processed data back to disk to ensure its persistence. Currently, all the transformations and data processing occur in memory, so writing to disk makes the data permanent.\n",
    "Here's a detailed explanation with code examples and notes based on the provided sources:\n",
    "\n",
    "\n",
    "!!! Code\n",
    "\n",
    "        # Assuming 'df' is your DataFrame\n",
    "        # Define your base location where you want to save the output\n",
    "        base_location = \"/user/hive/warehouse/your_database/output/\"\n",
    "\n",
    "        # Construct the full path for the CSV output folder\n",
    "        output_path = base_location + \"csv_write/\"\n",
    "\n",
    "        # Write the DataFrame to disk\n",
    "        df.write \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(output_path)\n",
    "\n",
    "\n",
    "General Structure for Writing a DataFrame to Disk\n",
    "\n",
    "The general structure for writing a DataFrame using the Spark DataFrame Writer API is as follows:\n",
    "\n",
    "DataFrame.write: This is the starting point, indicating that you intend to write a DataFrame.\n",
    "\n",
    "   .format(): Specifies the file format in which you want to save the data. Common formats include CSV or Parquet. If no format is explicitly passed, Spark will default to Parquet.\n",
    "   \n",
    "   .option(): Allows you to pass multiple options. For example, you can specify whether to include a header for CSV files (e.g., header as True). You can also specify the output path using path option, though save() method usually handles the path directly.\n",
    "\n",
    "   .mode(): Defines how Spark should behave if files or directories already exist at the target location. This is a very important aspect of writing data.\n",
    "   \n",
    "   .partitionBy(): (To be covered in a dedicated video mentioned in the source) This method allows you to partition the output data based on one or more columns, creating separate folders for each partition.\n",
    "\n",
    "   .bucketBy(): (To be covered in a dedicated video mentioned in the source) Similar to partitionBy, but it organizes data into a fixed number of buckets within partitions.\n",
    "\n",
    "   .save(): This is the final action that triggers the write operation and specifies the output path where the DataFrame will be written.\n",
    "\n",
    "A typical flow looks like: **df.write.format(...).option(...).mode(...).save(path)**.\n",
    "\n",
    "**Modes in DataFrame Writer API**\n",
    "\n",
    "The mode() method in the DataFrame Writer API is crucial as it dictates how Spark handles existing data at the target location. There are four primary modes:\n",
    "\n",
    "**append**\n",
    " \n",
    " Functionality: If files already exist at the specified location, the new data from the DataFrame will be added to the existing files.\n",
    "\n",
    " Example: If there were three files previously, and a new output DataFrame comes, it will simply append its data to that list of files.\n",
    "\n",
    "**overwrite**\n",
    "\n",
    " Functionality: This mode deletes any existing files at the target location before writing the new DataFrame.\n",
    "\n",
    " Example: If a previous file had records, overwrite will delete all old files and only the new file with its records (e.g., five new records) will be visible.\n",
    "\n",
    "**errorIfExists**\n",
    "\n",
    " Functionality: Spark will check if a file or location already exists at the target path. If it does, the write operation will fail and throw an error.\n",
    "\n",
    " Use Case: Useful when you want to ensure that you do not accidentally overwrite or append to existing data.\n",
    "\n",
    "**ignore**\n",
    "\n",
    " Functionality: If a file or location already exists at the target path, Spark will skip the write operation entirely without throwing an error. The new file will not be written.\n",
    "\n",
    " Use Case: This mode is suitable if you want to prevent new data from being written if data is already present, perhaps to avoid overwriting changes or to ensure data integrity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fbc06",
   "metadata": {},
   "source": [
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"overwrite\") \\\n",
    "    .option(\"path\", \"/sandbox/DataEngineering/manish/pyspark\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a7731",
   "metadata": {},
   "source": [
    "## **Paritition in spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d36c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/part.csv\") \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae3fd2",
   "metadata": {},
   "source": [
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"overwrite\") \\\n",
    "    .option(\"path\", \"/sandbox/DataEngineering/manish/pyspark/partbyid/\") \\\n",
    "    .partitionBy(\"address\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2701e5",
   "metadata": {},
   "source": [
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"overwrite\") \\\n",
    "    .option(\"path\", \"/sandbox/DataEngineering/manish/pyspark/partition/\") \\\n",
    "    .partitionBy(\"address\",\"gender\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01134c",
   "metadata": {},
   "source": [
    "## **Bucketing in pyspark**\n",
    "\n",
    "save doesnt work, need to use saveAsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd139c",
   "metadata": {},
   "source": [
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"overwrite\") \\\n",
    "    .option(\"path\", \"/sandbox/DataEngineering/manish/pyspark/bucket/\") \\\n",
    "    .bucketBy(3,\"id\")\\\n",
    "    .saveAsTable(\"bucketbyid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6391a9",
   "metadata": {},
   "source": [
    "## **Column selection and expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae98414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'age', 'salary', 'address', 'gender']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empdf = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/part.csv\") \n",
    "\n",
    "empdf.show()\n",
    "empdf.printSchema()\n",
    "empdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46bcba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 26|\n",
      "| 23|\n",
      "| 22|\n",
      "| 17|\n",
      "| 31|\n",
      "| 55|\n",
      "| 67|\n",
      "| 28|\n",
      "| 32|\n",
      "| 16|\n",
      "| 12|\n",
      "| 43|\n",
      "| 48|\n",
      "| 36|\n",
      "| 52|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d074b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  Manish|\n",
      "|  Nikita|\n",
      "|  Pritam|\n",
      "|Prantosh|\n",
      "|  Vikash|\n",
      "|   Rahul|\n",
      "|    Raju|\n",
      "| Praveen|\n",
      "|     Dev|\n",
      "|  Sherin|\n",
      "|    Ragu|\n",
      "|   Sweta|\n",
      "| Raushan|\n",
      "|  Mukesh|\n",
      "| Prakash|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select(col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f7098e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id + 5)|\n",
      "+--------+\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "|      13|\n",
      "|      14|\n",
      "|      15|\n",
      "|      16|\n",
      "|      17|\n",
      "|      18|\n",
      "|      19|\n",
      "|      20|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empdf.select(col(\"id\")+5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "853c832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|  Manish| 26|\n",
      "|  2|  Nikita| 23|\n",
      "|  3|  Pritam| 22|\n",
      "|  4|Prantosh| 17|\n",
      "|  5|  Vikash| 31|\n",
      "|  6|   Rahul| 55|\n",
      "|  7|    Raju| 67|\n",
      "|  8| Praveen| 28|\n",
      "|  9|     Dev| 32|\n",
      "| 10|  Sherin| 16|\n",
      "| 11|    Ragu| 12|\n",
      "| 12|   Sweta| 43|\n",
      "| 13| Raushan| 48|\n",
      "| 14|  Mukesh| 36|\n",
      "| 15| Prakash| 52|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select(\"id\",\"name\",\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "596976ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Manish|\n",
      "|  2|  Nikita|\n",
      "|  3|  Pritam|\n",
      "|  4|Prantosh|\n",
      "|  5|  Vikash|\n",
      "|  6|   Rahul|\n",
      "|  7|    Raju|\n",
      "|  8| Praveen|\n",
      "|  9|     Dev|\n",
      "| 10|  Sherin|\n",
      "| 11|    Ragu|\n",
      "| 12|   Sweta|\n",
      "| 13| Raushan|\n",
      "| 14|  Mukesh|\n",
      "| 15| Prakash|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select(col(\"id\"),col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "537da58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-------+\n",
      "| id|    name|salary|address|\n",
      "+---+--------+------+-------+\n",
      "|  1|  Manish| 75000|  INDIA|\n",
      "|  2|  Nikita|100000|    USA|\n",
      "|  3|  Pritam|150000|  INDIA|\n",
      "|  4|Prantosh|200000|  JAPAN|\n",
      "|  5|  Vikash|300000|    USA|\n",
      "|  6|   Rahul|300000|  INDIA|\n",
      "|  7|    Raju|540000|    USA|\n",
      "|  8| Praveen| 70000|  JAPAN|\n",
      "|  9|     Dev|150000|  JAPAN|\n",
      "| 10|  Sherin| 25000| RUSSIA|\n",
      "| 11|    Ragu| 35000|  INDIA|\n",
      "| 12|   Sweta|200000|  INDIA|\n",
      "| 13| Raushan|650000|    USA|\n",
      "| 14|  Mukesh| 95000| RUSSIA|\n",
      "| 15| Prakash|750000|  INDIA|\n",
      "+---+--------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empdf.select(\"id\",col(\"name\"),empdf['salary'],empdf.address).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd5ae712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id + 5)|\n",
      "+--------+\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "|      13|\n",
      "|      14|\n",
      "|      15|\n",
      "|      16|\n",
      "|      17|\n",
      "|      18|\n",
      "|      19|\n",
      "|      20|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select(expr(\"id +5\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee637830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------------------------------------------------------------+\n",
      "|emp_id|emp_name|concat(lateralAliasReference(emp_id),  , lateralAliasReference(emp_name))|\n",
      "+------+--------+-------------------------------------------------------------------------+\n",
      "|     1|  Manish|                                                                 1 Manish|\n",
      "|     2|  Nikita|                                                                 2 Nikita|\n",
      "|     3|  Pritam|                                                                 3 Pritam|\n",
      "|     4|Prantosh|                                                               4 Prantosh|\n",
      "|     5|  Vikash|                                                                 5 Vikash|\n",
      "|     6|   Rahul|                                                                  6 Rahul|\n",
      "|     7|    Raju|                                                                   7 Raju|\n",
      "|     8| Praveen|                                                                8 Praveen|\n",
      "|     9|     Dev|                                                                    9 Dev|\n",
      "|    10|  Sherin|                                                                10 Sherin|\n",
      "|    11|    Ragu|                                                                  11 Ragu|\n",
      "|    12|   Sweta|                                                                 12 Sweta|\n",
      "|    13| Raushan|                                                               13 Raushan|\n",
      "|    14|  Mukesh|                                                                14 Mukesh|\n",
      "|    15| Prakash|                                                               15 Prakash|\n",
      "+------+--------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select(expr(\"id as emp_id\"),expr(\"name as emp_name\"), expr(\"concat(emp_id, ' ', emp_name)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e066f8a",
   "metadata": {},
   "source": [
    "## **Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e3edba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "empdf.createOrReplaceTempView('employeetable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d29cbc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqldf = spark.sql(\"\"\" select * from employeetable \"\"\")\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e10d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqldf.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4334e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+\n",
      "|empid|    name|age|\n",
      "+-----+--------+---+\n",
      "|    1|  Manish| 26|\n",
      "|    2|  Nikita| 23|\n",
      "|    3|  Pritam| 22|\n",
      "|    4|Prantosh| 17|\n",
      "|    5|  Vikash| 31|\n",
      "|    6|   Rahul| 55|\n",
      "|    7|    Raju| 67|\n",
      "|    8| Praveen| 28|\n",
      "|    9|     Dev| 32|\n",
      "|   10|  Sherin| 16|\n",
      "|   11|    Ragu| 12|\n",
      "|   12|   Sweta| 43|\n",
      "|   13| Raushan| 48|\n",
      "|   14|  Mukesh| 36|\n",
      "|   15| Prakash| 52|\n",
      "+-----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.select(col(\"id\").alias(\"empid\"),\"name\",\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36bc6ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n",
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.filter(col(\"salary\")>150000).show()\n",
    "empdf.where(col(\"salary\")>150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "330936db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.filter((col(\"salary\")>150000) & (col(\"age\") < 18)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "299fe414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+--------+\n",
      "| id|    name|age|salary|address|gender|lastname|\n",
      "+---+--------+---+------+-------+------+--------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|   kumar|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|   kumar|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|   kumar|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|   kumar|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|   kumar|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|   kumar|\n",
      "|  7|    Raju| 67|540000|    USA|     m|   kumar|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|   kumar|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|   kumar|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|   kumar|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|   kumar|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|   kumar|\n",
      "| 13| Raushan| 48|650000|    USA|     m|   kumar|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|   kumar|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|   kumar|\n",
      "+---+--------+---+------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# literal with aliasing\n",
    "empdf.select(\"*\",lit(\"kumar\").alias(\"lastname\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4e1ec",
   "metadata": {},
   "source": [
    "## **Withcolumn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "276333c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+---------+\n",
      "| id|    name|age|salary|address|gender|  surname|\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|chetpalli|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|chetpalli|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|chetpalli|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|chetpalli|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|chetpalli|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|chetpalli|\n",
      "|  7|    Raju| 67|540000|    USA|     m|chetpalli|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|chetpalli|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|chetpalli|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|chetpalli|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|chetpalli|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|chetpalli|\n",
      "| 13| Raushan| 48|650000|    USA|     m|chetpalli|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|chetpalli|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|chetpalli|\n",
      "+---+--------+---+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.withColumn(\"surname\",lit(\"chetpalli\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ff04616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------+------+\n",
      "|empid|    name|age|salary|address|gender|\n",
      "+-----+--------+---+------+-------+------+\n",
      "|    1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|    2|  Nikita| 23|100000|    USA|     f|\n",
      "|    3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|    4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|    5|  Vikash| 31|300000|    USA|     m|\n",
      "|    6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|    7|    Raju| 67|540000|    USA|     m|\n",
      "|    8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|    9|     Dev| 32|150000|  JAPAN|     m|\n",
      "|   10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "|   11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "|   12|   Sweta| 43|200000|  INDIA|     f|\n",
      "|   13| Raushan| 48|650000|    USA|     m|\n",
      "|   14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "|   15| Prakash| 52|750000|  INDIA|     m|\n",
      "+-----+--------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.withColumnRenamed(\"id\",\"empid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "925d58aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.withColumn(\"id\",col(\"id\").cast(\"string\"))\\\n",
    "     .withColumn(\"salary\",col(\"salary\").cast(\"long\"))\\\n",
    ".printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf33bffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+\n",
      "|age|salary|address|gender|\n",
      "+---+------+-------+------+\n",
      "| 26| 75000|  INDIA|     m|\n",
      "| 23|100000|    USA|     f|\n",
      "| 22|150000|  INDIA|     m|\n",
      "| 17|200000|  JAPAN|     m|\n",
      "| 31|300000|    USA|     m|\n",
      "| 55|300000|  INDIA|     m|\n",
      "| 67|540000|    USA|     m|\n",
      "| 28| 70000|  JAPAN|     m|\n",
      "| 32|150000|  JAPAN|     m|\n",
      "| 16| 25000| RUSSIA|     f|\n",
      "| 12| 35000|  INDIA|     f|\n",
      "| 43|200000|  INDIA|     f|\n",
      "| 48|650000|    USA|     m|\n",
      "| 36| 95000| RUSSIA|     m|\n",
      "| 52|750000|  INDIA|     m|\n",
      "+---+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dropdf = empdf.drop(\"id\",col(\"name\"))\n",
    "dropdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31988d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-------+------+\n",
      "| id|   name|age|salary|address|gender|\n",
      "+---+-------+---+------+-------+------+\n",
      "|  5| Vikash| 31|300000|    USA|     m|\n",
      "|  6|  Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|   Raju| 67|540000|    USA|     m|\n",
      "| 12|  Sweta| 43|200000|  INDIA|     f|\n",
      "| 13|Raushan| 48|650000|    USA|     m|\n",
      "| 15|Prakash| 52|750000|  INDIA|     m|\n",
      "+---+-------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqldf2 = spark.sql(\"\"\"\n",
    "    select * from employeetable where age>18 and salary >150000\n",
    "\"\"\")\n",
    "sqldf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbe33d",
   "metadata": {},
   "source": [
    "## **Union vs Unionall**\n",
    "\n",
    "The primary distinction between UNION and UNION ALL depends heavily on the context in which they are used: Spark DataFrames (PySpark) versus Spark SQL. This is a very common interview question.\n",
    "a. In Spark DataFrames (PySpark)\n",
    "\n",
    "• Behavior: When working with Spark DataFrames (e.g., using df.union() or df.unionAll()), UNION and UNION ALL \n",
    "behave identically. Both operations combine the records from two DataFrames without removing any duplicate rows\n",
    "\n",
    "The operations will simply append all records from the second DataFrame below the first one, irrespective of whether those records are duplicates of existing records in the first DataFrame or duplicates within the second DataFrame\n",
    "\n",
    "\n",
    "In Spark SQL (or Hive/SQL Context)\n",
    "\n",
    "• Behavior: This is where the crucial difference between UNION and UNION ALL manifests.\n",
    "    ◦ UNION: In Spark SQL, the UNION operator removes duplicate records. It first checks if a record already exists in the combined result set. If it finds an exact duplicate, it drops it, ensuring that only distinct records are returned.\n",
    "    ◦ UNION ALL: In contrast, UNION ALL in Spark SQL retains all records, including duplicates. It does not perform any duplicate checking or removal.\n",
    "\n",
    "• Explanation: This distinction is vital in SQL contexts because duplicate handling is a common requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de3c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema=['id','name','salary','managerid']\n",
    "\n",
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a151b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managerdf = spark.createDataFrame(data=data,schema=schema)\n",
    "managerdf.show()\n",
    "managerdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc1224a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---------+\n",
      "| id| name|salary|managerid|\n",
      "+---+-----+------+---------+\n",
      "| 19|Sohan| 50000|       18|\n",
      "| 20| Sima| 75000|       17|\n",
      "+---+-----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "managerdfnew = spark.createDataFrame(data=data1,schema=schema)\n",
    "managerdfnew.show()\n",
    "managerdfnew.count()\n",
    "managerdfnew.createOrReplaceTempView(\"managerdfnew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85aba039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 19| Sohan| 50000|       18|\n",
      "| 20|  Sima| 75000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managerdf.union(managerdfnew).show()\n",
    "managerdf.union(managerdfnew).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e451fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 19| Sohan| 50000|       18|\n",
      "| 20|  Sima| 75000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managerdf.unionAll(managerdfnew).show()\n",
    "managerdf.unionAll(managerdfnew).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbbc7b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 18|   Sam| 65000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicate=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "duplicatedf = spark.createDataFrame(data=duplicate,schema=schema)\n",
    "duplicatedf.show()\n",
    "duplicatedf.createOrReplaceTempView(\"duplicatetable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0454b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicatedf.union(managerdf).show()\n",
    "duplicatedf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca72d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicatedf.unionAll(managerdf).show()\n",
    "duplicatedf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da73f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 19| Sohan| 50000|       18|\n",
      "| 20|  Sima| 75000|       17|\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 17| Raman| 55000|       16|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from managerdfnew \n",
    "union\n",
    "select * from duplicatetable\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "select * from managerdfnew \n",
    "union\n",
    "select * from duplicatetable\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "350bb0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 19| Sohan| 50000|       18|\n",
      "| 20|  Sima| 75000|       17|\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 18|   Sam| 65000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from managerdfnew \n",
    "union all\n",
    "select * from duplicatetable\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select * from managerdfnew \n",
    "union all\n",
    "select * from duplicatetable\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c05f3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+-----+\n",
      "| id|salary|managerid| name|\n",
      "+---+------+---------+-----+\n",
      "| 19| 50000|       18|Sohan|\n",
      "| 20| 75000|       17| Sima|\n",
      "+---+------+---------+-----+\n",
      "\n",
      "+---+------+---------+-----+-----+\n",
      "| id|salary|managerid| name|bonus|\n",
      "+---+------+---------+-----+-----+\n",
      "| 19| 50000|       18|Sohan|   10|\n",
      "| 20| 75000|       17| Sima|   20|\n",
      "+---+------+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrong_column_data1=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "wrongschema=['id','salary','managerid','name']\n",
    "wrong_column_data2=[(19 ,50000, 18,'Sohan',10),\n",
    "(20 ,75000,  17,'Sima',20)]\n",
    "wrongschema2=['id','salary','managerid','name','bonus']\n",
    "wrong1 = spark.createDataFrame(data=wrong_column_data1,schema=wrongschema)\n",
    "wrong2 = spark.createDataFrame(data=wrong_column_data2,schema=wrongschema2)\n",
    "wrong1.show()\n",
    "wrong2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d8783b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 19| 50000|    18|    Sohan|\n",
      "| 20| 75000|    17|     Sima|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managerdf.union(wrong1).show()\n",
    "managerdf.union(wrong1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4321621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+\n",
      "| id|  name|salary|managerid|\n",
      "+---+------+------+---------+\n",
      "| 10|  Anil| 50000|       18|\n",
      "| 11| Vikas| 75000|       16|\n",
      "| 12| Nisha| 40000|       18|\n",
      "| 13| Nidhi| 60000|       17|\n",
      "| 14| Priya| 80000|       18|\n",
      "| 15| Mohit| 45000|       18|\n",
      "| 16|Rajesh| 90000|       10|\n",
      "| 17| Raman| 55000|       16|\n",
      "| 18|   Sam| 65000|       17|\n",
      "| 19| Sohan| 50000|       18|\n",
      "| 20|  Sima| 75000|       17|\n",
      "+---+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managerdf.unionByName(wrong1).show()\n",
    "managerdf.unionByName(wrong1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca979d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+-----+\n",
      "| id|salary|managerid| name|\n",
      "+---+------+---------+-----+\n",
      "| 19| 50000|       18|Sohan|\n",
      "| 20| 75000|       17| Sima|\n",
      "| 10|  Anil|    50000|   18|\n",
      "| 11| Vikas|    75000|   16|\n",
      "| 12| Nisha|    40000|   18|\n",
      "| 13| Nidhi|    60000|   17|\n",
      "| 14| Priya|    80000|   18|\n",
      "| 15| Mohit|    45000|   18|\n",
      "| 16|Rajesh|    90000|   10|\n",
      "| 17| Raman|    55000|   16|\n",
      "| 18|   Sam|    65000|   17|\n",
      "+---+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrong2.select('id','salary','managerid','name').union(managerdf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b0b7f",
   "metadata": {},
   "source": [
    "## **Repartition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea3896c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read dataframe with header false\n",
    "flight_df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/flightdata.csv\") \n",
    "\n",
    "flight_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17a64c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c77bb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33adc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "patitionflightdf = flight_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ee3b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   63|\n",
      "|          1|   64|\n",
      "|          2|   64|\n",
      "|          3|   64|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patitionflightdf.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fde204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paritiononcolumn =  flight_df.repartition(300,\"ORIGIN_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ac48514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paritiononcolumn.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67cf0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:============================================>        (252 + 4) / 300]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|    1|\n",
      "|          2|    1|\n",
      "|          7|    1|\n",
      "|         10|    1|\n",
      "|         13|    1|\n",
      "|         15|    2|\n",
      "|         16|    2|\n",
      "|         19|    1|\n",
      "|         21|    1|\n",
      "|         22|    1|\n",
      "|         28|    1|\n",
      "|         31|    1|\n",
      "|         39|    1|\n",
      "|         42|    1|\n",
      "|         43|    1|\n",
      "|         44|    1|\n",
      "|         45|    2|\n",
      "|         48|    1|\n",
      "|         53|    1|\n",
      "|         54|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paritiononcolumn.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38015708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----+\n",
      "|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+--------------------+-----+\n",
      "|    United States|          Cape Verde|   18|\n",
      "|    United States|            Anguilla|   20|\n",
      "|    United States|Saint Kitts and N...|  127|\n",
      "|    United States|    French Polynesia|   38|\n",
      "|    United States|              Cyprus|    1|\n",
      "|    United States|           Singapore|   25|\n",
      "|    United States|Bonaire, Sint Eus...|   16|\n",
      "|    United States|              Mexico| 6220|\n",
      "|    United States|                Fiji|   51|\n",
      "|    United States|             Estonia|    1|\n",
      "|    United States|Saint Vincent and...|   16|\n",
      "|    United States|             Germany| 1406|\n",
      "|    United States|Federated States ...|   48|\n",
      "|    United States|            Honduras|  393|\n",
      "|    United States|         Switzerland|  334|\n",
      "|    United States|            Slovakia|    1|\n",
      "|    United States|             Jamaica|  757|\n",
      "|    United States|United Arab Emirates|  156|\n",
      "|    United States|              Angola|   18|\n",
      "|    United States|         Saint Lucia|  121|\n",
      "+-----------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paritiononcolumn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a1d6f",
   "metadata": {},
   "source": [
    "## **Coalesce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2c46ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coalescedf = flight_df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "552417fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threecoalescedf = coalescedf.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e64a2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   64|\n",
      "|          1|   95|\n",
      "|          2|   96|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threecoalescedf.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "432431b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coalescedf2 = flight_df.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8e250a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   85|\n",
      "|          1|   85|\n",
      "|          2|   85|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coalescedf2.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c39230b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalescedf.coalesce(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c171ad",
   "metadata": {},
   "source": [
    "## **Case when & when otherwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05baa18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'age', 'salary', 'address', 'gender']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empdf = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .load(\"/sandbox/DataEngineering/manish/pyspark/part.csv\") \n",
    "\n",
    "empdf.show()\n",
    "empdf.printSchema()\n",
    "empdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59ae9162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+-----+\n",
      "| id|    name|age|salary|address|gender|adult|\n",
      "+---+--------+---+------+-------+------+-----+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|  yes|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|  yes|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|  yes|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|   no|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|  yes|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|  yes|\n",
      "|  7|    Raju| 67|540000|    USA|     m|  yes|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|  yes|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|  yes|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|   no|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|   no|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|  yes|\n",
      "| 13| Raushan| 48|650000|    USA|     m|  yes|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|  yes|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|  yes|\n",
      "+---+--------+---+------+-------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empdf.withColumn(\"adult\", when(col(\"age\") < 18,\"no\")\\\n",
    "                          .when(col(\"age\") > 18,\"yes\")\\\n",
    "                         .otherwise(\"novalue\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d63aa97a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+-----+\n",
      "| id|    name|age|salary|address|gender|adult|\n",
      "+---+--------+---+------+-------+------+-----+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|  yes|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|  yes|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|  yes|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|   no|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|  yes|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|  yes|\n",
      "|  7|    Raju| 67|540000|    USA|     m|  yes|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|  yes|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|  yes|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|   no|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|   no|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|  yes|\n",
      "| 13| Raushan| 48|650000|    USA|     m|  yes|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|  yes|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|  yes|\n",
      "+---+--------+---+------+-------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empdf2 = empdf.withColumn(\"age\", when(col(\"age\").isNull(),lit(19))\n",
    "              .otherwise(col(\"age\")))\\\n",
    "              .withColumn(\"adult\", when(col(\"age\") > 18,\"yes\")\\\n",
    "              .otherwise(\"no\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0c80cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 165:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+-------+\n",
      "| id|    name|age|salary|address|gender|agewise|\n",
      "+---+--------+---+------+-------+------+-------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|    mid|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|    mid|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|    mid|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|  minor|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|  major|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|  major|\n",
      "|  7|    Raju| 67|540000|    USA|     m|  major|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|    mid|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|  major|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|  minor|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|  minor|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|  major|\n",
      "| 13| Raushan| 48|650000|    USA|     m|  major|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|  major|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|  major|\n",
      "+---+--------+---+------+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empdf3 = empdf.withColumn(\"agewise\",when((col(\"age\")>0) & (col(\"age\")<18),\"minor\")\\\n",
    "                                    .when((col(\"age\")>18) & (col(\"age\")<30),\"mid\")\\\n",
    "                                    .otherwise(\"major\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "395dedbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+-----+\n",
      "| id|    name|age|salary|address|gender|Adult|\n",
      "+---+--------+---+------+-------+------+-----+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|  Mid|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|  Mid|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|  Mid|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|Minor|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|  Mid|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|  Mid|\n",
      "|  7|    Raju| 67|540000|    USA|     m|  Mid|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|  Mid|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|  Mid|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|Minor|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|Minor|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|  Mid|\n",
      "| 13| Raushan| 48|650000|    USA|     m|  Mid|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|  Mid|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|  Mid|\n",
      "+---+--------+---+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *,\n",
    "       case when age < 18 then 'Minor'\n",
    "            when age > 18 then 'Mid'\n",
    "            else 'Major'\n",
    "        end as Adult\n",
    "from \n",
    "    employeetable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fceff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (pyspark_env)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
