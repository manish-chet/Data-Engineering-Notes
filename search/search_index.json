{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>Hello! I'm Manish. This site is my personal hub for sharing projects, insights, and lessons learned from my journey as a Big Data Engineer.</p> <p>Whether you're a beginner or a seasoned professional, there's something to learn from each project.</p> <p>Quote</p> <p>Once a new technology rolls over you, if you\u2019re not part of the steamroller, you\u2019re part of the road - Stewart Brand</p> <p>Feel free to explore my website to discover more about my work and interests.</p>"},{"location":"about/","title":"Hey there! I'm Manish Chetpalli","text":"<p>Welcome to my GitHub space \u2014 a curated collection of my work, ideas, and explorations in Big Data, Cloud Engineering, and open-source development.</p> <p>I\u2019m a Big Data Engineer with hands-on experience in Hadoop, Spark, Kafka, Airflow, Python, and distributed systems. I love building scalable data pipelines and solving real-world problems with elegant tech solutions.</p> <p>I'm always open to:</p> <ul> <li>Talking tech</li> <li>Sharing ideas or insights</li> <li>Collaborating on open-source or data engineering projects</li> </ul> <p>Feel free to reach out \u2014 let's build something amazing together!</p> <ul> <li>LinkedIn</li> <li>GitHub</li> <li>Email</li> </ul> <p>\u2728 \u201cBuild systems that work. Then build systems that last.\u201d  </p>"},{"location":"DataEngineering/DE/","title":"Overview","text":""},{"location":"DataEngineering/DE/#data-engineering-defined","title":"Data Engineering Defined","text":"<p>Despite its popularity, there is much confusion about data engineering. It has existed in some form since companies started working with data, coming into sharp focus with the rise of data science in the 2010s.</p> <p>One definition states that data engineering is a set of operations aimed at creating interfaces and mechanisms for the flow and access of information. Data engineers maintain data, ensuring it remains available and usable, and they set up and operate an organization's data infrastructure for analysis by data analysts and scientists.</p> <p>Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high quality consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering is the intersection of security, data management, DataOps, data architecture, orchestration, and software engineering.</p> <p>Historically, there were two types of data engineering: SQL-focused (relational databases, SQL processing) and Big Data-focused (Hadoop, Cassandra, Spark, programming languages like Java, Scala, Python).</p> <p>Note</p> <p>A data engineer gets data, stores it, and prepares it for consumption by data scientists, analysts.</p> <p>\u2014 Fundamentals of Data Engineering (O\u2019Reilly)</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#the-data-engineering-lifecycle","title":"The Data Engineering Lifecycle","text":"<p>The five stages are: Generation, Storage, Ingestion, Transformation, and Serving.</p> <p></p> <p>Storage underpins other stages as it occurs throughout.</p> <p>Undercurrents are critical ideas that cut across the entire lifecycle: security, data management, DataOps, data architecture, orchestration, and software engineering.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#evolution-of-the-field","title":"Evolution of the Field","text":"<p>Big Data Era (2000s and 2010s): Coincided with data explosion and cheap commodity hardware.</p> <p>Big data is defined as \"extremely large data sets that may be analyzed computationally to reveal patterns, trends, and associations\". It's also characterized by the three Vs: velocity, variety, and volume.</p> <p>Google's papers on Google File System (2003) and MapReduce (2004) were a \"big bang\" for data technologies.This inspired Apache Hadoop (2006) at Yahoo, leading to the birth of the big data engineer.Amazon Web Services (AWS) emerged around the same time, offering elastic computing (EC2), scalable storage (S3), and NoSQL databases (DynamoDB), creating a pay-as-you-go resource marketplace.</p> <p>The era saw the rise of tools like Hadoop, Apache Pig, Apache Hive, Apache Storm, Apache Spark, and a shift from GUI-based tools to code-first engineering.</p> <p>The transition from batch computing to event streaming ushered in \"real-time\" big data.</p> <p>2020s: Engineering for the Data Lifecycle: The role is rapidly evolving towards decentralized, modularized, managed, and highly abstracted tools.</p> <p>Data engineers are becoming data lifecycle engineers, focusing on higher-value aspects like security, data management, DataOps, data architecture, orchestration, and general data lifecycle management, rather than low-level framework details.There's a shift towards managing and governing data, making it easier to use and discover, and improving its quality.</p> <p>Data engineers are now concerned with privacy, anonymization, data garbage collection, and compliance with regulations like CCPA and GDPR.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#relationship-with-data-science","title":"Relationship with Data Science","text":"<p>Data science often struggled with basic data problems (collection, cleansing, access, transformation, infrastructure) that data engineering aims to solve.</p> <p>Data scientists aren't typically trained for production-grade data systems. Data engineers build a solid foundation for data scientists to succeed, allowing them to focus on analytics, experimentation, and ML.</p> <p>Data engineering is of equal importance and visibility to data science, playing a vital role in its production success. The authors themselves moved from data science to data engineering due to this fundamental need.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#skills-and-activities","title":"Skills and Activities","text":"<p> The skillset encompasses the undercurrents: security, data management, DataOps, data architecture, and software engineering. It requires understanding how to evaluate data tools and their fit across the lifecycle, how data is produced, and how it's consumed.</p> <p>Data engineers balance cost, agility, scalability, simplicity, reuse, and interoperability. Historically, data engineers managed monolithic technologies; now, the focus is on high-level abstractions or writing pipelines as code within orchestration frameworks.</p> <p>Business Responsibilities: Communicate with technical and non-technical people, scope and gather requirements, understand business impact, and continuously learn.</p> <p>Technical Responsibilities: Build architectures optimizing performance and cost, understand the data engineering lifecycle stages (generation, storage, ingestion, transformation, serving) and undercurrents, and possess production-grade software engineering chops.</p> <p>SQL is a powerful tool for complex analytics and data transformation problems, essential for high productivity. Data engineers should also develop expertise in composing SQL with frameworks like Spark and Flink, or using orchestration.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#types-of-data-engineers","title":"Types of Data Engineers","text":"<p>Drawing from the \"type A\" (analysis) and \"type B\" (building) data scientists analogy:</p> <p>Type A Data Engineers: Focus on understanding and deriving insight from data.</p> <p>Type B Data Engineers: Share similar backgrounds but possess strong programming skills to build systems that make data science work in production.</p> <p>Internal-Facing vs. External-Facing Data Engineers: Serve various end users, with primary responsibilities being external, internal, or a blend.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#data-engineers-work-with-key-technical-stakeholders","title":"Data Engineers Work With (Key Technical Stakeholders)","text":"<p>Data engineers are a hub between data producers (software engineers, data architects, DevOps/SREs) and data consumers (data analysts, data scientists, ML engineers).</p> <p></p> <ul> <li> <p>Upstream Stakeholders:</p> <p>Data Architects: Design application data layers that serve as source systems and interact across other lifecycle stages. Data engineers should understand architecture best practices.</p> <p>Software Engineers: Build and maintain source systems; data engineers need to understand these systems and collaborate to make data production-ready.</p> <p>DevOps/Site-Reliability Engineers (SREs): Ensure systems are reliable and available; data engineers collaborate on deployment, monitoring, and incident response.</p> </li> <li> <p>Downstream Stakeholders:</p> <p>Data Analysts: Consume data for reports, dashboards, and ad hoc analysis. Data engineers ensure data quality and provide necessary datasets.</p> <p>Data Scientists: Build models; data engineers provide the data automation and scale for data science to be efficient and production-ready.</p> <p>ML Engineers: Similar to data scientists, they build and deploy ML models, relying on data engineers for robust data pipelines.</p> </li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#data-maturity-models","title":"Data Maturity Models","text":"<p>Stage 1: Starting with data</p> <p>Early stages, often small teams, data engineer acts as a generalist. Focus on defining the right data architecture, identifying and auditing data, and building a solid data foundation. Avoid jumping to ML without a foundation. Get quick wins (though they create technical debt), talk to people (avoid silos), and avoid undifferentiated heavy lifting.</p> <p>Stage 2: Scaling with data</p> <p>Company has some data, looking to expand usage. Focus on automating data flows, building systems for ML, and continuing to avoid undifferentiated heavy lifting. The main bottleneck is often the data engineering team, so focus on simple deployment/management. Shift to pragmatic leadership.</p> <p>Stage 3: Leading with data</p> <p>Data is a competitive advantage. Focus on automation for seamless data introduction, building custom tools for competitive advantage, \"enterprisey\" aspects like data management and DataOps, and deploying tools for data dissemination (catalogs, lineage). Efficient collaboration is key.</p> <p>Note</p> <p>\u201cData engineering is not about tools\u2014it's about building systems that let others derive value from data.\u201d</p>"},{"location":"DataEngineering/DEcycle/","title":"Data Engineering Lifecycle","text":"<p>The  data  engineering  lifecycle  comprises  stages  that  turn  raw  data  ingredients  into  a   useful end product, ready for consumption by analysts, data scientists, ML engineers,  and others.</p> <p></p> <p>The data engineering lifecycle is divided into five stages:</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#generation","title":"Generation","text":"<p>The origin of raw data in the data engineering lifecycle.  </p> <p>Examples: Transactional databases (RDBMS), Application message queues (e.g., Kafka), IoT devices or sensor swarms, Web/mobile apps, spreadsheets.</p> <p>Tip</p> <p>Key engineering considerations:</p> <ul> <li>What type is it? (App DB, IoT, etc.)</li> <li>How is data stored? (Persistent or temporary)</li> <li>How fast is data generated? (Events/sec or GB/hr)</li> <li>Are there data quality issues? (Nulls, bad formats)</li> <li>Can duplicates or late-arriving data occur?</li> <li>What\u2019s the schema structure? Does it evolve?</li> <li>How often is data pulled? (Real-time, hourly?)</li> <li>What\u2019s the data update method? (Snapshot or CDC)</li> <li>Who owns/provides the data?</li> <li>Will reading from it affect app performance?</li> <li>Are there upstream dependencies?  </li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#storage","title":"Storage","text":"<p>The place where data is kept during any stage of the lifecycle. Not just passive storage\u2014modern systems can process and query data too. Storage impacts ingestion, transformation, and serving stages.</p> <p></p> <p>Warning</p> <p>Why It\u2019s Complex</p> <ul> <li>Multiple storage types are often used together (e.g., cloud storage + data warehouses).</li> <li>Storage overlaps with other stages of the lifecycle (Kafka, S3, Snowflake).</li> <li>Some systems support storage + compute together (e.g., cloud data warehouses).</li> </ul> <p>Tip</p> <p>Key Considerations When Choosing Storage</p> <ul> <li>Performance: Are read/write speeds sufficient for your architecture? Will it bottleneck downstream jobs?</li> <li>Fit for Purpose: Are you misusing the storage (e.g., doing random writes in object storage)?</li> <li>Scalability: Can it handle future data volume, query load, and write throughput?</li> <li>SLA Support: Can it deliver data within required time frames for users and systems?</li> <li>Metadata &amp; Governance: Does it support schema evolution tracking, lineage, and data discovery?</li> <li>Capabilities: Pure storage (e.g., S3) vs. compute + storage (e.g., BigQuery, Snowflake)?</li> <li>Schema flexibility: Schema-less (S3), Flexible (Cassandra), Strict (Redshift, RDBMS)</li> <li>Compliance: Can it support data sovereignty and regulatory needs (e.g., GDPR, HIPAA)?</li> </ul> Type Description Storage Tip Hot Frequently accessed (e.g., per second) Store in fast-access systems (RAM, SSD) Warm Accessed occasionally (weekly/monthly) Moderate-speed, cost-efficient storage Cold Rarely accessed, kept for compliance Cheap storage, archival (e.g., Glacier) <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#ingestion","title":"Ingestion","text":"<p>Ingestion is the process of gathering data from source systems. It\u2019s often a major bottleneck due to:</p> <ul> <li>Unreliable source systems</li> <li>Ingestion service failures</li> <li>Poor data quality or availability</li> </ul> <p></p> <p>Tip</p> <p>Key Considerations When Ingesting data</p> <ul> <li>Can storage systems handle real-time load?</li> <li>Do you need real-time or will micro-batch (e.g., every minute) suffice?</li> <li>Does real-time ingestion add value (e.g., live decisions)?</li> <li>Will streaming increase cost/complexity over batch?</li> <li>Are streaming tools managed or self-hosted?</li> <li>Does the ML model benefit from online predictions?</li> </ul> <p></p> Type Description Use Cases Batch Collects data in intervals or chunks Analytics, ML training, reports Streaming Processes data in real-time (low latency) Real-time dashboards, alerts <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#transformation","title":"Transformation","text":"<p>The process of changing raw data into useful formats for analysis, reporting, or machine learning. This is where data starts creating business value.  </p> <p> Common early transformations: type casting, format standardization, and error removal. Later stages include: aggregations, normalization, featurization (for ML), and schema changes.</p> Type Description Example Use Cases Batch Processed in chunks Reporting, model training Streaming (in-flight) Transformed as it flows through a stream Real-time analytics, alerts <p></p> <p>Note</p> <p>Where Does Transformation Occur?</p> <ul> <li>In source systems (e.g., app adds timestamp before ingestion)</li> <li>During ingestion (e.g., enrich data in-stream)</li> <li>Post-ingestion in data warehouses/lakehouses</li> </ul> <p>Business Logic &amp; Data Modeling:</p> <ul> <li>Business logic makes data actionable and understandable.</li> <li>Example: Convert raw transactions to revenue using accounting logic.</li> <li>Consistency in applying business rules is critical.</li> </ul> <p>Featurization for ML:</p> <ul> <li>Extracting and engineering important features for model training.</li> <li>Combines domain knowledge and data science.</li> <li>Can be automated by data engineers in pipelines once defined.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#serving-data","title":"Serving data","text":"<p>Making transformed data available for practical use: analytics, dashboards, machine learning, or operational systems. Data has no value if it isn\u2019t used or consumed.</p> <p>Use Cases for Data Serving</p> <p>Analytics</p> <p></p> <p>Business Intelligence (BI): Reporting on past and current trends using business logic.</p> <p>Logic-on-read is becoming more common (business rules applied during querying, not transformation).</p> <p>Self-Service Analytics: Empowers non-technical users to explore data. Requires high data quality and organizational maturity.</p> <p>Operational Analytics: Real-time dashboards (e.g., app health, live inventory). Consumed immediately to trigger action.</p> <p>Embedded Analytics: Delivered to customers via apps (e.g., SaaS dashboards). Requires robust access control and multitenancy to prevent data leaks.</p> <p></p> <p>Machine Learning</p> <p></p> <p>Feature Stores: Systems that store and manage features for ML models.  </p> <p>Data Engineers may Maintain Spark clusters, orchestrate pipelines, and monitor metadata. Collaborate closely with ML and analytics engineers.</p> <p>ML maturity depends on solid data foundations\u2014master analytics first.</p> <p>Reverse ETL</p> <p></p> <p>Sends data back to source systems (e.g., CRMs, marketing platforms). Example: Push customer segments from a warehouse to Google Ads.</p> <p>Challenges: Must maintain lineage, business logic, and security. May be replaced by event-driven architectures, but remains practical today.</p>"},{"location":"DataEngineering/DEundercurrent/","title":"Data Engineering UnderCurrents","text":"<p>Data engineering is evolving beyond just tools and technology. Undercurrents are foundational practices that support the entire data engineering lifecycle. </p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#security-a-critical-undercurrent","title":"Security: A Critical Undercurrent","text":"<p>Only grant users/systems the minimum access necessary to perform their function. Avoid giving admin or superuser access unless strictly required. Prevents accidental damage and maintains a security-first mindset. Giving all users full admin access. Running commands with root privileges unnecessarily. Querying data with superuser roles when not needed.</p> <p>People and organizational behavior are the biggest vulnerabilities. Security breaches often stem from: Neglecting security protocols, Falling for phishing, Irresponsible behavior.</p> <p>Timely and Contextual Data Access: Data should be accessible: Only to the right people/systems, Only for the necessary time period. Data must be protected in transit and at rest using: Encryption, Tokenization, Data masking, Obfuscation, Access controls.</p> <p>Security is Continuous: Security is embedded across all phases of the data engineering lifecycle. Must be revisited and reinforced at every stage of data handling.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#data-management-importance-of-metadata-in-data-engineering","title":"Data Management: Importance of Metadata in Data Engineering","text":"<p>Metadata is not just technical\u2014it's social. Airbnb emphasized human-centric metadata via tools like Dataportal to capture data ownership and usability context.</p> <p></p> <p>Types of Metadata</p> <ul> <li>Business Metadata: Describes how data is used in business (definitions, rules, ownership).</li> <li>Technical Metadata: Covers schema, lineage, pipeline workflows, etc.</li> <li>Operational Metadata: Includes logs, stats, job run data\u2014useful for debugging and monitoring.</li> <li>Reference Metadata: Lookup values like codes and classifications (e.g., ISO country codes).</li> </ul> <p></p> <p>Data Accountability: Assign a responsible person (not necessarily a data engineer) for specific data artifacts (tables, fields, logs). Supports data quality by enabling someone to coordinate governance.</p> <p>Data Quality Characteristics:</p> <ul> <li>Accuracy: Data should be correct, deduplicated, and valid.</li> <li>Completeness: All required fields are filled with valid data. </li> <li>Timeliness: Data should be available at the right time for its intended use.</li> </ul> <p>Real-World Complexity: Accuracy and completeness are challenged by factors like bots or offline data submissions (e.g., ad views in video apps). Data engineers must define and enforce acceptable latency standards.</p> <p>Master Data Management (MDM): Centralized \"golden records\" for business entities (customers, products, etc.). Combines business policy with tech (like APIs) to enforce consistency across systems and partners.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#dataops","title":"DataOps","text":"<p>DataOps aims to improve the delivery, quality, and reliability of data products, just as DevOps does for software products. Data products, unlike software products, revolve around business logic, metrics, and decision-making processes. DataOps applies Agile principles, DevOps practices, and statistical process control (SPC) to the data engineering lifecycle to reduce time to value, improve data quality, and facilitate collaboration.</p> <p></p> <p>Key Aspects of DataOps:</p> <ul> <li>Cultural Practices: Data engineers need to foster a culture of communication, collaboration with the business, breaking silos, and continuous learning from both successes and failures.</li> <li>Automation: Automates processes like environment, code, and data version control, CI/CD, and configuration management. This ensures data workflows are reliable and consistent.</li> <li>Monitoring &amp; Observability: Ensures that data systems and transformations are constantly monitored, with alerting and logging in place to avoid errors or delays in reporting.</li> <li>Incident Response: Focuses on identifying and resolving issues rapidly, leveraging automation, monitoring, and observability tools, with a proactive, blameless communication culture.</li> </ul> <p>DataOps Lifecycle in an Organization:</p> <ul> <li>Low Maturity: A company may rely on cron jobs to schedule data transformation, which can lead to failures when jobs fail or take too long. Engineers often aren't aware of these failures until stakeholders report issues.</li> <li>Medium Maturity: Adoption of orchestration frameworks like Airflow helps automate dependencies and scheduling. However, challenges like broken DAGs may still occur, which necessitate automated DAG deployment and pre-deployment testing to prevent issues.</li> <li>High Maturity: Engineers continuously enhance automation, possibly introducing next-gen orchestration frameworks or frameworks for automatic DAG generation based on data lineage.</li> </ul> <p>Core Pillars of DataOps:</p> <p></p> <ul> <li>Automation: Ensures consistency and reliability in data product delivery by automating various aspects of the data lifecycle. This includes CI/CD and automating data quality checks, metadata integrity, and model drift.</li> <li>Observability and Monitoring: Critical to catch problems early, prevent data disasters, and keep stakeholders informed of system performance and data quality.</li> <li>Incident Response: Ensures fast and effective responses to failures or issues, using both proactive identification and retrospective resolution, supported by clear communication channels.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#data-architecture","title":"Data Architecture","text":"<p>Data engineers need to start by understanding the business requirements and use cases. These needs will inform the design and decisions about how to capture, store, transform, and serve data. The design of data systems must strike a balance between simplicity, cost, and operational efficiency. </p> <p>Data engineers must understand the trade-offs involved in choosing tools and technologies, whether for data ingestion, storage, transformation, or serving data. While data engineers and data architects often have distinct roles, collaboration is key. </p> <p>Data engineers should be able to implement the designs created by data architects and provide valuable feedback on those designs. With the rapid evolution of tools, technologies, and practices in the data space, data engineers must remain agile and continuously update their knowledge to maintain a relevant and effective data architecture.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#orchestration","title":"Orchestration","text":"<p>Orchestration is the process of managing and coordinating the execution of multiple jobs in a way that optimizes efficiency and speed. This is typically achieved through an orchestration engine like Apache Airflow, which monitors job dependencies, schedules tasks, and ensures tasks run in the correct order.</p> <p>Not Just a Scheduler: Unlike simple schedulers (like cron), which only manage time-based scheduling, orchestration engines like Airflow manage complex task dependencies using Directed Acyclic Graphs (DAGs). DAGs define the order in which tasks should execute and can be scheduled for regular intervals.</p> <p>Key Features of Orchestration Systems:</p> <ul> <li>High Availability: Orchestration systems should stay online continuously, ensuring they can monitor and trigger jobs without manual intervention.</li> <li>Job Monitoring &amp; Alerts: They monitor job execution and send alerts when tasks don\u2019t complete as expected (e.g., jobs not finishing by the expected time).</li> <li>Job History &amp; Visualization: Orchestration systems often include visualization tools to track job progress, along with maintaining historical data to help with debugging and performance monitoring.</li> <li>Backfilling: If new tasks or DAGs are added, orchestration systems can backfill these tasks, ensuring that missing or delayed data can be processed.</li> <li>Complex Dependencies: Orchestration engines allow setting dependencies over time (e.g., ensuring a monthly reporting job doesn\u2019t run until all necessary ETL tasks are completed for that month).</li> </ul> <p>Evolution of Orchestration Tools:</p> <ul> <li>Early Tools: Traditional tools like Apache Oozie were primarily used in large enterprise environments, particularly with Hadoop. However, they were costly and not easily adaptable to different infrastructures.</li> <li>Airflow's Rise: Apache Airflow, introduced by Airbnb in 2014, revolutionized orchestration by offering an open-source, Python-based solution that is highly extensible and cloud-friendly. Airflow became widely adopted due to its flexibility and ability to manage complex workflows in modern, multi-cloud environments.</li> <li>Newer Tools: New orchestration tools like Prefect, Dagster, Argo, and Metaflow are emerging, focusing on improving portability, testability, and performance. Prefect and Dagster, in particular, aim to address issues related to portability and transitioning workflows from local development to production.</li> </ul> <p>Batch vs. Streaming Orchestration:</p> <ul> <li>Batch Orchestration: Most orchestration engines focus on batch processing, where tasks are executed based on scheduled time intervals or job dependencies.</li> <li>Streaming Orchestration: For real-time or continuous data processing, orchestration becomes more complex. While streaming DAGs are difficult to implement and maintain, next-generation streaming platforms like Pulsar are attempting to simplify the process.</li> </ul> <p>Orchestration plays a pivotal role in data engineering by enabling the coordination of data workflows, ensuring that tasks are executed in the correct order and monitoring their progress. Tools like Apache Airflow have made orchestration accessible to a wider range of organizations, and newer solutions continue to improve the scalability and portability of orchestration tasks across various environments.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#software-engineering","title":"Software Engineering","text":"<p>Software engineering plays a central role in data engineering, and its importance has only grown as the field has evolved. While modern frameworks like Spark, SQL-based cloud data warehouses, and dataframes have abstracted much of the complexity, core software engineering skills are still crucial.</p> <p></p> <p>Key Areas of Software Engineering for Data Engineers:</p> <p>Core Data Processing Code:</p> <ul> <li>Data engineers still need to write and optimize core data processing code, which is a significant part of the data lifecycle. Whether using tools like Spark, SQL, or Beam, proficiency in these frameworks is necessary.</li> <li>Writing efficient data transformations and processing pipelines requires a solid understanding of software engineering principles, such as modularity, maintainability, and performance optimization.</li> </ul> <p>Code Testing: - Data engineers need to apply proper testing methodologies to ensure that their code is correct and reliable. This includes unit testing, regression testing, integration testing, end-to-end testing, and smoke testing. - Testing is essential in all stages of the data pipeline to guarantee data quality and system reliability.</p> <p>Development of Open Source Frameworks:</p> <ul> <li>Many data engineers are involved in creating and contributing to open-source frameworks. These tools help address specific challenges within the data engineering lifecycle, whether related to data processing, orchestration, or monitoring.</li> <li>While there is a proliferation of open-source tools like Airflow, Prefect, Dagster, and Metaflow, data engineers must evaluate the best fit for their organization's needs and consider factors like cost, scalability, and ease of use.</li> </ul> <p>Streaming Data Processing:</p> <ul> <li>Streaming data processing is more complex than batch processing. Engineers face challenges with handling real-time joins, applying windowing techniques, and ensuring high throughput and low latency.</li> <li>Frameworks like Spark, Beam, Flink, and Pulsar are used for processing streaming data, and data engineers need to be familiar with them to effectively design real-time analytics and reporting systems.</li> </ul> <p>Infrastructure as Code (IaC):</p> <ul> <li>As data systems move to the cloud, Infrastructure as Code (IaC) becomes increasingly important. IaC allows engineers to automate the deployment and management of infrastructure using code, improving repeatability and version control.</li> <li>Cloud services like AWS, GCP, and Azure provide IaC frameworks, such as Terraform, CloudFormation, and Kubernetes, which help automate data system management.</li> </ul> <p>Pipelines as Code:</p> <ul> <li>Modern orchestration systems, such as Apache Airflow, allow engineers to define data pipelines as code. This approach provides flexibility, version control, and scalability in managing complex workflows across various stages of the data lifecycle.</li> <li>Data engineers need to be proficient in writing Python code to define tasks and dependencies, which the orchestration engine interprets and runs.</li> </ul> <p>General-Purpose Problem Solving:</p> <ul> <li>Despite using high-level tools, data engineers often encounter situations where custom code is needed. This could involve writing connectors for unsupported data sources or integrating new tools into existing pipelines.</li> <li>Proficiency in software engineering allows data engineers to solve these problems by understanding APIs, handling exceptions, and ensuring smooth integration of new components into the system.</li> </ul> <p>In essence, while the tools and abstractions for data engineering have advanced significantly, software engineering remains foundational. Data engineers must not only be skilled in using these tools but also in developing solutions to the unique challenges they encounter in the data engineering lifecycle.</p>"},{"location":"DataEngineering/DesigningGDA/","title":"Data Architecture & Design","text":"<p>Cloud data architecture, by design, handles the ingestion, transformation, and analysis of data that is too large or complex for traditional data architectures.</p>"},{"location":"DataEngineering/DesigningGDA/#patterns-of-good-cloud-data-architecture","title":"Patterns of Good Cloud Data Architecture","text":"<p>Let's learn about 5 principles for cloud-native data architecture that are useful for designing and operating reliable, cost-effective and efficient systems in the cloud.</p> <p>Cloud offers incredible capabilities, but without deliberate design decisions, the architecture can become fragile, expensive, and challenging to maintain. Most cloud environments have not just one application but several technologies that need to be integrated.</p> <p>The overarching goal of cloud architecture is to connect the dots to provide customers with a valuable online platform.</p>"},{"location":"DataEngineering/DesigningGDA/#5-cloud-native-architecture-principles","title":"5 cloud-native architecture principles","text":"<p>This is essential for creating a good design:</p> <ul> <li> <p>Reliable: The system should continue to work correctly regardless of system faults or human errors.</p> </li> <li> <p>Efficient: The system must use computing resources efficiently to meet system requirements and maintain efficiency as the business grows.</p> </li> <li> <p>Maintainable: The world is fluid. Good data architecture should be able to respond to changes within the business and new technologies to unlock more possibilities in the future.</p> </li> <li> <p>Cost-optimized: The system should leverage various payment options to achieve cost efficiency.</p> </li> <li> <p>Secure: The system should be hardened to avoid insider attacks.</p> </li> </ul>"},{"location":"DataEngineering/DesigningGDA/#principle-1-have-an-automation-mindset","title":"Principle 1: Have an automation mindset","text":"<p>Automation has always been good practice for software systems. In traditional environments, automation refers to building, testing, and deploying software through continuous integration/continuous delivery (CI/CD) pipelines.</p> <p>A good cloud architecture takes a step ahead by automating the infrastructure as well as the internal components.</p> <p>The five common areas for automation are shown below:</p> <ul> <li>Software</li> </ul> <p>Software has been the most common area for automation regardless of the environment. Automation happens throughout the software's life cycle, from coding and deployment to maintenance and updates.</p> <ul> <li>Infrastructure</li> </ul> <p>In the cloud, we can apply the same engineering principles we use for applications to the entire environment. This implies the ability to create and manage infrastructure through code.</p> <p>Note</p> <p>Infrastructure as Code (IaC) is a process that enables us to manage infrastructure provisioning and configuration in the same way as we handle application code.</p> <p>Example</p> <p>we first provision a VM in the dev environment and then decide to create the same one in the production environment.</p> <p>Provisioning a server manually through a graphic interface often leads to mistakes.</p> <p>IaC means storing infrastructure configurations in a version-control environment and benefiting from CI/CD pipelines to ensure consistency across environments.</p> <ul> <li>Autoscaling</li> </ul> <p>The world is fluctuating, and a reliable system must handle the fluctuation in the load accordingly. Autoscaling helps the applications handle traffic increases and reduce costs when the demand is low without disrupting business operations.</p> <ul> <li>Recovery</li> </ul> <p>According to Google SRE philosophy, building a system with 100% availability is almost impossible and unnecessary. The team should, instead, embrace the risk and develop mechanisms to allow systems to recover from the failure quickly.</p> <p>Tip</p> <p>Automatic recovery works by monitoring workloads for key indicators and triggering operations when specific thresholds are reached.</p> <p>Example</p> <p>In the event of full memory or disk, the cloud will automatically request more resources and scale the system vertically, instead of just throwing an error and disrupting the system.</p> <ul> <li>Backup</li> </ul> <p>A backup strategy guarantees the business won't get interrupted during system failure, outage, data corruption, or natural disaster. Cloud backup operates by copying and storing data in a different physical location.</p>"},{"location":"DataEngineering/DesigningGDA/#principle-2-outsource-with-caution","title":"Principle 2: Outsource with caution","text":"<p>Most cloud providers offer different abstract levels of services, namely IaaS, PaaS, and SaaS. Their ever-growing features help us offload day-to-day management to the vendors. However, some organizations are concerned with giving providers access to their internal data for security reasons.</p> <p>Warning</p> <p>The decision of whether or not to use managed services comes down to operational overhead and security.</p> <p>Tip</p> <p>The best practice is to find a cloud provider with a high reputation, express our concerns, and find a solution together. Even if the provider can't solve the problem immediately, the discussion might open the door to future possibilities.</p>"},{"location":"DataEngineering/DesigningGDA/#principle-3-keep-an-eye-on-the-cost","title":"Principle 3: Keep an eye on the cost","text":"<p>Cost control isn\u2019t a prominent concern in traditional architecture because the assets and costs are pretty much fixed. However, in the cloud, the cost can be highly dynamic, and the team might surprisingly end up with a high bill.</p> <p>Warning</p> <p>Implementing cloud financial management is vital, and the organization must allocate time and resources to build knowledge around it and share the best practices with the teams.</p> <p>Fortunately, most cloud providers offer a centralized cost-monitoring tool that helps the team analyze and optimize the costs.</p> <p>A few quick wins on saving the cost:</p> <ul> <li> <p>Only pay what you need. Turn off stale servers and delete stale data.</p> </li> <li> <p>Enable table expiration on temporary data so they won't cost money after the expiration date.</p> </li> <li> <p>Maximize utilization. Implement efficient design to ensure high utilization of the underlying hardware.</p> </li> <li> <p>Query optimization. Learn different query optimization strategies such as incremental load, partitioning, and clustering.</p> </li> </ul>"},{"location":"DataEngineering/DesigningGDA/#principle-4-embrace-changes","title":"Principle 4: Embrace changes","text":"<p>The world is constantly evolving, and that's true for cloud architecture. As the business changes, the landscape of systems also needs to change. Good architecture doesn't stay in the existing state forever. Instead, they are very agile and can respond to business changes and adapt to them with the least effort.</p> <p>Example changes in cloud architecture, as below</p> <ol> <li>Migrate Database</li> <li>Switch Vendor</li> <li>Migrate from batch to realtime stream processing</li> <li>Upgrade Services</li> <li>Adapt to high volume or low volume.</li> </ol>"},{"location":"DataEngineering/DesigningGDA/#principle-5-do-not-neglect-security","title":"Principle 5: Do not neglect security","text":"<p>Last but not least, implementing a strong identity foundation becomes a huge responsibility of the data team.</p> <p>Tip</p> <p>Traditional architectures place a lot of faith in perimeter security, crudely a hardened network perimeter with \"trusted\" things inside and \"untrusted\" things outside. Unfortunately, this approach has always been vulnerable to insider attackers, as well as external threats such as spear phishing.</p> <p>In the cloud environment, all assets are connected to the outside world to some degree. Zero Trust architecture has been created to eliminate the risk from both outside and inside. Zero Trust is a strategy that secures an organization by eliminating implicit trust and validating every stage of digital interaction.</p> <p>Another important concept in terms of security is the shared responsibility model. It divides security into the security of the cloud and security in the cloud. Most cloud providers are responsible for the security of the cloud, and it's the user's responsibility to design a custom security model for their applications. Users are responsible for managing sensitive data, internal access to data and services, and ensuring GDPR compliance.</p>"},{"location":"DataEngineering/DesigningGDA/#lambda-architecture","title":"LAMBDA Architecture","text":"<p>In the \u201cold days\u201d (the early to mid-2010s), the popularity of working with streaming data exploded with the emergence of Kafka as a highly scalable message queue and frameworks such as Apache Storm and Samza for streaming/real-time analyt\u2010 ics. These technologies allowed companies to perform new types of analytics and modeling on large amounts of data, user aggregation and ranking, and product recommendations. Data engineers needed to figure out how to reconcile batch and streaming data into a single architecture. The Lambda architecture was one of the early popular responses to this problem. </p> <p>In a Lambda architecture, you have systems operating independently of each other\u2014batch, streaming, and serving. The source system is ideally immutable and append-only, sending data to two destinations for processing: stream and batch. In-stream processing intends to serve the data with the lowest possible latency in a \u201cspeed\u201d layer, usually a NoSQL database. In the batch layer, data is processed and transformed in a system such as a data warehouse, creating precomputed and aggre\u2010 gated views of the data. The serving layer provides a combined view by aggregating query results from the two layers</p> <p>Lambda architecture has its share of challenges and criticisms. Managing multiple systems with different codebases is as difficult as it sounds, creating error-prone systems with code and data that are extremely difficult to reconcile.</p> <p>How it works: The system will dispatch all incoming data to batch and streaming layers. The batch layer will maintain an append-only primary dataset and precompute the batch views The streaming layer will only handle the most recent data to achieve low latency. Both batch and stream views are served in the serving layer to be queried.The result of merging batch and real-time results can answer any incoming query.</p> <p>Challenges: Complexity and cost of running 2 parallel systems instead of 1. This approach often uses systems with different software ecosystems, making it challenging to replicate the business logic across the systems. It's also quite difficult to reconcile the outputs of 2 pipelines at the end.</p>"},{"location":"DataEngineering/DesigningGDA/#kappa-architecture","title":"KAPPA Architecture","text":"<p>As a response to the shortcomings of Lambda architecture, Jay Kreps proposed an alternative called Kappa architecture. The central thesis is this: why not just use a stream-processing platform as the backbone for all data handling\u2014inges\u2010 tion, storage, and serving? This facilitates a true event-based architecture. Real-time and batch processing can be applied seamlessly to the same data by reading the live event stream directly and replaying large chunks of data for batch processing. </p> <p>Kappa architecture Though the original Kappa architecture article came out in 2014, we haven\u2019t seen it widely adopted. There may be a couple of reasons for this. First, streaming itself is still a bit of a mystery for many companies; it\u2019s easy to talk about, but harder than expected to execute. Second, Kappa architecture turns out to be complicated and expensive in practice. While some streaming systems can scale to huge data volumes, they are complex and expensive; batch storage and processing remain much more efficient and cost-effective for enormous historical datasets.</p> <p>Advantages: In Kappa architecture, a streaming processing engine continuously processes real-time data and ingests it into long-term storage. When code changes occur, developers can recompute using the raw data stored in the event logs database.</p> <p>Challenges: Streaming remains a challenge for many companies due to its complexity and most likely high cost and maintainance. Managing duplicates and preserving order, for instance, can be more challenging than batch processing. data replay is often trickier than it may seem.</p>"},{"location":"DataEngineering/DesigningGDA/#data-lake","title":"Data Lake","text":"<p>A data lake is a popular data architecture comparable, to a data warehouse. It\u2019s a storage repository that holds a large amount of data, but unlike a data warehouse where data is structured, data in a data lake is in its raw format.</p> Topic Data Lake Data Warehouse Data Format Store unstructured, semi-structured and structured data in its raw format. Store only structured data after the transformation. Schema Schema-on-read: Schema is defined after data is stored. Schema-on-write: Schema is predefined prior to when data is stored. Usecase Data exploration: Unstructured data opens more possibilities for analysis and ML algorithms, A landing place before loading data into a data warehouse. Reporting: Reporting tools and dashboards prefer highly coherent data. Data Quality Data is in its raw format without cleaning, so data quality is not ensured. Data is highly curated, resulting in higher data quality. Cost Both storage and operational costs are lower. Storing data in the data warehouse is usually more expensive and time-consuming. <p>The following graph illustrates the key components of a data lake</p> <p></p> <ul> <li> <p>Ingestion layer: The ingestion layer collects raw data and loads them into the data lake. The raw data is not modified in this layer.</p> </li> <li> <p>Processing layer: Data lake uses object storage to store data. Object storage stores data with metadata tags and a unique identifier, making searching and accessing data easier. Due to the variety and high volume of data, a data lake usually provides tools for features like data catalog, authentication, data quality, etc.</p> </li> <li> <p>Insights layer: The insights layer is for clients to query the data from the data lake. Direct usage could be feeding the reporting tools, dashboards, or a data warehouse.</p> </li> </ul>"},{"location":"DataEngineering/DesigningGDA/#data-mesh","title":"Data Mesh","text":"<p>The term data mesh was coined by Zhamak Dehghani in 2019 and created the idea of domain-oriented decentralization for analytical data. Centrally managed architectures tend to create data bottlenecks and hold back analytics agility. On the other hand, completely decentralized architectures create silos and duplicates, making management across domains very difficult.</p> <p>The data mesh architecture proposes distributed data ownership, allowing teams to own the entire life cycle of their domains and deliver quicker analyses.</p> <p>The organization's IT team is responsible for the overall infrastructure, governance, and efficiency without owning any domain-related business.</p> <p>Adopting data mesh requires some pretty cultural and organizational changes.</p> <p>Currently, no template solutions for a data mesh, so many companies are still trying to figure out if it's a good fit for their organizations.</p> <p></p> <p>Each domain team is responsible for ingesting the operational data and building analytics models.</p> <p>The domain team agrees with the rest on global policies to safely and efficiently interact with the other domains within the mesh.</p> <p>A centralized data platform team builds infrastructures and tools for domain teams to build data products and perform analysis more effectively and quickly.</p> <ul> <li> <p>Principles</p> <ol> <li> <p>Domain ownership: Each domain team takes responsibility for the entire data life cycle.</p> </li> <li> <p>Data as a product: Treat provided data as a high-quality product, like APIs to other domains.</p> </li> <li> <p>Self-serve data platform: Build an effective data platform for domain teams to build data products quickly.</p> </li> <li> <p>Federated governance: Standardize data policies to create a healthy data ecosystem for domain interoperability.</p> </li> </ol> </li> </ul>"},{"location":"DataEngineering/fileformat/","title":"File Formats","text":""},{"location":"DataEngineering/fileformat/#how-data-is-stored-physically-on-disk","title":"How data is stored physically on disk","text":"<p>Row-based File Formats (e.g., CSV, traditional databases for OLTP):</p> <p>Data for an entire row is stored contiguously on disk.</p> <p>Advantage for OLTP: When you need all the details of a specific record (e.g., a customer's entire banking transaction), a row-based format allows you to access all its columns continuously. If you need to update multiple columns for a single transaction, the data is already together.</p> <p>Disadvantage for OLAP: If you only need a few columns (e.g., \"Title\" and \"Chart\") from a large dataset, a row-based system still has to read through all the interleaved data (including \"Date\") for every row. This leads to excessive I/O operations and slower performance because the system has to \"jump\" across the disk to pick out the desired columns from different rows.</p> <p>Column-based File Formats (e.g., Parquet for OLAP):</p> <p>Data for each column is stored contiguously on disk, independent of other columns.</p> <p>Advantage for OLAP: This format shines in \"read-many\" scenarios prevalent in Big Data analytics. If you only need \"Title\" and \"Chart\" columns, the system can go directly to the contiguous \"Title\" block and \"Chart\" block, skipping the \"Date\" column entirely. This significantly reduces I/O, leading to faster query performance and lower computational cost.</p> <p>Disadvantage for OLTP: If you need to retrieve or update an entire row, the system has to jump between different column blocks to gather all the data for that single row</p>"},{"location":"DataEngineering/fileformat/#parquet","title":"Parquet","text":"<p>Columnar storage format, available to any project in the Hadoop ecosystem. It's designed to bring efficient columnar storage of data compared to row-based like CSV or TSV files.</p> <p>It is columnar in nature and designed to bring efficient columnar storage of data. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in comparison to row-based files like CSV.</p> <p>Schema evolution is handled in the file metadata allowing compatible schema evolution. It supports all data types, including nested ones, and integrates well with flat data, semi-structured data, and nested data sources.</p> <p>Parquet is considered a de-facto standard for storing data nowadays</p> <p>Data compression - by applying various encoding and compression algorithms, Parquet file provides reduced memory consumption</p> <p>Columnar storage - this is of paramount importance in analytic workloads, where fast data read operation is the key requirement. But, more on that later in the article\u2026</p> <p>Language agnostic - as already mentioned previously, developers may use different programming languages to manipulate the data in the Parquet file</p> <p>Open-source format - meaning, you are not locked with a specific vendor</p> <p>Why is this additional structure super important?</p> <p>In OLAP scenarios, we are mainly concerned with two concepts: projection and predicate(s). Projection refers to a SELECT statement in SQL language \u2013 which columns are needed by the query. </p> <p>Predicate(s) refer to the WHERE clause in SQL language \u2013 which rows satisfy criteria defined in the query. In our case, we are interested in T-Shirts only, so the engine can completely skip scanning Row group 2, where all the values in the Product column equal socks!</p> <p>This means, every Parquet file contains \u201cdata about data\u201d \u2013 information such as minimum and maximum values in the specific column within the certain row group. Furthermore, every Parquet file contains a footer, which keeps the information about the format version, schema information, column metadata, and so on.</p> <p>While Parquet is primarily columnar, it actually uses a hybrid model to combine the efficiencies of both row and column storage. This hierarchical structure helps manage very large datasets.</p> <p></p> <p>Let\u2019s stop for a moment and understand above diagram, as this is exactly the structure of the Parquet file  Columns are still stored as separate units, but Parquet introduces additional structures, called Row group.</p> <p>The structure of a Parquet file can be visualized as a tree</p> <ol> <li> <p>File: The top-level entity.  It contains metadata about the entire file, such as the number of columns, total rows, and number of row groups.</p> </li> <li> <p>Row Group: This is a logical horizontal partition of the data within the file.  Instead of storing all data for a column as one huge block, Parquet breaks the data into smaller, manageable chunks called Row Groups.  By default, a row group stores around 128 MB of data.  For example, if you have 100 million records, a row group might contain 100,000 records.  Each row group also contains metadata, including the minimum and maximum values for each column within that specific row group. This metadata is crucial for optimization.</p> </li> <li> <p>Column (Column Chunk): Inside each row group, data is organized by column.  All the data for a specific column within that row group (e.g., all \"Title\" values for the first 100,000 records) is stored together contiguously.</p> </li> <li>Page: This is a further logical partition within a column chunk, where the actual data values are stored.  Each page contains its own metadata, which includes information like the maximum and minimum values present on that page.</li> </ol> <p>This hierarchical structure and the abundant metadata at different levels (file, row group, column, page) are what make Parquet highly efficient</p> <p>Metadata and its Role</p> <p>Parquet stores a rich set of metadata (data about data) internally. This metadata is a key factor in Parquet's performance advantages:</p> <ol> <li>File-level metadata: Includes information like the total number of columns, total rows, and the number of row groups.</li> <li>Row Group-level metadata: Crucially stores the minimum and maximum values for each column within that row group.</li> <li>Page-level metadata: Also contains statistics like minimum and maximum values for data within that specific page. Because of this extensive metadata, when a Parquet file is read, it doesn't need to be given additional parameters like schema information; it already contains all necessary details. This self-describing nature simplifies data processing.</li> </ol> <p>Encoding and Compression Techniques Parquet uses several intelligent encoding and compression techniques to reduce file size and improve query speed without losing information.</p> <p></p> <ol> <li> <p>Encoding: These techniques transform data into a more compact format before compression.</p> <p>Dictionary Encoding:</p> <p>Used for columns with many repeating values (low cardinality), like \"Destination Country Name\" where there might be millions of records but only ~200 distinct countries. Parquet identifies the distinct values in a column and creates a dictionary (a mapping) where each distinct value is assigned a small integer code (e.g., 0 for \"United States,\" 1 for \"France,\" etc.). The actual data in the column is then stored as these compact integer codes instead of the full strings. When reading, Parquet uses the dictionary to convert the codes back to the original values. This drastically reduces storage space.</p> <p>Run Length Encoding (RLE):</p> <p>Used for sequences of repeating values. Instead of storing \"AAAAABBCD,\" RLE would store \"A5B2C1D1\" (A appears 5 times, B 2 times, etc.). This makes the data much smaller, especially for columns with many consecutive identical values.</p> <p>Bit Packing:</p> <p>Optimizes storage at the bit level. If a column's values (after dictionary encoding) only range from 0 to 3, these values can be stored using just 2 bits per value (00, 01, 10, 11) instead of the standard 8 bits (1 byte) or more. This significantly reduces the byte size required to store each value.</p> </li> <li> <p>Compression: After encoding, Parquet applies compression algorithms to further reduce the file size.  Common compression codecs include Gzip, Snappy, and LZ4.  The choice of compression can impact performance. For example, Snappy is often much faster for reads than Gzip, even if Gzip provides slightly better compression ratios. The source states that a query running in 3000 seconds with Gzip might run in just 29 seconds with Snappy, making it 100 times faster</p> </li> </ol> <p>Optimization Techniques in Parquet</p> <p>The combination of columnar storage, hierarchical structure, rich metadata, and intelligent encoding/compression enables two powerful optimization techniques.</p> <ol> <li> <p>Predicate Pushdown (Filter Pushdown): This technique uses the row group-level metadata (min/max values for each column) to skip scanning entire row groups that cannot possibly satisfy a query's filter condition.</p> <p>Example: Consider a query SELECT * FROM table WHERE age &lt; 18.</p> <p>Parquet will first check the metadata of each row group.If a row group's metadata indicates that its age column has a minimum value of 22 and a maximum value of 35, Parquet immediately knows that this row group cannot contain any data where age &lt; 18.</p> <p>Therefore, the system discards that entire row group without reading any of its data from disk. This saves significant I/O, CPU utilization, time, and cost.</p> <p>This optimization also works with equality checks (e.g., WHERE age = 18) by checking if 18 exists in the row group's dictionary (if dictionary encoding is used) or falls within its min/max range.</p> </li> <li> <p>Projection Pruning: This technique capitalizes on Parquet's columnar storage by only reading the columns that are explicitly required by the query.Example: If a query is SELECT name, age FROM users, Parquet will only read the name and age column data from disk and completely skip reading any other columns like address, phone_number, etc.. Since columns are stored separately, this is highly efficient as it avoids bringing unnecessary data into memory, reducing I/O and processing load</p> </li> </ol>"},{"location":"airflow/airflow/","title":"Airflow","text":""},{"location":"airflow/airflow/#what-is-orchestration-in-bigdata","title":"What is Orchestration in BigData?","text":"<p>Orchestration in big data refers to the automated configuration, coordination, and management of complex big data systems and services. Like an orchestra conductor ensures each section of the orchestra plays its part at the right time and the right way to create harmonious music, orchestration in big data ensures that each component of a big data system interacts in the correct manner at the right time to execute complex, multi-step processes efficiently and reliably.</p> <p>In practical terms, orchestration involves:</p> <ol> <li>Workflow management: It defines, schedules, and manages workflows involving multiple tasks across disparate systems. These workflows can be simple linear sequences or complex directed acyclic graphs (DAGs) with branching and merging paths.</li> <li>Task scheduling: Orchestration tools schedule tasks based on their dependencies. This ensures that tasks are executed in the correct order and that tasks that can run in parallel do so, increasing overall system efficiency.</li> <li>Failure handling: Orchestration tools handle failures in the system, either by retrying failed tasks, skipping them, or alerting operators to the failure.</li> </ol>"},{"location":"airflow/airflow/#need-of-workflowdependency-management-while-designing-data-pipelines","title":"Need of Workflow/Dependency management while Designing Data Pipelines","text":"<p>Workflow or dependency management is a crucial component in the design of data pipelines, especially in the context of big data.</p> <ol> <li>Ordering and Scheduling: Data processing tasks often have dependencies, meaning one task needs to complete before another can begin. For example, a task that aggregates data may need to wait until the data has been extracted and cleaned. A workflow management system can keep track of these dependencies and ensure tasks are executed in the right order.</li> <li>Parallelization: When tasks don't have dependencies, they can often run in parallel. This can significantly speed up data processing. Workflow management systems can manage parallel execution, maximizing the use of computational resources and reducing overall processing time.</li> <li>Error Handling: If a task in a data pipeline fails, it can have a knock-on effect on other tasks. Workflow management systems can handle these situations, for instance by retrying failed tasks, skipping them, or stopping the pipeline and alerting operators.</li> <li>Visibility and Monitoring: Workflow management systems often provide tools for monitoring the progress of data pipelines and visualizing their structure. This can make it easier to spot bottlenecks or failures, and to understand the flow of data through the pipeline.</li> <li>Resource Management: Workflow management systems can allocate resources (like memory, CPU, etc.) depending on the requirements of different tasks. This helps in efficiently utilizing resources and ensures optimal performance of tasks.</li> </ol>"},{"location":"airflow/airflow/#what-is-airflow","title":"What is AirFlow?","text":"<p>Apache Airflow is an open-source platform that programmatically allows you to author, schedule, and monitor workflows. It was originally developed by Airbnb in 2014 and later became a part of the Apache Software Foundation's project catalog.</p> <p>Airflow uses directed acyclic graphs (DAGs) to manage workflow orchestration. DAGs are a set of tasks with directional dependencies, where the tasks are the nodes in the graph, and the dependencies are the edges. In other words, each task in the workflow executes based on the completion status of its predecessors.</p> <p>Key features of Apache Airflow include:</p> <ol> <li>Dynamic Pipeline Creation: Airflow allows you to create dynamic pipelines using Python. This provides flexibility and can be adapted to complex dependencies and operations.</li> <li>Easy Scheduling: Apache Airflow includes a scheduler to execute tasks at defined intervals. The scheduling syntax is quite flexible, allowing for complex scheduling.</li> <li>Robust Monitoring and Logging: Airflow provides detailed status and logging information about each task, facilitating debugging and monitoring. It also offers a user-friendly UI to monitor and manage the workflow.</li> <li>Scalability: Airflow can distribute tasks across a cluster of workers, meaning it can scale to handle large workloads.</li> <li>Extensibility: Airflow supports custom plugins and can integrate with several big data technologies. You can define your own operators and executors, extend the library, and even use the user interface API.</li> <li>Failure Management: In case of a task failure, Airflow sends alerts and allows for retries and catchup of past runs in a robust way</li> </ol>"},{"location":"airflow/airflow/#airflow-architecture","title":"Airflow Architecture","text":""},{"location":"airflow/airflow/#scheduler","title":"Scheduler","text":"<p>The scheduler is a critical component of Airflow. Its primary function is to continuously scan the DAGs (Directed Acyclic Graphs) directory to identify and schedule tasks based on their dependencies and specified time intervals. The scheduler is responsible for determining which tasks to execute and when. It interacts with the metadata database to store and retrieve task state and execution information.</p>"},{"location":"airflow/airflow/#metadata-database","title":"Metadata Database","text":"<p>Airflow leverages a metadata database, such as PostgreSQL or MySQL, to store all the configuration details, task states, and execution metadata. The metadata database provides persistence and ensures that Airflow can recover from failures and resume tasks from their last known state. It also serves as a central repository for managing and monitoring task execution.</p>"},{"location":"airflow/airflow/#web-server","title":"Web Server","text":"<p>The web server component provides a user interface for interacting with Airflow. It enables users to monitor task execution, view the status of DAGs, and access logs and other operational information. The web server communicates with the metadata database to fetch relevant information and presents it in a user-friendly manner. Users can trigger manual task runs, monitor task progress, and configure Airflow settings through the web server interface.</p>"},{"location":"airflow/airflow/#executors","title":"Executors","text":"<p>Airflow supports different executor types to execute tasks. The executor is responsible for allocating resources and running tasks on the specified worker nodes.</p> <p>Example</p> <p>the local executor executes tasks on the same node as the scheduler, while the Kubernetes executor executes tasks in containers.</p>"},{"location":"airflow/airflow/#worker-nodes","title":"Worker Nodes","text":"<p>Worker nodes are responsible for executing the tasks assigned to them by the executor. They retrieve the task details, dependencies, and code from the metadata database and execute the tasks accordingly. The number of worker nodes can be scaled up or down based on the workload and resource requirements.</p>"},{"location":"airflow/airflow/#message-queue","title":"Message Queue","text":"<p>Airflow relies on a message queue system, such as RabbitMQ, Apache Kafka, or Redis, to enable communication between the scheduler and the worker nodes. The scheduler places task execution requests in the message queue, and the worker nodes pick up these requests, execute the tasks, and update their status back to the metadata database. The message queue acts as a communication channel, ensuring reliable task distribution and coordination.</p>"},{"location":"airflow/airflow/#dags-and-tasks","title":"DAGs and Tasks","text":"<p>DAGs are at the core of Airflow\u2019s architecture. A DAG is a directed graph consisting of interconnected tasks. Each task represents a unit of work within the data pipeline. Tasks can have dependencies on other tasks, defining the order in which they should be executed. Airflow uses the DAG structure to determine task dependencies, schedule task execution, and track their progress.</p> <p>Each task within a DAG is associated with an operator, which defines the type of work to be performed. Airflow provides a rich set of built-in operators for common tasks like file operations, data processing, and database interactions. Additionally, custom operators can be created to cater to specific requirements. Tasks within a DAG can be triggered based on various events, such as time-based schedules, the completion of other tasks, or the availability of specific data.</p> <p>In Airflow, there's a task which perform 1 unique job across the DAG, the task is usually calling Airflow Operator.</p> <p>An operator is essentially a Python class that defines what a task should do and how it should be executed within the context of the DAG.</p> <ul> <li>Operators can perform a wide range of tasks, such as running a Bash script or Python script, executing SQL, sending an email, or transferring files between systems.</li> <li>Operators provide a high-level abstraction of the tasks, letting us focus on the logic and flow of the workflow without getting bogged down in the implementation details.</li> <li>Apache Airflow has several types of operators that allow you to perform different types of tasks. </li> </ul>"},{"location":"airflow/airflow/#task-level-operator","title":"Task level Operator","text":"<p>Tip</p> <p>While Airflow provides a wide variety of operators out of the box, we may still need to create custom operators to address our specific use cases.</p> <p>All operators are extended from <code>BaseOperator</code> and we need to override two methods: <code>__init__</code> and <code>execute</code>.</p> <p>The <code>execute</code> method is invoked when the runner calls the operator.</p> <p>The following example creates a custom operator <code>DataAnalysisOperator</code> that performs the requested type of analysis on an input file and saves the results to an output file.</p> <p>Warning</p> <p>It's advised not to put expensive operations in <code>__init__</code> because it will be instantiated once per scheduler cycle.</p> <pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass DataAnalysisOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, dataset_path, output_path, analysis_type, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.analysis_type = analysis_type\n\n    def execute(self, context):\n        # Load the input dataset into a pandas DataFrame.\n        data = pd.read_csv(self.dataset_path)\n\n        # Perform the requested analysis.\n        if self.analysis_type == 'mean':\n            result = np.mean(data)\n        elif self.analysis_type == 'std':\n            result = np.std(data)\n        else:\n            raise ValueError(f\"Invalid analysis type '{self.analysis_type}'\")\n\n        # Write the result to a file.\n        with open(self.output_path, 'w') as f:\n            f.write(str(result))\n</code></pre> <p>The extensibility of the operator is one of many reasons why Airflow is so powerful and popular.</p> <p>Tip</p> <pre><code>BashOperator: Executes a bash command.\nPythonOperator: Calls a Python function.\nEmailOperator: Sends an email.\nSimpleHttpOperator: Sends an HTTP request.\nMySqlOperator, SqliteOperator, PostgresOperator, MsSqlOperator, OracleOperator, etc.: Executes a SQL command.\nDummyOperator: A placeholder operator that does nothing.\nSensor: Waits for a certain time, file, database row, S3 key, etc. There are many types of sensors, like HttpSensor, SqlSensor, S3KeySensor, TimeDeltaSensor, ExternalTaskSensor, etc.\nSSHOperator: Executes commands on a remote server using SSH.\nDockerOperator: Runs a Docker container.\nSparkSubmitOperator: Submits a Spark job.\nOperators in Airflow \nS3FileTransformOperator: Copies data from a source S3 location to a temporary location on the local filesystem, transforms the data, and then uploads it to a destination S3 location.\nS3ToRedshiftTransfer: Transfers data from S3 to Redshift.\nEmrAddStepsOperator: Adds steps to an existing EMR (Elastic Map Reduce) job flow.\nEmrCreateJobFlowOperator: Creates an EMR JobFlow, i.e., a cluster.\nAthenaOperator: Executes a query on AWS Athena.\nAwsGlueJobOperator: Runs an AWS Glue Job.\nS3DeleteObjectsOperator: Deletes objects from an S3 bucket.\nBigQueryOperator: Executes a BigQuery SQL query.\nBigQueryToBigQueryOperator: Copies data from one BigQuery table to another.\nDataprocClusterCreateOperator: This operator is used to create a new cluster of machines on GCP's Dataproc service.\nDataProcPySparkOperator: This operator is used to submit PySpark jobs to a running Dataproc cluster.\nDataProcSparkOperator: This operator is used to submit Spark jobs written in Scala or Java to a running Dataproc cluster.\nDataprocClusterDeleteOperator: This operator is used to delete an existing cluster.\n</code></pre>"},{"location":"airflow/airflow/#sensor-operator","title":"Sensor Operator","text":"<p>A special type of operator is called a sensor operator. It's designed to wait until something happens and then succeed so their downstream tasks can run. The DAG has a few sensors that are dependent on external files, time, etc.</p> <p>Common sensor types are:</p> <ul> <li> <p>TimeSensor: Wait for a certain amount of time to pass before executing a task.</p> </li> <li> <p>FileSensor: Wait for a file to be in a location before executing a task.</p> </li> <li> <p>HttpSensor: Wait for a web server to become available or return an expected result before executing a task.</p> </li> <li> <p>ExternalTaskSensor: Wait for an external task in another DAG to complete before executing a task.</p> </li> </ul> <p>We can create a custom sensor operator by extending the <code>BaseSensorOperator</code> class and overriding two methods: <code>__init__</code> and <code>poke</code>.</p> <p>The <code>poke</code> method performs the necessary checks to determine whether the condition has been satisfied. If so, return <code>True</code> to indicate that the sensor has succeeded. Otherwise, return <code>False</code> to continue checking in the next interval.</p> <p>Here is an example of a custom sensor operator that pulls an endpoint until the response matches with <code>expected_text</code>.</p> <pre><code>from airflow.sensors.base_sensor_operator import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\nimport requests\n\nclass EndpointSensorOperator(BaseSensorOperator):\n\n    @apply_defaults\n    def __init__(self, url, expected_text, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.url = url\n        self.expected_text = expected_text\n\n    def poke(self, context):\n        response = requests.get(self.url)\n        if response.status_code != 200:\n            return False\n        return self.expected_text in response.text\n</code></pre>"},{"location":"airflow/airflow/#poke-mode-vs-reschedule-mode","title":"Poke mode vs. Reschedule mode","text":"<p>There are two types of modes in a sensor:</p> <ul> <li> <p>poke: the sensor repeatedly calls its <code>poke</code> method at the specified <code>poke_interval</code>, checks the condition, and reports it back to Airflow.   As a consequence, the sensor takes up the worker slot for the entire execution time.</p> </li> <li> <p>reschedule: Airflow is responsible for scheduling the sensor to run at the specified <code>poke_interval</code>.   But if the condition is not met yet, the sensor will release the worker slot to other tasks between two runs.</p> </li> </ul> <p>Tip</p> <p>In general, poke mode is more appropriate for sensors that require short run-time and <code>poke_interval</code> is less than five minutes.</p> <p>Reschedule mode is better for sensors that expect long run-time (e.g., waiting for data to be delivered by an external party) because it is less resource-intensive and frees up workers for other tasks.</p>"},{"location":"airflow/airflow/#backfilling","title":"Backfilling","text":"<p>Backfilling is an important concept in data processing. It refers to the process of populating or updating historical data in the system to ensure that the data is complete and up-to-date.</p> <p>This is typically required in two use cases:</p> <ul> <li> <p>Implement a new data pipeline: If the pipeline uses an incremental load, backfilling is needed to populate historical data that falls outside the reloading window.</p> </li> <li> <p>Modify an existing data pipeline: When fixing a SQL bug or adding a new column, we also want to backfill the table to update the historical data.</p> </li> </ul> <p>Warning</p> <p>When backfilling the table, we must ensure that the new changes are compatible with the existing data; otherwise, the table needs to be recreated from scratch.</p> <p>Sometimes, the backfilling job can consume significant resources due to the high volume of historical data.</p> <p>It's also worth checking any possible downstream failure before executing the backfilling job.</p> <p>Airflow provides the backfilling process in its cli command.</p> <pre><code>airflow backfill [dag name] -s [start date] -e [end date]\n</code></pre> <p>To create DAGs, we just need basic knowledge of Python. However, to create efficient and scalable DAGs, it's essential to master Airflow's specific features and nuances.</p>"},{"location":"airflow/airflow/#create-a-dag-object","title":"Create a DAG object","text":"<p>A DAG file starts with a <code>dag</code> object. We can create a <code>dag</code> object using a context manager or a decorator.</p> <pre><code>from airflow.decorators import dag\nfrom airflow import DAG\nimport pendulum\n\n# dag1 - using @dag decorator\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag1():\n    pass\n\nrandom_dag1()\n\n# dag2 - using context manager\nwith DAG(\n    dag_id=\"random_dag2\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n) as dag2:\n    pass\n</code></pre> <p>Either way, we need to define a few parameters to control how a DAG is supposed to run.</p> <p>Some of the most-used parameters are:</p> <ul> <li> <p>start_date: If it's a future date, it's the timestamp when the scheduler starts to run. If it's a past date, it's the timestamp from which the scheduler will attempt to backfill.</p> </li> <li> <p>catch_up: Whether to perform scheduler catch-up. If set to true, the scheduler will backfill runs from the start date.</p> </li> <li> <p>schedule: Scheduling rules. Currently, it accepts a cron string, time delta object, timetable, or list of dataset objects.</p> </li> <li> <p>tags: List of tags helping us search DAGs in the UI.</p> </li> </ul>"},{"location":"airflow/airflow/#create-a-task-object","title":"Create a task object","text":"<p>A DAG object is composed of a series of dependent tasks. A task can be an operator, a sensor, or a custom Python function decorated with <code>@task</code>. Then, we will use <code>&gt;&gt;</code> or the opposite way, which denotes the dependencies between tasks.</p> <pre><code># dag3\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag3():\n\n    s1 = TimeDeltaSensor(task_id=\"time_sensor\", delta=timedelta(seconds=2))\n    o1 = BashOperator(task_id=\"bash_operator\", bash_command=\"echo run a bash script\")\n    @task\n    def python_operator() -&gt; None:\n        logging.info(\"run a python function\")\n    o2 = python_operator()\n\n    s1 &gt;&gt; o1 &gt;&gt; o2\n\nrandom_dag3()\n</code></pre> <p>A task has a well-defined life cycle, including 14 states, as shown in the following graph:</p> <p></p>"},{"location":"airflow/airflow/#pass-data-between-tasks","title":"Pass data between tasks","text":"<p>One of the best design practices is to split a heavy task into smaller tasks for easy debugging and quick recovery.</p> <p>Example</p> <p>we first make an API request and use the response as the input for the second API request. To do so, we need to pass a small amount of data between tasks.</p> <p>XComs (cross-communications) is a method for passing data between Airflow tasks. Data is defined as a key-value pair, and the value must be serializable.</p> <ul> <li> <p>The <code>xcom_push()</code> method pushes data to the Airflow metadata database and is made available for other tasks.</p> </li> <li> <p>The <code>xcom_pull()</code> method retrieves data from the database using the key.</p> </li> </ul> <p></p> <p>Every time a task returns a value, the value is automatically pushed to XComs.</p> <p>We can find them in the Airflow UI <code>Admin</code> -&gt; <code>XComs</code>. If the task is created using <code>@task</code>, we can retrieve XComs by using the object created from the upstream task.</p> <p>In the following example, the operator <code>o2</code> uses the traditional syntax, and the operator <code>o3</code> uses the simple syntax:</p> <pre><code>@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag4():\n\n    @task\n    def python_operator() -&gt; None:\n        logging.info(\"run a python function\")\n        return str(datetime.now()) # return value automatically stores in XCOMs\n    o1 = python_operator()\n    o2 = BashOperator(task_id=\"bash_operator1\", bash_command='echo \"{{ ti.xcom_pull(task_ids=\"python_operator\") }}\"') # traditional way to retrieve XCOM value\n    o3 = BashOperator(task_id=\"bash_operator2\", bash_command=f'echo {o1}') # make use of @task feature\n\n    o1 &gt;&gt; o2 &gt;&gt; o3\n\nrandom_dag4()\n</code></pre> <p>Warning</p> <p>Although nothing stops us from passing data between tasks, the general advice is to not pass heavy data objects, such as pandas DataFrame and SQL query results because doing so may impact task performance.</p>"},{"location":"airflow/airflow/#use-jinja-templates","title":"Use Jinja templates","text":"<p>Info</p> <p>Jinja is a templating language used by many Python libraries, such as Flask and Airflow, to generate dynamic content.</p> <p>It allows us to embed variables within the text and then have those variables replaced with actual values during runtime.</p> <p>Airflow's Jinja templating engine provides built-in functions that we can use between double curly braces, and the expression will be evaluated at runtime.</p> <p>Users can also create their own macros using user_defined_macros and the macro can be a variable as well as a function.</p> <pre><code>def days_to_now(starting_date):\n    return (datetime.now() - starting_date).days\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"],\n    user_defined_macros={\n        \"starting_date\": datetime(2015, 5, 1),\n        \"days_to_now\": days_to_now,\n    })\ndef random_dag5():\n\n    o1 = BashOperator(task_id=\"bash_operator1\", bash_command=\"echo Today is {{ execution_date.format('dddd') }}\")\n    o2 = BashOperator(task_id=\"bash_operator2\", bash_command=\"echo Days since {{ starting_date }} is {{ days_to_now(starting_date) }}\")\n\n    o1 &gt;&gt; o2\n\nrandom_dag5()\n</code></pre> <p>Note</p> <p>The full list of built-in variables, macros and filters in Airflow can be found in the Airflow Documentation</p> <p>Another feature around templating is the <code>template_searchpath</code> parameter in the DAG definition.</p> <p>It's a list of folders where Jinja will look for templates.</p> <p>Example</p> <p>A common use case is invoking an SQL file in a database operator such as BigQueryInsertJobOperator. Instead of hardcoding the SQL query, we can refer to the SQL file, and the content will be automatically rendered during runtime.</p> <pre><code>@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"],\n    template_searchpath=[\"/usercode/dags/sql\"])\ndef random_dag6():\n\n    BigQueryInsertJobOperator(\n        task_id=\"insert_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"{% include 'sample.sql' %}\",\n                \"useLegacySql\": False,\n            }\n        }\n    )\n\nrandom_dag6()\n</code></pre>"},{"location":"airflow/airflow/#manage-cross-dag-dependencies","title":"Manage cross-DAG dependencies","text":"<p>In principle, every DAG is an independent workflow. However, sometimes, it's necessary to create dependencies between DAGs.</p> <p>Example</p> <p>a DAG performs an ETL job that produces a table sales. The sales table is the source of two downstream DAGs, where one generates revenue reports, and the other one uses it to train a machine learning model.</p> <p>There are several ways to implement cross-DAG dependencies in Airflow.</p> <ul> <li><code>TriggerDagOperator</code> is an operator that triggers a downstream DAG from any point in the DAG. It's similar to a push mechanism where the producer decides when to notify the consumers.</li> </ul> <pre><code>from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    TriggerDagRunOperator(\n        task_id=\"trigger_dagrun\",\n        trigger_dag_id=\"random_dag1\",\n        conf={},\n    )\n\nrandom_dag7()\n</code></pre> <ul> <li><code>ExternalTaskSensor</code> is a sensor operator for downstream DAGs to pull states of the upstream DAG, similar to a pull mechanism. The downstream DAG will wait until the task is completed in the upstream DAG.</li> </ul> <pre><code>from airflow.sensors.external_task import ExternalTaskSensor\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag8():\n\n    ExternalTaskSensor(\n        task_id=\"external_sensor\",\n        external_dag_id=\"random_dag3\",\n        external_task_id=\"python_operator\",\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n    )\n\nrandom_dag8()\n</code></pre> <p>Another method introduced in <code>version 2.4</code> uses datasets to create data-driven dependencies between DAGs.</p> <p>An Airflow dataset is a logical grouping of data updated by upstream tasks. The upstream task defines the output dataset via <code>outlets</code> parameter. The completion of the task means the successful update of the dataset.</p> <p>In downstream DAGs, instead of using a time-based schedule, the DAG refers to the corresponding dataset produced by the upstreams. Therefore, the downstream DAG will be triggered in a data-driven manner rather than a scheduled-based manner.</p> <pre><code>dag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag9_producer():\n    BashOperator(outlets=[dag1_dataset], task_id=\"producer\", bash_command=\"sleep 5\")\n\nrandom_dag9_producer()\n\n@dag(\n    schedule=[dag1_dataset],\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag9_consumer():\n    BashOperator(task_id=\"consumer\", bash_command=\"sleep 5\")\n\nrandom_dag9_consumer()\n</code></pre>"},{"location":"airflow/airflow/#best-practices","title":"Best practices","text":"<p>When working with Airflow, there are several best practices to keep in mind that help ensure our pipelines run smoothly and efficiently.</p>"},{"location":"airflow/airflow/#idempotency","title":"Idempotency","text":"<p>Idempotency is a fundamental concept for data pipelines. In the context of Airflow, idempotency means running the same DAG Run multiple times has the same effect as running it only once. When a DAG is designed to be idempotent, it can be executed repeatedly without causing unexpected changes to the pipeline's output.</p> <p>This is especially necessary when a DAG Run might be rerun due to failures or errors in the processing.</p> <p>Example</p> <p>An example to make DAG idempotent is to use templates such as variable <code>{{ execution_date }}</code>.</p> <p>It's associated with the expected scheduled time of each run, and the date won't be changed even if we rerun the DAG Run a few hours later.</p>"},{"location":"airflow/airflow/#avoid-top-level-code-in-the-dag-file","title":"Avoid top-level code in the DAG file","text":"<p>By default, Airflow reads the dag folder every 30 seconds, including the top-level code that is outside of DAG context.</p> <p>Because of this, having expensive top-level code, such as making requests to external APIs, can cause performance issues because they are called every 30 seconds rather than only when DAG is scheduled.</p> <p>The general advice is to limit the amount of top-level code in the DAG file and move it within the DAG context or operators.</p> <p>This can help reduce unnecessary overheads and allow Airflow to focus on executing the right things.</p> <p>The following example shows both good and bad ways of making an API request:</p> <pre><code># Bad example - requests will be made every 30 seconds instead of everyday at 4:30am\nres = requests.get(\"https://api.sampleapis.com/coffee/hot\")\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    @task\n    def python_operator() -&gt; None:\n        logging.info(f\"API result {res}\")\n    python_operator()\n\nrandom_dag7()\n\n# Good example\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    @task\n    def python_operator() -&gt; None:\n        res = requests.get(\"https://api.sampleapis.com/coffee/hot\") # move API request within DAG context\n        logging.info(f\"API result {res}\")\n    python_operator()\n\nrandom_dag7()\n</code></pre>"},{"location":"airflow/airflow/#how-to-execute-tasks-parallelly","title":"How to execute tasks parallelly?","text":"<pre><code>```bash\nstart_task = DummyOperator(task_id='start_task', dag=dag)\nparallel_task_1 = DummyOperator(task_id='parallel_task_1', dag=dag) \nparallel_task_2 = DummyOperator(task_id='parallel_task_2', dag=dag) \nparallel_task_3 = DummyOperator(task_id='parallel_task_3', dag=dag) \nend_task = DummyOperator(task_id='end_task', dag=dag)\n# Setting up the dependencies start_task &gt;&gt; [parallel_task_1, parallel_task_2, parallel_task_3] &gt;&gt; end_task\n```\n</code></pre> <p>In this example, the three parallel tasks (parallel_task_1, parallel_task_2, parallel_task_3) are specified as a list in the dependency chain. The start_task runs first. Once it completes, all three parallel tasks begin. When all three of them complete, the end_task starts.</p>"},{"location":"airflow/airflow/#how-to-overwrite-depends_on_past-property","title":"How to overwrite depends_on_past property?","text":"<p>The depends_on_past attribute in the default_args dictionary of a DAG applies globally to all tasks in the DAG when set. However, if you want to override this behavior for specific tasks, you can specify the depends_on_past attribute directly on those tasks.</p> <p>For specific tasks where you want to override this behavior, set depends_on_past directly on the task:     <pre><code>task_with_custom_dep = DummyOperator(     \n    task_id='task_with_custom_dep',\n    depends_on_past=False, \n    dag=dag\n    )\n</code></pre></p>"},{"location":"airflow/airflowiq/","title":"AIRFLOW_IQ","text":"<p>1. What is Airflow?</p> <p>Airflow is an open-source tool for programmatically authoring, scheduling, and monitoring data pipelines. Apache Airflow is an open source data orchestration tool that allows data practitioners to define data pipelines programmatically with the help of Python. Airflow is most commonly used by data engineering teams to integrate their data ecosystem and extract, transform, and load data.</p> <p>2. What issues does Airflow resolve?</p> <p>Crons are an old technique of task scheduling. Scalable Cron requires external assistance to log, track, and manage tasks. The Airflow UI is used to track and monitor the workflow's execution. Creating and maintaining a relationship between tasks in cron is a challenge, whereas it is as simple as writing Python code in Airflow. Cron jobs are not reproducible until they are configured externally. Airflow maintains an audit trail of all tasks completed.</p> <p>3. Explain how workflow is designed in Airflow?</p> <p>A directed acyclic graph (DAG) is used to design an Airflow workflow. That is to say, when creating a workflow, consider how it can be divided into tasks that can be completed independently. The tasks can then be combined into a graph to form a logical whole. The overall logic of your workflow is based on the shape of the graph. An Airflow DAG can have multiple branches, and you can choose which ones to follow and which to skip during workflow execution. Airflow Pipeline DAG Airflow could be completely stopped, and able to run workflows would then resume through restarting the last unfinished task. It is important to remember that airflow operators can be run more than once when designing airflow operators. Each task should be idempotent, or capable of being performed multiple times without causing unintended consequences.</p> <p>4. Explain Airflow Architecture and its components?</p> <p>Airflow has six main components:</p> <ul> <li>The web server for serving and updating the Airflow user interface.</li> <li>The metadata database for storing all metadata (e.g., users, tasks) related to your Airflow instance.</li> <li>The scheduler for monitoring and scheduling your pipelines.</li> <li>The executor for defining how and on which system tasks are executed.</li> <li>The queue for holding tasks that are ready to be executed.</li> <li>The worker(s) for executing instructions defined in a task.</li> </ul> <p>Airflow runs DAGs in six different steps:</p> <ol> <li>The scheduler constantly scans the DAGs directory for new files. The default time is every 5 minutes.</li> <li>After the scheduler detects a new DAG, the DAG is processed and serialized into the metadata database.</li> <li>The scheduler scans for DAGs that are ready to run in the metadata database. The default time is every 5 seconds.</li> <li>Once a DAG is ready to run, its tasks are put into the executor's queue.</li> <li>Once a worker is available, it will retrieve a task to execute from the queue.</li> <li>The worker will then execute the task.</li> </ol> <p>5. What are the types of Executors in Airflow?</p> <p>The executors are the components that actually execute the tasks, while the Scheduler orchestrates them. Airflow has different types of executors, including SequentialExecutor, LocalExecutor, CeleryExecutor and KubernetesExecutor. People generally choose the executor which is best for their use case.</p> <ul> <li>SequentialExecutor: Only one task is executed at a time by SequentialExecutor. The scheduler and the workers both use the same machine.</li> <li>LocalExecutor: LocalExecutor is the same as the Sequential Executor, except it can run multiple tasks at a time.</li> <li>CeleryExecutor: Celery is a Python framework for running distributed asynchronous tasks. As a result, CeleryExecutor has long been a part of Airflow, even before Kubernetes. CeleryExecutors has a fixed number of workers on standby to take on tasks when they become available.</li> <li>KubernetesExecutor: Each task is run by KubernetesExecutor in its own Kubernetes pod. It, unlike Celery, spins up worker pods on demand, allowing for the most efficient use of resources.</li> </ul> <p>6. What are the pros and cons of SequentialExecutor?</p> <ul> <li>Pros: It's simple and straightforward to set up. It's a good way to test DAGs while they're being developed.</li> <li>Cons: It isn't scalable. It is not possible to perform many tasks at the same time. Unsuitable for use in production.</li> </ul> <p>7. What are the pros and cons of LocalExecutor?</p> <ul> <li>Pros: Able to perform multiple tasks. Can be used to run DAGs during development.</li> <li>Cons: The product isn't scalable. There is only one point of failure. Unsuitable for use in production.</li> </ul> <p>8. What are the pros and cons of CeleryExecutor?</p> <ul> <li>Pros: It allows for scalability. Celery is responsible for managing the workers. Celery creates a new one in the case of a failure.</li> <li>Cons: Celery requires RabbitMQ/Redis for task queuing, which is redundant with what Airflow already supports. The setup is also complicated due to the above-mentioned dependencies.</li> </ul> <p>9. What are the pros and cons of KubernetesExecutor?</p> <ul> <li>Pros: It combines the benefits of CeleryExecutor and LocalExecutor in terms of scalability and simplicity. Fine-grained control over task-allocation resources. At the task level, the amount of CPU/memory needed can be configured.</li> <li>Cons: Airflow is newer to Kubernetes, and the documentation is complicated.</li> </ul> <p>10. How to define a workflow in Airflow?</p> <p>Python files are used to define workflows. DAG (Directed Acyclic Graph) The DAG Python class in Airflow allows you to generate a Directed Acyclic Graph, which is a representation of the workflow.</p> <p><pre><code>from airflow.models import DAG\nfrom airflow.utils.dates import days_ago\nargs = { 'start_date': days_ago(0) }\ndag = DAG(\n    dag_id='bash_operator_example',\n    default_args=args,\n    schedule_interval='* * * * *',\n)\n</code></pre> You can use the start date to launch a task on a specific date. The schedule interval specifies how often each workflow is scheduled to run. '* * * * *' indicates that the tasks must run every minute.</p> <p>11. How do you make the module available to airflow if you're using Docker Compose?</p> <p>If we are using Docker Compose, then we will need to use a custom image with our own additional dependencies in order to make the module available to Airflow. Refer to the following Airflow Documentation for reasons why we need it and how to do it.</p> <p>12. How to schedule DAG in Airflow?</p> <p>DAGs could be scheduled by passing a timedelta or a cron expression (or one of the @ presets), which works well enough for DAGs that need to run on a regular basis, but there are many more use cases that are presently difficult to express \"natively\" in Airflow, or that require some complicated workarounds.</p> <p>13. What is XComs In Airflow?</p> <p>XCom (short for cross-communication) are messages that allow data to be sent between tasks. The key, value, timestamp, and task/DAG id are all defined.</p> <p>14. What is xcom_pull in XCom Airflow?</p> <p>The xcom push and xcom pull methods on Task Instances are used to explicitly \"push\" and \"pull\" XComs to and from their storage. Whereas if do xcom push parameter is set to True (as it is by default), many operators and @task functions will auto-push their results into an XCom key named return value. If no key is supplied to xcom pull, it will use this key by default, allowing you to write code like this:</p> <pre><code>value = task_instance.xcom_pull(task_ids='pushing_task')\n</code></pre> <p>15. What is Jinja templates?</p> <p>Jinja is a templating engine that is quick, expressive, and extendable. The template has special placeholders that allow you to write code that looks like Python syntax. After that, data is passed to the template in order to render the final document.</p> <p>16. How to use Airflow XComs in Jinja templates?</p> <p>We can use XComs in Jinja templates as given below:</p> <pre><code>SELECT * FROM {{ task_instance.xcom_pull(task_ids='foo', key='table_name') }}\n</code></pre> <p>17. How does Apache Airflow act as a Solution?</p> <ul> <li>Failures: This tool assists in retrying in case there is a failure.</li> <li>Monitoring: It helps in checking if the status has been succeeded or failed.</li> <li>Dependency: There are two different types of dependencies, such as:<ul> <li>Data Dependencies that assist in upstreaming the data</li> <li>Execution Dependencies that assist in deploying all the new changes</li> </ul> </li> <li>Scalability: It helps centralize the scheduler</li> <li>Deployment: It is useful in deploying changes with ease</li> <li>Processing Historical Data: It is effective in backfilling historical data</li> </ul> <p>18. How would you design an Airflow DAG to process a large dataset?</p> <p>When designing an Airflow DAG to process a large dataset, there are several key considerations to keep in mind.</p> <ul> <li>The DAG should be designed to be modular and scalable. This means that the DAG should be broken down into smaller tasks that can be run in parallel, allowing for efficient processing of the data. Additionally, the DAG should be designed to be able to scale up or down depending on the size of the dataset.</li> <li>The DAG should be designed to be fault-tolerant. This means that the DAG should be designed to handle errors gracefully and be able to recover from them. This can be done by using Airflow's retry and catchup features, as well as by using Airflow's XCom feature to pass data between tasks.</li> <li>The DAG should be designed to be efficient. This means that the DAG should be designed to minimize the amount of data that needs to be processed and to minimize the amount of time it takes to process the data. This can be done by using Airflow's features such as branching, pooling, and scheduling.</li> <li>The DAG should be designed to be secure. This means that the DAG should be designed to protect the data from unauthorized access and to ensure that only authorized users can access the data. This can be done by using Airflow's authentication and authorization features.</li> </ul> <p>By following these guidelines, an Airflow DAG can be designed to efficiently and securely process a large dataset.</p> <p>19. What strategies have you used to optimize Airflow performance?</p> <p>When optimizing Airflow performance, I typically focus on three main areas:</p> <ul> <li>Utilizing the right hardware: Airflow is a distributed system, so it's important to ensure that the hardware you're using is up to the task. This means having enough memory, CPU, and disk space to handle the workload. Additionally, I make sure to use the latest version of Airflow, as this can help improve performance.</li> <li>Optimizing the DAGs: I make sure to optimize the DAGs by using the best practices for Airflow. This includes using the right operators, setting the right concurrency levels, and using the right execution dates. Additionally, I make sure to use the right parameters for the tasks, such as setting the right retry limits and timeouts.</li> <li>Utilizing the right tools: I make sure to use the right tools to monitor and analyze the performance of Airflow. This includes using the Airflow UI, the Airflow CLI, and the Airflow Profiler. Additionally, I make sure to use the right metrics to measure performance, such as task duration, task throughput, and task latency.</li> </ul> <p>By focusing on these three areas, I am able to optimize Airflow performance and ensure that the system is running as efficiently as possible.</p> <p>20. How do you debug an Airflow DAG when it fails?</p> <p>When debugging an Airflow DAG that has failed, the first step is to check the Airflow UI for the failed task. The UI will provide information about the task, such as the start and end time, the duration of the task, and the error message. This information can help to identify the cause of the failure.</p> <p>The next step is to check the Airflow logs for the failed task. The logs will provide more detailed information about the task, such as the exact command that was executed, the environment variables, and the stack trace. This information can help to pinpoint the exact cause of the failure.</p> <p>The third step is to check the code for the failed task. This can help to identify any errors in the code that may have caused the failure.</p> <p>Finally, if the cause of the failure is still not clear, it may be necessary to set up a debugging environment to step through the code and identify the exact cause of the failure. This can be done by setting up a local Airflow instance and running the DAG in debug mode. This will allow the developer to step through the code and identify the exact cause of the failure.</p> <p>21. What is the difference between a Directed Acyclic Graph (DAG) and a workflow in Airflow?</p> <p>A Directed Acyclic Graph (DAG) is a graph structure that consists of nodes and edges, where the edges represent the direction of the flow of data between the nodes. A DAG is acyclic, meaning that there are no loops or cycles in the graph. A DAG is used to represent the flow of data between tasks in a workflow.</p> <p>Airflow is a platform for programmatically authoring, scheduling, and monitoring workflows. Airflow uses DAGs to define workflows as a collection of tasks. A workflow in Airflow is a DAG that is composed of tasks that are organized in a way that reflects their relationships and dependencies. The tasks in a workflow are connected by edges that represent the flow of data between them.</p> <p>The main difference between a DAG and a workflow in Airflow is that a DAG is a graph structure that is used to represent the flow of data between tasks, while a workflow in Airflow is a DAG that is composed of tasks that are organized in a way that reflects their relationships and dependencies.</p> <p>22. How do you handle data dependencies in Airflow?</p> <p>Data dependencies in Airflow are managed using the concept of Operators. Operators are the building blocks of an Airflow workflow and are used to define tasks that need to be executed. Each Operator is responsible for a specific task and can be configured to handle data dependencies.</p> <p>For example, the PythonOperator can be used to define a task that runs a Python script. This script can be configured to read data from a source, process it, and write the results to a destination. The PythonOperator can also be configured to wait for a certain set of data to be available before executing the task.</p> <p>The TriggerRule parameter of an Operator can also be used to define data dependencies. This parameter can be used to specify the conditions that must be met before the task is executed. For example, a task can be configured to run only when a certain file is present in a certain directory.</p> <p>Finally, the ExternalTaskSensor Operator can be used to wait for the completion of a task in another DAG before executing a task. This is useful when a task in one DAG depends on the completion of a task in another DAG.</p> <p>23. How do you ensure data integrity when using Airflow?</p> <p>Data integrity is an important consideration when using Airflow. To ensure data integrity when using Airflow, I would recommend the following best practices:</p> <ul> <li>Use Airflow's built-in logging and monitoring features to track data changes and detect any anomalies. This will help you identify any potential issues with data integrity.</li> <li>Use Airflow's built-in data validation features to ensure that data is accurate and complete. This will help you ensure that data is consistent and reliable.</li> <li>Use Airflow's built-in scheduling and task management features to ensure that data is processed in a timely manner. This will help you ensure that data is up-to-date and accurate.</li> <li>Use Airflow's built-in security features to protect data from unauthorized access. This will help you ensure that data is secure and protected.</li> <li>Use Airflow's built-in data backup and recovery features to ensure that data is recoverable in the event of a system failure. This will help you ensure that data is not lost in the event of a system failure.</li> </ul> <p>By following these best practices, you can ensure that data integrity is maintained when using Airflow.</p> <p>24. How do you handle data security when using Airflow?</p> <p>When using Airflow, data security is of utmost importance. To ensure data security, I take the following steps:</p> <ul> <li>I use secure authentication methods such as OAuth2 and Kerberos to authenticate users and restrict access to the Airflow environment.</li> <li>I use encryption for data in transit and at rest. This includes encrypting data stored in databases, files, and other storage systems.</li> <li>I use secure protocols such as HTTPS and SFTP to transfer data between systems.</li> <li>I use role-based access control (RBAC) to restrict access to sensitive data and resources.</li> <li>I use logging and monitoring tools to detect and respond to security incidents.</li> <li>I use vulnerability scanning tools to identify and address potential security issues.</li> <li>I use secure coding practices to ensure that the code is secure and free from vulnerabilities.</li> <li>I use secure configuration management to ensure that the Airflow environment is configured securely.</li> <li>I use secure deployment processes to ensure that the Airflow environment is deployed securely.</li> <li>I use secure backup and disaster recovery processes to ensure that data is backed up and can be recovered in the event of a disaster.</li> </ul> <p>25. How do you ensure scalability when using Airflow?</p> <p>When using Airflow, scalability can be achieved by following a few best practices.</p> <ul> <li>First, it is important to ensure that the Airflow DAGs are designed in a way that allows them to be easily scaled up or down. This can be done by using modular components that can be reused and scaled independently. Additionally, it is important to use Airflow's built-in features such as the ability to set up multiple workers and the ability to set up multiple DAGs. This allows for the DAGs to be scaled up or down as needed.</li> <li>Second, it is important to use Airflow's built-in features to ensure that the DAGs are running efficiently. This includes using Airflow's scheduling capabilities to ensure that tasks are running at the right time and using Airflow's logging capabilities to ensure that tasks are running correctly. Additionally, it is important to use Airflow's built-in features to ensure that tasks are running in the most efficient way possible. This includes using Airflow's task retry capabilities to ensure that tasks are retried if they fail and using Airflow's task concurrency capabilities to ensure that tasks are running in parallel.</li> <li>Finally, it is important to use Airflow's built-in features to ensure that the DAGs are running securely. This includes using Airflow's authentication and authorization capabilities to ensure that only authorized users can access the DAGs and using Airflow's encryption capabilities to ensure that the data is secure.</li> </ul> <p>By following these best practices, scalability can be achieved when using Airflow.</p> <p>26. What are Variables (Variable Class) in Apache Airflow?</p> <p>Variables are a general way to store and retrieve content or settings as a simple key-value pair within Airflow. Variables in Airflow can be listed, created, updated, and deleted from the UI. Technically, Variables are Airflow's runtime configuration concept.</p> <p>27. Why don't we use Variables instead of Airflow XComs, and how are they different?</p> <p>An XCom is identified by a \"key,\" \"dag id,\" and the \"task id\" it had been called from. These work just like variables but are alive for a short time while the communication is being done within a DAG. In contrast, the variables are global and can be used throughout the execution for configurations or value sharing.</p> <p>There might be multiple instances when multiple tasks have multiple task dependencies; defining a variable for each instance and deleting them at quick successions would not be suitable for any process's time and space complexity.</p> <p>28. What are the states a Task can be in? Define an ideal task flow.</p> <p>Just like the state of a DAG (directed acyclic graph) being running is called a \"DAG run\", the tasks within that dag can have several tasks instances. they can be:</p> <ul> <li>none: the task is defined, but the dependencies are not met.</li> <li>scheduled: the task dependencies are met, has got assigned a scheduled interval, and are ready for a run.</li> <li>queued: the task is assigned to an executor, waiting to be picked up by a worker.</li> <li>running: the task is running on a worker.</li> <li>success: the task has finished running, and got no errors.</li> <li>shutdown: the task got interrupted externally to shut down while it was running.</li> <li>restarting: the task got interrupted externally to restart while it was running.</li> <li>failed: the task encountered an error.</li> <li>skipped: the task got skipped during a dag run due to branching (another topic for airflow interview, will cover branching some reads later)</li> <li>upstream_failed: An upstream task failed (the task on which this task had dependencies).</li> <li>up_for_retry: the task had failed but is ongoing retry attempts.</li> <li>up_for_reschedule: the task is waiting for its dependencies to be met (It is called the \"Sensor\" mode).</li> <li>deferred: the task has been postponed.</li> <li>removed: the task has been taken out from the DAG while it was running.</li> </ul> <p>Ideally, the expected order of tasks should be : none -&gt; scheduled -&gt; queued -&gt; running -&gt; success.</p> <p>30. What is the role of Airflow Operators?</p> <p>There are three main types of operators: - Action: Perform a specific action such as running code or a bash command. - Transfer: Perform transfer operations that move data between two systems. - Sensor: Wait for a specific condition to be met (e.g., waiting for a file to be present) before running the next task</p> <p>31. What is Branching in Directed Acyclic Graphs (DAGs)?</p> <p>Branching tells the DAG to run all dependent tasks, but you can choose which Task to move onto based on a condition. A task_id (or list of task_ids) is given to the \"BranchPythonOperator\", the task_ids are followed, and all other paths are skipped. It can also be \"None\" to ignore all downstream tasks.</p> <p>Even if tasks \"branch_a\" and \"join\" both are directly downstream to the branching operator, \"join\" will be executed for sure if \"branch_a\" will get executed, even if \"join\" is ruled out of the branching condition.</p> <p>32. What are ways to Control Airflow Workflow?</p> <p>By default, a DAG will only run an airflow task when all its Task dependencies are finished and successful. However, there are several ways to modify this:</p> <ul> <li>Branching (BranchPythonOperator): We can apply multiple branches or conditional limits to what path the flow should go after this task.</li> <li>Latest Only (LatestOnlyOperator): This task will only run if the date the DAG is running is on the current data. It will help in cases when you have a few tasks which you don't want to run while backfilling historical data.</li> <li>Depends on Past (depends_on_past = true; arg): Will only run if this task run succeeded in the previous DAG run.</li> <li>Trigger rules (\"trigger_rule\"; arg): By default, a DAG will only run an airflow task when all of its previous tasks have succeeded, but trigger rules can help us alter those conditions. Like \"trigger_rule = always\" to run it anyways, irrespective of if the previous tasks succeeded or not, OR \"trigger_rule = all_success\" to run it only when all of its previous jobs succeed.</li> </ul> <p>33. Explain the External task Sensor?</p> <p>An External task Sensor is used to sense the completion status of a DAG_A from DAG_B or vice-versa. If two tasks are in the same Airflow DAG we can simply add the line of dependencies between the two tasks. But Since these two are completely different DAGs, we cannot do this.</p> <p>We can Define an ExternalTaskSensor in DAG_B if we want DAG_B to wait for the completion of DAG_A for a specific execution date.</p> <p>There are six parameters to an External Task Sensor: - external_dag_id: The DAG Id of the DAG, which contains the task which needs to be sensed. - external_task_id: The Task Id of the task to be monitored. If set to default(None), the external task sensor waits for the entire DAG to complete. - allowed_states: The task state at which it needs to be sensed. The default is \"success.\" - execution_delta: Time difference with the previous execution, which is needed to be sensed; the default is the same execution_date as the current DAG. - execution_date_fn: It's a callback function that returns the desired execution dates to the query.</p> <p>34. What is TaskFlow API? and how is it helpful?</p> <p>We have read about Airflow XComs (cross-communication) and how it helps to transfer data/messages between tasks and fulfill data dependencies. There are two basic commands of XComs which are \"xcompull\" used to pull a list of return values from one or multiple tasks and \"xcom_push\" used for pushing a value to the Airflow XComs.</p> <p>Now, Imagine you have ten tasks, and all of them have 5-6 data dependencies on other tasks; writing an xcom_pull and x_push for passing values between tasks can get tedious.</p> <p>So TaskFlow API is an abstraction of the whole process of maintaining task relations and helps in making it easier to author DAGs without extra code, So you get a natural flow to define tasks and dependencies.</p> <p>_Note: TaskFlow API was introduced in the later version of Airflow, i.e., Airflow 2.0. So can be of minor concern in airflow interview questions.</p> <p>35. How are Connections used in Apache Airflow?</p> <p>Apache Airflow is often used to pull and push data into other APIs or systems via hooks that are responsible for the connection. But since hooks are the intermediate part of the communication between the external system and our dag task, we can not use them to contain any personal information like authorization credentials, etc. Now let us assume the external system here is referred to as a MySQL database. We do need credentials to access MySQL, right? So where does the \"Hook\" get the credentials from?</p> <p>That's the role of \"Connection\" in Airflow.</p> <p>Airflow has a Connection concept for storing credentials that are used to talk to external systems. A Connection is a set of parameters - such as login username, password, and hostname - along with the system type it connects to and a unique id called the \"conn_id\".</p> <p>If the connections are stored in the metadata database, metadata database airflow supports the use of \"Fernet\" (an encryption technique) to encrypt the password and other sensitive data.</p> <p>Connections can be created in multiple ways: - Creating them directly from the airflow UI. - Using Environment Variables. - Using Airflow's REST API. - Setting it up in the airflows configuration file itself \"airflow.cfg\". - Using airflow CLI (Command Line Interface)</p> <p>36. Explain Dynamic DAGs.</p> <p>Dynamic-directed acyclic graphs are nothing but a way to create multiple DAGs without defining each of them explicitly. This is one of the major qualities of apache airflow, which makes it a supreme \"workflow orchestration tool\".</p> <p>Let us say you have ten different tables to modify every day in your MySQL database, so you create ten DAG's to upload the respective data to their respective databases. Now think if the table names change, would you go to each dag and change the table names? Or make new dags for them? Certainly not, because sometimes there can be hundreds of tables.</p> <p>37. How to control the parallelism or concurrency of tasks in Apache Airflow configuration?</p> <p>Concurrency is the number of tasks allowed to run simultaneously. This can be set directly in the airflow configurations for all dags in the Airflow, or it can be set per DAG level. Below are a few ways to handle it: - In config :     - parallelism: maximum number of tasks that can run concurrently per scheduler across all dags.     - max_active_tasks_per_dag: maximum number of tasks that can be scheduled at once.     - max_active_runs_per_dag: . the maximum number of running tasks at once. - DAG level (as an argument to an Individual DAG) :     - concurrency: maximum number of tasks that can run concurrently in this dag.     - max_active_runs: maximum number of active runs for this DAG. The scheduler will not create new DAG runs once the limit hits.</p> <p>38. What are Macros in Airflow?</p> <p>Macros are functions used as variables. In Airflow, you can access macros via the \"macros\" library. There are pre-defined macros in Airflow that can help in calculating the time difference between two dates or more! But we can also define macros by ourselves to be used by other macros as well, like we can use a macro to dynamically generate the file path for a file. Some of the examples of pre-defined and most-used macros are: - Airflow.macros.datetimediff_for_humans(dt, _since=None): Returns difference between two datetimes, or one and now. (Since = None refers to \"now\")** - airflow.macros.dsadd(_ds, numberof__days) : Add or subtract n number of days from a YYYY-MM-DD(ds), will subtract if number_of_days is negative.</p> <p>39. List the types of Trigger rules.</p> <ul> <li>all_success: the task gets triggered when all upstream tasks have succeeded.</li> <li>all_failed: the task gets triggered if all of its parent tasks have failed.</li> <li>all_done: the task gets triggered once all upstream tasks are done with their execution irrespective of their state, success, or failure.</li> <li>one_failed: the task gets triggered if any one of the upstream tasks gets failed.</li> <li>one_success: the task gets triggered if any one of the upstream tasks gets succeeds.</li> <li>none_failed: the task gets triggered if all upstream tasks have finished successfully or been skipped.</li> <li>none_skipped: the task gets triggered if no upstream tasks are skipped, irrespective of if they succeeded or failed.</li> </ul> <p>40. What are SLAs?</p> <p>SLA stands for Service Level Agreement; this is a time by which a task or a DAG should have succeeded. If an SLA is missed, an email alert is sent out as per the system configuration, and a note is made in the log. To view the SLA misses, we can access it in the web UI.</p> <p>It can be set at a task level using the \"timedelta\" object as an argument to the Operator, as sla = timedelta(seconds=30).</p> <p>41. What is Data Lineage?</p> <p>Many times, we may encounter an error while processing data. To determine the root cause of this error, we may need to track the path of the data transformation and find where the error occurred. If we have a complex data system then it would be challenging to investigate its root. Lineage allows us to track the origins of data, what happened to it, and how did it move over time, such as in S3, HDFS, MySQL or Hive, etc. It becomes very useful when we have multiple data tasks reading and writing into storage. We need to define the input and the output data sources for each task, and a graph is created in Apache Atlas, which depicts the relationships between various data sources.</p> <p>42. What if your Apache Airflow DAG failed for the last ten days, and now you want to backfill those last ten days' data, but you don't need to run all the tasks of the dag to backfill the data?</p> <p>We can use the Latest Only (LatestOnlyOperator) for such a case. While defining a task, we can set the latest_only to True for those tasks, which we do not need to use for backfilling the previous ten days' data.</p> <p>43. What will happen if you set 'catchup=False' in the dag and 'latest_only = True' for some of the dag tasks?</p> <p>Since in the dag definition, we have set catchup to False, the dag will only run for the current date, irrespective of whether latest_only is set to True or False in any one or all the tasks of the dag. 'catchup = False' will just ensure you do not need to set latest_only to True for all the tasks.</p> <p>44. How would you handle a task which has no dependencies on any other tasks?</p> <p>We can set \"trigger_rules = 'always'\" in a task, which will make sure the task will run irrespective of if the previous tasks have succeeded or not.</p> <p>45. How can you use a set or a subset of parameters in some of the dags tasks without explicitly defining them in each task?</p> <p>We can use the \"params\" argument. It is a dictionary of DAG-level parameters that are made accessible in jinja templates. These \"params\" can be used at the task level. We can pass \"params\" as a parameter to our dag as a dictionary of parameters such as {\"param1\": \"value1\", \"param2\": \"value2\"}. And these can be used as \"echo {{params.param1}}\" in a bash operator.</p> <p>46. What Executor will you use to test multiple jobs at a low scale?</p> <p>Local Executor is ideal for testing multiple jobs in parallel for performing tasks for a smallscale production environment. The Local Executor runs the tasks on the same node as the scheduler but on different processors. There are other executors as well who use this style while distributing the work. Like, Kubernetes Executor would also use Local Executor within each pod to run the task.</p> <p>47. If we want to exchange large amounts of data, what is the solution to the limitation of XComs?</p> <p>Since Airflow is an orchestrator tool and not a data processing framework, if we want to process large gigabytes of data with Airflow, we use Spark (which is an open-source distributed system for large-scale data processing) along with the Airflow DAGs because of all the optimizations that It brings to the table.</p> <p>48. What would you do if you wanted to create multiple dags with similar functionalities but with different arguments?</p> <p>We can use the concept of Dynamic DAGs generation. We can define a create_dag method which can take a fixed number of arguments, but the arguments will be dynamic. The dynamic arguments can be passed to the create_dag method through Variables, Connections, Config Files, or just passing a hard-coded value to the method.</p> <p>49. Is there any way to restrict the number of variables to be used in your directed acyclic graph, and why would we need to do that?</p> <p>Airflow Variables are stored in the Metadata Database, so any call to a variable would mean a connection to the database. Since our DAG files are parsed every X seconds, using a large number of variables in our DAG might end up saturating the number of allowed connections to our database. To tackle that, we can just use a single Airflow variable as a JSON, as an Airflow variable can contain JSON values such as {\"var1\": \"value1\", \"var2\": \"value2\"}.</p> <p>50. How can you use a set or a subset of parameters in some of the dags tasks without explicitly defining them in each task?</p> <p>We can use the \"params\" argument. It is a dictionary of DAG-level parameters that are made accessible in jinja templates. These \"params\" can be used at the task level. We can pass \"params\" as a parameter to our dag as a dictionary of parameters such as {\"param1\": \"value1\", \"param2\": \"value2\"}. And these can be used as \"echo {{params.param1}}\" in a bash operator.</p>"},{"location":"airflow/dataorchestration/","title":"Data Orchestration","text":"<p>To support business continuity, data models need to be regularly refreshed. In the past, engineers used the cron tool in Linux systems to schedule ELT jobs.</p> <p>However, as data volume and system complexity increase exponentially, creating cron jobs becomes a bottleneck and eventually hits the limitation of scalability and maintainability.</p> <p>To be more specific, the problems are:</p> <ul> <li>Dependencies between jobs: In a large-scale data team, it's expected to have many dependencies between data models.</li> </ul> <p>Example</p> <p>updating the revenue table should be done only after the sales table has been updated.</p> <p>In more complicated scenarios, a table can have multiple upstream dependencies, each with a different schedule. Managing all these dependencies manually is time-consuming and error-prone.</p> <ul> <li> <p>Performance: If not managed well, cron jobs can consume a significant amount of system resources such as CPU, memory, and disk space. With the ever-increasing volume of data, performance can quickly become an issue.</p> </li> <li> <p>Engineering efforts: To maintain the quality of dozens of cron jobs or apps and process a variety of data formats, data engineers have to spend a significant amount of time writing low-level code rather than creating new data pipelines.</p> </li> <li> <p>Data silos: Scattered cron jobs can easily lead to data silos, resulting in duplicated efforts, conflicting data, and inconsistent data quality. Enforcing data governance policies can also be difficult, leading to potential security issues.</p> </li> </ul> <p>The emergence of data orchestration marks a significant step in the evolution of modern data stacks.</p> <p>Data orchestration is an automated process that combines and organizes data from multiple sources, making it ready for use by data consumers. Orchestrators ease the workload of data engineering teams by providing prebuilt solutions for scheduling, monitoring, and infrastructure setup.</p>"},{"location":"airflow/dataorchestration/#tasks-of-data-orchestration","title":"Tasks of data orchestration","text":"<p>The actual tasks of data orchestration can vary from system to system, but essentially it consists of 3 parts as described:</p> <ul> <li>Data collection</li> <li>Data unification</li> <li>Data activation</li> </ul> <p>Note</p> <p>it's worth noting that data orchestration is not a database engine.</p> <p>It's a platform that schedules different jobs to run at the right time, in the right order and in the right way.</p> <p>It's common for a company to use multiple data orchestration platforms. This is because each platform may perform different tasks and target different users.</p> <p>Example</p> <p>For instance, tools like Airflow and Prefect are used for creating and managing complex workflows and are, therefore, mostly used by data engineers.</p> <p>On the other hand, **dbt* is focused on the data unification stage and is heavily adopted by data analysts and data scientists.</p> <p>Note</p> <p>One of the challenges of using multiple data orchestration platforms is managing the dependency and lineage between the platforms, if any exist.</p> <p>Fortunately, tool integration is becoming increasingly common.</p>"},{"location":"airflow/dataorchestration/#data-collection","title":"Data collection","text":"<p>Within an organization, data may come from multiple sources and in various formats. Adapting every single format from every single source can be time-consuming. Data orchestration automates the process of collecting data from disparate sources with little to no human effort.</p> <p>Example</p> <p>Many data orchestration platforms have easy integration with various tools such as Google Sheets, CSV files, BigQuery, Zendesk support, which speeds up the onboarding process of a new data source.</p> <p>This is expandable to other complex data format, such as Parquet, Avro, ORC, or delta format.</p> <p>Data orchestration provides the additional advantage of reducing data silos. The platform can swiftly access data from anywhere, whether it's from legacy systems, data warehouses, or the cloud.</p> <p>This prevents data from being trapped in a single location and makes it easily accessible.</p>"},{"location":"airflow/dataorchestration/#data-unification","title":"Data unification","text":"<p>Data inevitably needs to be unified and converted into a standard format.</p> <p>There are three types of platforms in this category, and all the platforms allow users to schedule jobs.</p> <ul> <li>Platforms that don't interfere with the data transformation logic.   Users can use any method to transform data, such as SQL, Python, or Bash scripts.</li> </ul> <p>Example</p> <p>Airflow is only responsible for submitting the data transformation job created by the user to a data warehouse and waiting for the result. It doesn't care how the user implements the job.</p> <ul> <li> <p>Platforms that manage the data transformation logic.   Platforms like dbt define their own way of transforming data and managing model dependency within the tool. They guide users to use optimized methods to transform data.</p> </li> <li> <p>Platforms that manage the data transformation logic through UI.   Certain low-code or no-code tools only require users to define transformation logic through user-friendly UI. These tools open the doors to non-engineers, but their functionalities may be limited.</p> </li> </ul>"},{"location":"airflow/dataorchestration/#data-activation","title":"Data activation","text":"<p>Business users who use the dashboards can define Service Level Agreements (SLA), meaning that the data must be ready before a certain time.</p> <p>Note</p> <p>If not, the orchestrator will notify the stakeholders through one of the communication channels.</p> <p>In addition to the above steps, orchestration platforms monitor the data life cycle in real time through a UI, where users can manually intervene with the process if needed.</p> <p>Another critical feature is dependency management. Before going into specific platforms, it's important to understand an important concept in data orchestration: directed acyclic graph (DAG).</p>"},{"location":"airflow/dataorchestration/#directed-acyclic-graph-dag","title":"Directed Acyclic Graph (DAG)","text":"<p>DAG is the bedrock of managing dependencies between different tasks.</p> <p>A DAG is a graphical representation of a series of tasks in the data pipeline. By its name, we can tell that it is a graph, but with two conditions:</p> <ul> <li> <p>Directed: Every edge in the graph points in one direction.</p> </li> <li> <p>Acyclic: The graph doesn't have directed cycles.</p> </li> </ul> <p>Considered the DAG below. In this DAG, data can go from A to B, but never from B to A. Nodes from which an edge extends are upstream nodes, while nodes at the receiving end of the edge are downstream nodes. In a DAG, nodes can have multiple upstream and downstream nodes.</p> <p></p> <p>Danger</p> <p>In addition, nodes can never inform themselves because this could create an infinite loop.</p> <p>For example, data cannot go from E to A. Otherwise, A becomes the downstream of E as well as the upstream.</p>"},{"location":"airflow/dataorchestration/#advantages","title":"Advantages","text":"<p>DAG is a fundamental concept in data orchestration for a few reasons:</p> <ul> <li> <p>It ensures that there is no infinite loop in the data pipeline. The scenario where the pipeline could run forever will not happen.</p> </li> <li> <p>It ensures the consistent execution order of tasks. Tasks will be executed in the same order every day.</p> </li> <li> <p>The graph helps us to visualize the dependencies in a user-friendly way. We can break down a complex task into smaller subtasks, making it easier to debug and maintain.</p> </li> <li> <p>It opens the possibility of parallelism. Independent tasks can be executed in parallel. For example, nodes B and C can run simultaneously.</p> </li> </ul> <p>Info</p> <p>In general, data orchestration connects the dots and enables data to flow in a consistent and manageable way.</p> <p>With a solid orchestration platform in place, organizations can quickly and efficiently scale their data volumes and deliver high-quality data to stakeholders for better decision-making.</p>"},{"location":"cloud/databricks/","title":"Overview","text":""},{"location":"cloud/databricks/#challenges-in-a-normal-day-to-day-data-platform","title":"Challenges in a Normal Day-to-Day Data Platform","text":"<ol> <li> <p>Too Many Tools and Integration Issues:</p> <p>Organizations often use a multitude of separate tools for different tasks, such as data warehousing, ETL (Extract, Transform, Load), running Spark jobs, saving data to data lakes, orchestration, AI/ML solutions, and BI (Business Intelligence) reporting.</p> <p>Each of these tools needs to integrate properly with one another to function effectively. For instance, if a BI dashboard doesn't integrate with a data warehousing solution, proper results cannot be obtained.</p> <p>Furthermore, governance (handling security, lineage, and metadata) must work across all these tools; otherwise, data leaks and security issues can arise. The complexity and challenges increase significantly when dealing with numerous individual tools.</p> </li> <li> <p>Proprietary Solutions or Vendor Lock-in:</p> <p>Many data warehousing solutions are proprietary, meaning they require data to be stored in their specific, encoded format. This creates a vendor lock-in, preventing direct communication with or extraction of data without using the vendor's data engine. If the vendor's solution is not used, accessing and reading the data becomes impossible.</p> <p>Databricks addresses this by providing open-source solutions. Data can reside in an organization's data cloud platform in open-source formats like Parquet, CSV, or Avro.</p> <p>On top of this, Databricks uses an open-source data engine called Delta Lake, which communicates with the data. This allows users the freedom to switch vendors if desired, as their data remains accessible in an open format within their data lake, ensuring no vendor lock-in.</p> </li> <li> <p>Data Silos:</p> <p>Traditional platforms often have separate data lakes (used for AI/ML solutions and ETL jobs) and data warehousing solutions (used by BI tools). This leads to duplicate copies of data. Data is often moved from the data lake to populate a separate data warehouse, resulting in the same data existing in different places, possibly with different owners.</p> <p>Maintaining multiple copies by different owners is a significant challenge. Databricks tackles this by merging the data lake and data warehouse into a Data Lakehouse</p> </li> </ol> <p>The Data Lakehouse Solution and Delta Lake</p> <p>The Data Lakehouse is essentially a combination of a data lake and a data warehouse. With a Data Lakehouse, data remains in the data lake in open formats (e.g., Parquet, CSV). On top of this data, an engine called Delta Lake communicates with the data lake to provide the functionalities of a Data Lakehouse.</p> <p>Delta Lake provides properties similar to RDBMS systems, including:</p> <ol> <li>ACID transactions (Atomicity, Consistency, Isolation, Durability).</li> <li>Versioning of data.</li> <li>Transaction logs.</li> <li>Ability to restore data from different versions.</li> <li>Audit history.</li> </ol> <p>This means a single copy of data in the data lake can serve both AI/ML purposes and BI purposes, eliminating the need to move data to proprietary data warehousing solutions. This is the core benefit of the Data Lakehouse: combining the advantages of both data warehousing and data lake on the same platform. In Databricks, the Data Lakehouse is powered by Delta Lake, an open-source solution.</p> <p>Databricks Lakehouse Platform Architecture</p> <p></p> <p>The Databricks Lakehouse platform is built in layers:</p> <ol> <li>Bottom Layer - Cloud and Data Storage:     Users can utilize any major cloud provider: Azure, GCP, or AWS.     Data is stored in the data lake within the chosen cloud provider.</li> <li>Data Lakehouse Engine - Delta Lake:     Delta Lake sits on top of the data lake and is the core functionality that empowers the Data Lakehouse.</li> <li>Unified Governance - Unity Catalog:     Databricks provides another open solution called Unity Catalog for unified governance, handling security, lineage, and metadata across the platform.</li> <li>Data Intelligence Engine:     Built on top of the Data Lakehouse, this engine is governed by Unity Catalog and provides insights from data. This makes Databricks a data intelligence platform.</li> <li> <p>Top Layer - Personas and Tools:     The platform provides tools for all three key personas:</p> <p>Data Engineering: Uses jobs, workflows, notebooks, and Spark scripts.</p> <p>Data Analysts: Utilize Databricks SQL and dashboards.</p> <p>Data Scientists: Have access to AI/ML solutions provided by Databricks.</p> </li> </ol> <p>Databricks Data Intelligence Platform</p> <p>The Databricks Data Intelligence Platform is defined as Data Lakehouse plus Generative AI. Generative AI provides the platform with its power for natural language and allows enterprises to gain insights from their enterprise data. Therefore, Databricks is called a data intelligence platform because it combines the benefits of a Data Lakehouse with Generative AI capabilities.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#what-is-databricks","title":"What is Databricks?","text":"<p>Databricks is a cloud-based platform built on Apache Spark that provides a collaborative environment for big data processing and analytics. It offers an integrated workspace where data engineers, data scientists, and analysts can work together to leverage the power of Spark for various use cases.</p> <p>Databricks is important because it makes it easier to use a Apache Spark. Instead of having to worry about all the technical stuff behind the scenes, Databricks gives you a simple and friendly way to use Spark. It takes care of all the complicated setup and management stuff so that you can focus on working with your data and doing cool analytics tasks. It\u2019s like having a magic helper that takes care of the boring stuff, so you can have more fun exploring and analyzing your data.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#architecture","title":"Architecture","text":"<p>Working with Databricks \u2013 Initial Setup</p> <ol> <li> <p>Cloud Providers: To work with Databricks, users can choose any of three major cloud providers: AWS, Azure, or GCP. If an organization already has a preferred cloud partner, they can use Databricks with that same partner.</p> </li> <li> <p>Databricks Account: The first step is to create a Databricks account.</p> </li> <li> <p>Workspaces: Once an account is created, users can then create one or more workspaces.     A single Databricks account can manage multiple workspaces.     For example, an account can have separate workspaces for development (Dev), user acceptance testing (UAT), and production (Prod) environments. This allows for segregation of different workspaces and assignment of different permissions to various users.     All work, including writing code, creating clusters, and working with data, is done within these workspaces.</p> </li> <li> <p>Account-Level Management: At the account level, various administrative tasks can be performed, such as:     Creating metastores and assigning them to different workspaces.     Assigning users to different workspaces.     Managing groups or service principals.     These elements can be managed at the account level and assigned to workspaces as per requirements.     An account can be used to create and manage multiple workspaces, including new ones if needed later (e.g., for pre-production).</p> </li> </ol> <p>Workspace URLs</p> <p>When a new workspace is created, it gets a unique Workspace ID (a number).</p> <p>A web URL is generated for the workspace, which users access to work within Databricks. These URLs often follow patterns like https://adb-.azuredatabricks.net or https://adb-.cloud.databricks.com. <p>All users connect to this web URL to perform their tasks, regardless of their persona (Data Engineering, Data Analytics, or Data Scientist). Access to these URLs can be managed at the account level by assigning specific users to different workspaces.</p> <p>High-Level Architecture of Databricks</p> <p>The high-level architecture of Databricks consists of two main parts:</p> <ol> <li> <p>Control Plane:     The control plane is managed by Databricks and resides within the Databricks cloud account.     Its primary purpose is to manage Databricks' backend services.     It manages the web application used to interact with the workspace.     It also handles information like notebook configurations, cluster configurations, job information, and logs required to manage the data plane.     The main purpose of the control plane is to orchestrate and provide configurations necessary to run jobs, clusters, and code.</p> </li> <li> <p>Data Plane:     The data plane resides within the customer's cloud account.     This is where customer data is processed.     Client data always resides in the customer's cloud account within the data plane, never at the control plane.     Clusters created to process this data are also created and run within the customer's cloud account (data plane). These clusters are managed by configurations from the control plane.     Processed data is saved back to the client's cloud account only. There is no data movement to the control plane; configurations and access are managed by the control plane, but data remains in the data plane.     If a cluster needs to connect to external data sources (e.g., MySQL, a different data lake), it will connect and process that data within the data plane.</p> </li> </ol> <p>Roles in Databricks</p> <p>There are four major roles involved in a Databricks platform:</p> <ol> <li> <p>Account Administrator:     Main tasks include creating workspaces.     Works with metastores.     Responsible for user management and assigning permissions to users for different workspaces. These are the three major tasks, among others.</p> </li> <li> <p>Metastore Administrator:     Main purpose is to create catalogs and manage data objects. (Catalogs will be discussed in upcoming sessions).     Can delegate required privileges to different users or owners.</p> </li> <li> <p>Workspace Administrator:     This role is the administrator at the workspace level, similar to how an account administrator is at the account level.     Manages users at the workspace level.     Manages workspace assets.     Decides the privileges for workspace assets.</p> </li> <li> <p>Owner:     An owner is the user who creates an object (e.g., a table, a schema).     The owner of an object can delegate permissions for that object to different users.</p> </li> </ol> <p>While these roles might seem confusing initially, their responsibilities will be demonstrated with live examples in upcoming sessions</p> <p>Databricks Account Console Overview</p> <p>The Account Console is where users can manage their Databricks account and associated workspaces from a single place. This is particularly useful for organizations with many workspaces (e.g., 50-60), as it simplifies management.</p> <p>Features and Settings in the Account Console</p> <ol> <li> <p>Workspace Management:     The Account Console displays workspaces tied to the account, such as the one created in the previous video.     Users can click on a workspace to view its settings.     Security and Compliance: Within the workspace settings, specific security and compliance measures can be applied to that workspace.     Configuration and Permissions (Unity Catalog): Once Unity Catalog is enabled, a \"permissions\" tab becomes active. From here, specific users, groups, or service principals can be granted access to that particular workspace. This tab allows for controlling access to each workspace.</p> </li> <li> <p>Catalog:     This tab is used to create metastores.     A metastore is the top-level container for Unity Catalog.     To enable Unity Catalog, a metastore must first be created. The creation of a metastore will be covered in a subsequent video.</p> </li> <li> <p>User Management (Most Important Tab):</p> <p>This section allows for managing users, service principals, and groups. Users: Users created in Azure Entra ID (or equivalent identity provider for other clouds) are listed here. For each user, different roles can be defined. For example, a user can be enabled or disabled as an Account Admin or Marketplace Admin. Only Account Admins have the privileges to manage users from the Account Console. Service Principals: These are referred to as robot accounts, meaning they act as a user and can perform jobs on behalf of a user. It is considered best practice to use service principals, not user accounts, for running jobs in a production environment (e.g., if a user leaves the organization). Service principals can be configured from this tab. Groups: Users can create new groups to segregate different users and provide varying privileges. Example: An organization might have HR and normal employee groups. HR may have elevated privileges (e.g., viewing salary) that normal employees do not. Once a group is created, users or service principals can be assigned to it. Granting privileges to a group means all members (users and service principals) of that group automatically receive those roles or responsibilities. Groups simplify management, making it easier to assign permissions to a group rather than to individual users.</p> </li> <li> <p>Network Connectivity Configurations (NCC):     This tab is used to add configurations for securely connecting to Azure services that cannot be directly connected with Databricks.     NCC helps establish secure connections for such services. </p> </li> <li> <p>Previews:     Databricks offers a look and feel of upcoming features.     Users can enable or disable these preview features from this tab.</p> </li> <li> <p>Settings:     This section contains various settings, but the video focuses on Feature Enablement.     Feature Enablement:     Some features are enabled or disabled by default for the account.     Serverless Compute for Workflow, Notebooks, and DLT (Delta Live Tables) is a new feature that is disabled by default.     If a workspace needs to access serverless compute for its workflows, notebooks, or DLT, it must be enabled from the Account Console.     Once enabled at the account level, all workspaces under that account will have access to this serverless compute. This will be enabled when serverless features are discussed later in the course.     Other features can also be enabled or disabled at the account level for all workspaces</p> </li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#databricks-components","title":"Databricks Components","text":"<p>Databricks is composed of several main components that work together to provide a comprehensive data analytics and AI platform:</p> <ul> <li>Apache Spark: At its core, Databricks is built on Apache Spark, a powerful open-source, distributed computing system that provides fast data processing and analytics capabilities. Databricks enhances Spark with optimized performance and additional features.</li> <li>Databricks Workspace: This is a collaborative environment where data scientists, data engineers, and analysts can work together. It includes interactive notebooks (supporting languages like Python, Scala, SQL, and R), dashboards, and APIs for collaborative development and data exploration.</li> <li>Databricks Runtime: A performance-optimized version of Apache Spark with enhancements for reliability and performance, including optimizations for cloud environments and additional data sources.</li> <li>Delta Lake: An open-source storage layer that brings reliability to Data Lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing.</li> <li>Workflow: Databricks Workflows simplify job orchestration, allowing you to create, schedule, and manage data pipelines using a no-code or low-code interface. They support tasks like ETL, machine learning, and batch or streaming workflows, ensuring seamless integration with Databricks Jobs and other tools.</li> <li>Databricks SQL: A feature for running SQL queries on your data lakes. It provides a simple way for analysts and data scientists to query big data using SQL, visualize results, and share insights.</li> <li>SQL Warehouses: Databricks SQL Warehouse is a scalable, cloud-native data warehouse that supports high-performance SQL queries on your data lake. It enables analytics and BI reporting with integrated tools like Power BI and Tableau, ensuring fast, cost-efficient query execution with fully managed infrastructure.</li> <li>Catalog: Databricks Catalog provides centralized governance, organizing your data and metadata.</li> <li>Data Integration Tools: These allow for easy integration with various data sources, enabling users to import data from different storage systems.</li> <li>Cluster Management: Databricks allows for easy management of Apache Spark clusters, automating their deployment, scaling, and maintenance. Users can quickly start clusters and adjust their size according to the workload requirements.</li> <li>Delta Live Tables: They simplify ETL with declarative pipelines, ensuring data quality with built-in validation and real-time monitoring. DLT leverages Delta Lake for incremental processing, making pipelines efficient and reliable.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#what-is-governance-and-unity-catalog","title":"What is Governance and Unity Catalog?","text":"<p>Governance Defined: In the context of data, governance means ensuring your data is secure, available, and accurate. Unity Catalog is the feature in Databricks that provides this capability.</p> <p>Unified Governance: Unity Catalog offers a \"unified\" governance model, which means you \"define it once and secure it everywhere\". This is a key benefit, making it a popular feature in Databricks.</p> <p>Centralized Management:     Without Unity Catalog, each Databricks workspace must be managed individually, including its users, metastore, and compute resources.     With Unity Catalog, you get centralized governance, allowing you to manage all your workspaces from a single, account-level location. This simplifies administration, as you can manage users and metastores at the account level and then assign them to specific workspaces.</p> <p>Key Features: Unity Catalog is a centralized, open-source solution that provides security, auditing, data lineage, and data discovery capabilities. Because it's open source, the code is publicly available on GitHub.</p> <p>The Unity Catalog Object Model</p> <p></p> <p>Unity Catalog organizes data and non-data assets in a hierarchical object model.</p> <ol> <li> <p>Metastore:     This is the top-level object in the hierarchy. It is responsible for storing metadata.     Metadata is \"data about data,\" which includes information like data schemas, Access Control Lists (ACLs), and user permissions.     The metadata itself is stored in the Databricks control plane, while the actual data is stored in a location you define in your own cloud account's data plane (e.g., an Azure storage account).     Best Practice: It is highly recommended to have only one metastore per region.</p> </li> <li> <p>Securable Objects: Below the metastore, the objects are divided into two main types:</p> <p>Data Securable Objects: These objects, starting with the Catalog, deal directly with your data assets.</p> <p>Non-Data Securable Objects: These objects support your data assets and include things like storage credentials, external locations, shares, and connections.</p> </li> </ol> <p>Data Securable Objects Hierarchy</p> <p>Catalog: This is the primary object for managing your data assets, whether they are structured, semi-structured, or unstructured. All securable data assets are organized under a catalog.</p> <p>Schema: Within a catalog, you can define schemas to further organize your data assets.</p> <p>Tables and Views: Used to maintain your structured data within a schema.</p> <p>Volumes:     Volumes are essentially file systems that can be used to manage and govern structured, unstructured, or semi-structured data.     Any files stored in a volume can have their permissions managed directly through Unity Catalog.</p> <p>Functions and ML Models: You can also create and govern user-defined functions and machine learning models within Unity Catalog, managing their permissions centrally.</p> <p>The Three-Level Namespace</p> <p>A key concept introduced with Unity Catalog is the three-level namespace for accessing data. To access a table or view, you must specify its full path using the format: catalog_name.schema_name.table_name. Example: If you have a table named sales under a schema named bronze in a catalog named dev, you would access it using dev.bronze.sales. This structure ensures data is securely accessed based on its specific catalog and schema location.</p> <p>What are Deletion Vectors? \u2022 In a typical Delta table scenario, when you modify or delete even a single row in a Parquet file, the entire file has to be rewritten. This can be inefficient, especially with files containing millions of records, creating an optimization issue. \u2022 Deletion Vectors offer a solution to this problem. When a table has deletion vectors enabled, instead of rewriting the file, a deleted row is simply marked with a vector or a flag indicating it has been deleted. \u2022 When you read data from the table, the query engine will only process the rows that have not been flagged for deletion. \u2022 The actual removal of these flagged rows occurs during a maintenance operation, such as running the OPTIMIZE command. This process rewrites the Parquet file, permanently removing the rows marked for deletion. Practical Demonstration of Deletion Vectors The video demonstrates this concept using a Databricks notebook. 1. Reading Data and Creating a Table:     \u25e6 A sample sales CSV file is read using the read_files utility in SQL.     \u25e6 A Delta table named dev.bronze.sales is created from this data using a CREATE TABLE AS SELECT (CTAS) command. 2. Checking Table Properties:     \u25e6 Using the DESCRIBE EXTENDED command on the newly created table reveals its metadata.     \u25e6 By default, the table property delta.enableDeletionVectors is set to true, meaning deletion vectors are enabled. 3. Deleting Data with Deletion Vectors Disabled:     \u25e6 To show the traditional behavior, deletion vectors are first disabled using the ALTER TABLE command:     \u25e6 A DELETE statement is executed to remove all records with a specific InvoiceNo.     \u25e6 The DESCRIBE HISTORY command is then used to inspect the transaction log. The history shows:         \u25aa Files were rewritten: two files were removed and one new file was added.         \u25aa The operation metrics confirm that numDeletionVectorsAdded is zero, as the feature was turned off. 4. Deleting Data with Deletion Vectors Enabled:     \u25e6 The feature is re-enabled for the table by setting delta.enableDeletionVectors back to true.     \u25e6 Another DELETE operation is performed for a different InvoiceNo.     \u25e6 Viewing the table history again shows a different outcome:         \u25aa No files were removed or added (numFilesAdded is zero).         \u25aa Instead, the operation metrics show that deletion vectors were added for the four deleted rows.         \u25aa This confirms that the data file was not rewritten; the rows were simply flagged for deletion. 5. Prerequisites:     \u25e6 To use deletion vectors, you need Delta Lake version 2.3.0 or higher.     \u25e6 If using Databricks, the runtime must be 12.2 LTS or higher.</p> <p>Part 2: Liquid Clustering What is Liquid Clustering? \u2022 Traditional optimization techniques like partitioning and Z-ordering require rewriting the entire table to apply the optimization. \u2022 Liquid Clustering is a newer Delta Lake feature that avoids this full table rewrite. When you enable clustering on a column, new or incremental data is automatically and efficiently adjusted according to the clustering key. \u2022 Delta Lake automatically handles the repartitioning and file sizes for the specified clusters. When to Use Liquid Clustering Liquid clustering is recommended in several scenarios: \u2022 Tables with high-cardinality columns that are frequently used in query filters. \u2022 Data with significant skew. \u2022 Tables where data grows very quickly. \u2022 When there are varied or changing data access patterns. \u2022 For tables that are over-partitioned or under-partitioned. Practical Demonstration of Liquid Clustering 1. Prerequisites:     \u25e6 Liquid clustering requires Delta Lake version 3.1.0 or higher.     \u25e6 In Databricks, the runtime version must be 13.3 LTS or higher. 2. Enabling Liquid Clustering on an Existing Table:     \u25e6 You can enable clustering on an existing table using the ALTER TABLE command with the CLUSTER BY clause.     \u25e6 Example: ALTER TABLE dev.bronze.sales CLUSTER BY (InvoiceNo).     \u25e6 The table history will show that the clustering column has been set. The process also upgrades some table protocols automatically. 3. Creating a New Table with Liquid Clustering:     \u25e6 When creating a new table, you can define clustering columns directly in the CREATE TABLE statement.     \u25e6 Example:     \u25e6 DESCRIBE EXTENDED on this new table will list InvoiceNo under \"Clustering Columns\". 4. Important Limitation:     \u25e6 A column chosen for clustering must be within the first 32 columns of the Delta table. 5. Benefits in Practice:     \u25e6 Once enabled, you only need to ensure your queries use the clustering column in filter predicates (WHERE clauses) to see performance benefits.     \u25e6 A major advantage is that no manual maintenance is needed for incremental data loads; new data is automatically clustered, ensuring sustained query performance</p> <p>Introduction to Volumes in Databricks \u2022 Purpose: Volumes are a feature in Databricks Unity Catalog designed to store and govern unstructured, semi-structured, or structured data files. While Delta tables are used for tabular data, volumes handle other types of files. \u2022 Governance: Because volumes exist within the Unity Catalog's catalog.schema hierarchy, they can be governed using Unity Catalog's features, just like tables. \u2022 Prerequisites:     \u25e6 Unity Catalog must be enabled in your Databricks workspace.     \u25e6 You must use a Databricks runtime version of 13.3 LTS or higher.</p> <p>Part 1: Managed Volumes A managed volume is one where Unity Catalog controls both the metadata and the physical storage location of the files. Creating a Managed Volume 1. SQL Command: You can create a managed volume using the CREATE VOLUME command. 2. Naming Convention: You must use a three-level namespace: catalog.schema.volume_name. 3. Location: For a managed volume, you do not specify a storage location. Unity Catalog automatically manages this, storing the data in a default location determined by the metastore, catalog, or schema settings. Example Command: CREATE VOLUME dev.bronze.managed_volume COMMENT 'This is a managed volume'; After running the command, the new volume will appear in the Catalog Explorer under the specified schema, separate from the tables. Inspecting a Managed Volume You can view details about a volume using the DESCRIBE VOLUME command. This command provides information such as: \u2022 Catalog and schema (database) name. \u2022 Owner. \u2022 Physical storage location (which will be under a metastore-managed path). \u2022 Volume type, which will be listed as MANAGED. Working with Files in a Managed Volume You can use dbutils commands within Python or shell magic commands to manage files and directories inside a volume. 1. Downloading a File: The video demonstrates downloading an EMP.csv file from the internet to the local driver file system using the %sh wget command. 2. Creating a Directory: You can create a directory inside the volume using dbutils.fs.mkdirs().     \u25e6 Path Syntax: When accessing volumes in Python or shell commands, you must use the specific path format: /volumes////. 3. Copying Files into the Volume: Use dbutils.fs.cp() to copy files from the local file system into the volume. The source path for a local file should be prefixed with file:/. <p>Part 2: External Volumes An external volume is one where you specify the physical storage location in your cloud storage (like Azure Data Lake Storage), and Unity Catalog only manages the metadata associated with it. Creating an External Volume 1. Prerequisite: External Location: Before creating an external volume, you must have an External Location configured in Unity Catalog. This involves:     \u25e6 Creating a directory in your cloud storage account (e.g., a folder in an Azure container).     \u25e6 Creating an External Location in Databricks that points to this directory and uses a Storage Credential for access. You can test the connection to ensure permissions are correctly set up. 2. SQL Command: Use the CREATE EXTERNAL VOLUME command. 3. Location Clause: You must provide the path to your pre-configured External Location using the LOCATION clause. Example Command: CREATE EXTERNAL VOLUME dev.bronze.external_volume COMMENT 'This is an external volume' LOCATION 'abfss://data@adbewithdata01.dfs.core.windows.net/adb/EXT_volume'; After creation, you can work with files in an external volume the same way you do with managed volumes (creating directories, copying files). The files will physically reside in the specified external cloud storage location.</p> <p>Reading Data from Volumes You can directly query files stored within a volume using SQL. The syntax requires specifying the file format (e.g., csv) followed by the path to the file within the volume. Example Query: SELECT * FROM csv.<code>/volumes/dev/bronze/managed_volume/files/EMP.csv</code> Note: The path in the SQL query uses a format similar to file system paths, with / separating components, not . as used for table names. Dropping Volumes The behavior when dropping a volume differs significantly based on its type. \u2022 Dropping a Managed Volume: When a managed volume is dropped, both the metadata in Unity Catalog and the underlying data files are permanently deleted. \u2022 Dropping an External Volume: When an external volume is dropped, only the metadata in Unity Catalog is removed. The actual data files remain untouched in your external cloud storage location.</p> <p>Introduction to Databricks Compute \u2022 What is Compute? In Databricks, \"compute\" refers to the data processing capability within your workspace. \u2022 What is a Cluster? The term \"compute\" is often used interchangeably with \"cluster\". A cluster is a group of machines combined to process and work on your jobs. In a Spark context, a cluster consists of one driver and one or more worker nodes. Types of Compute The video focuses on two primary types of compute available on the \"Compute\" page in the Databricks workspace sidebar: 1. All-Purpose Compute: Used for interactive tasks like running notebooks or SQL queries manually. 2. Job Compute: Specifically created at runtime for automated jobs and is recommended for this purpose. Creating an All-Purpose Compute Cluster To create an All-Purpose Compute, you navigate to the \"Compute\" page and click the \"Create Compute\" button. This opens a configuration page with several options. 1. Cluster Name &amp; Policy \u2022 Cluster Name: The first step is to name your cluster. \u2022 Policy: Policies are used to define or restrict the way a compute is configured.     \u25e6 Unrestricted: This is the default policy, which allows you to design any type of cluster without limitations.     \u25e6 Predefined Policies: Databricks provides other predefined policies (like Personal Compute, Shared Compute) which are subsets of the \"Unrestricted\" policy. They autofill certain configurations to simplify setup and enforce restrictions, such as limiting available node types. Custom policies can also be created. 2. Node Configuration \u2022 Multi-Node vs. Single-Node:     \u25e6 Multi-Node: A cluster with more than one machine (one driver, multiple workers).     \u25e6 Single-Node: A cluster with only one node, which acts as the driver. Options like autoscaling are not available for single-node clusters. 3. Access Modes This setting determines how users can access the cluster, especially in relation to Unity Catalog. \u2022 Single User:     \u25e6 Only one specific user can use the cluster.     \u25e6 It is Unity Catalog enabled.     \u25e6 The user's Azure Active Directory credentials are automatically passed for secure access to Unity Catalog objects. \u2022 Shared:     \u25e6 Multiple users with access can use the same cluster.     \u25e6 It is Unity Catalog enabled. \u2022 No Isolation Shared:     \u25e6 This is a shared cluster for multiple users but is not supported by Unity Catalog.     \u25e6 It is intended for legacy workloads that run on the Hive metastore. 4. Databricks Runtime (DBR) Version \u2022 The DBR is a pre-packaged image containing specific versions of Spark, Scala, Python, necessary libraries, and bug fixes. \u2022 It is always recommended to use the latest LTS (Long-Term Support) version available. \u2022 You can select different DBRs for standard workloads or Machine Learning (ML) workloads. 5. Photon Acceleration \u2022 Photon is a high-performance runtime engine developed by Databricks in C++ that can significantly speed up jobs like data ingestion, ETL, and interactive queries. \u2022 Enabling Photon adds a \"Photon\" tag to the cluster summary and comes with an additional cost, which is reflected in the DBU (Databricks Unit) estimate. 6. Worker and Driver Configuration \u2022 VM Type: You can select the virtual machine type for your workers and driver based on your needs, such as general purpose, memory-optimized, or Delta Cache accelerated. Details like memory and core count are provided for each VM type. \u2022 Autoscaling: This feature allows the cluster to dynamically scale between a minimum and maximum number of workers.     \u25e6 The cluster starts with the minimum number of workers and scales up as the workload increases, up to the defined maximum.     \u25e6 This is useful when you are unsure of the required resources.     \u25e6 If autoscaling is disabled, the cluster will have a fixed number of workers, resulting in a fixed DBU cost per hour. \u2022 Driver Type: You can select the same VM type for the driver as the workers, or choose a different one. 7. Cost-Saving Options \u2022 Terminate After: This is a critical cost-saving feature for all-purpose computes.     \u25e6 You can specify a period of inactivity (e.g., 30 minutes) after which the cluster will automatically terminate. 8. Advanced Options \u2022 Tags: You can add key-value tags (e.g., owner: ease with data) for cost tracking and organization. \u2022 Spark Configs &amp; Environment Variables: These can be passed directly to the cluster during setup. \u2022 Logging: Cluster logs can be configured to be saved to a specific DBFS (Databricks File System) path. Managing and Monitoring a Cluster Once a cluster is created, several tabs become available for management and monitoring: \u2022 Libraries: You can install libraries (e.g., JAR or Python wheel files) on the cluster. \u2022 Event Log: This log shows cluster events, such as the addition or removal of worker nodes when autoscaling is enabled. \u2022 Spark UI: Provides access to the traditional Spark UI for debugging jobs and viewing execution details. \u2022 Driver Logs: Shows standard out, standard error, and Log4j logs for the cluster's driver node. \u2022 Metrics: Displays real-time and historical metrics on CPU utilization, memory usage, and other hardware information to help debug performance. \u2022 Notebooks: Shows which notebooks are currently attached to the cluster. Cluster Permissions You can manage who can access the cluster and what they can do. There are three permission levels: \u2022 Can Manage: Allows a user to edit, delete, start, and stop the cluster. \u2022 Can Restart: Allows a user to only start or stop the cluster, but not edit or delete it. \u2022 Can Attach To: The most restrictive level; allows a user only to attach notebooks and run jobs using the cluster. Job Compute vs. All-Purpose Compute \u2022 Job Computes are designed specifically for running automated jobs via Databricks Workflows. \u2022 They are created on-demand when a job is triggered and automatically shut down once the job completes or fails. \u2022 This is the recommended approach for running production jobs as it is more cost-effective. \u2022 In contrast, All-Purpose Computes are for interactive analysis and development work</p> <p>Introduction to Databricks Auto Loader \u2022 What is Auto Loader? Auto Loader is a Databricks utility designed to incrementally and efficiently process new files arriving in cloud storage. It can ingest files from various cloud storage providers like AWS, Azure, and GCP, as well as from Databricks File System (DBFS) locations. For the examples in the video, volumes are used as the storage location. \u2022 How it Works: Auto Loader provides a structured streaming source called cloudFiles. This makes its implementation very similar to standard structured streaming, using spark.readStream. \u2022 Key Benefits:     \u25e6 It supports ingesting data in both streaming and batch modes; you just need to set the trigger accordingly.     \u25e6 It guarantees exactly-once processing of incremental files, preventing data duplication.     \u25e6 It is highly scalable, capable of ingesting millions of files per hour.     \u25e6 It provides robust schema evolution capabilities to handle changes in the structure of incoming files. 2. Initial Setup Before using Auto Loader, the video walks through setting up the necessary directory structures. \u2022 Input Data Location: A complex, nested folder structure is created to demonstrate Auto Loader's ability to read from such directories: .../Landing/autoloader_input/year/month/date/. The files for ingestion are placed into the respective date folders. Wildcards (**) are later used in the path to read files from this nested structure. \u2022 Checkpoint Location: Auto Loader relies on a checkpoint directory to manage state and track which files have been processed, ensuring incremental and exactly-once ingestion. A separate folder is created for this purpose. 3. File Detection Modes Auto Loader offers two modes to detect new files arriving in the source directory. \u2022 Directory Listing (Default Mode):     \u25e6 This is the default mode used by Auto Loader.     \u25e6 It uses API calls to the cloud storage to list files in the source directory and identify new ones.     \u25e6 It manages state within the checkpoint location using a scalable key-value store called RocksDB to ensure exactly-once processing. \u2022 File Notification:     \u25e6 This mode uses cloud-native notification and queuing services (e.g., AWS SNS/SQS, Azure Event Grid/Queue Storage).     \u25e6 When a new file arrives, a notification is sent to a queue, which Auto Loader then consumes to discover the new file.     \u25e6 This mode requires elevated cloud privileges because Databricks needs to automatically set up these services in your cloud account.     \u25e6 To enable this mode, you must set the option cloudFiles.useNotifications to true. 4. Implementing Auto Loader The implementation is very similar to a standard Spark Structured Streaming job. \u2022 Reading Data:     \u25e6 Use spark.readStream.     \u25e6 Set the format to cloudFiles.     \u25e6 Specify the actual file format (e.g., CSV, JSON) using the option cloudFiles.format.     \u25e6 Provide the path to the input data using .load(). \u2022 Key Configuration Options:     \u25e6 pathGlobFilter: Used to specify the file extension, for example, *.csv.     \u25e6 cloudFiles.schemaLocation: This is a crucial option. Auto Loader uses this location to store and evolve the data schema over time.     \u25e6 cloudFiles.schemaHints: A useful option that allows you to specify the data types for certain columns (e.g., quantity Integer, unitPrice Double) without having to define the entire schema. Auto Loader will infer the rest. \u2022 Writing Data:     \u25e6 Use the .writeStream method on the DataFrame.     \u25e6 checkpointLocation: Specify the path for the write stream's checkpoint.     \u25e6 trigger(availableNow=True): This trigger processes all available new files in a single batch and then stops the stream, effectively running the stream in batch mode. You can also use other triggers like processingTime for continuous streaming.     \u25e6 toTable(): Saves the output to a Delta table.     \u25e6 option(\"mergeSchema\", \"true\"): This option is necessary to allow the schema of the target Delta table to evolve if new columns are added by Auto Loader. 5. Schema Evolution Modes Auto Loader provides four modes to handle situations where the schema of incoming files changes. This is configured using the cloudFiles.schemaEvolutionMode option. 1. addNewColumns (Default Mode)     \u25e6 If no mode is specified, this is the default.     \u25e6 When a file with a new column arrives, the stream initially fails.     \u25e6 Upon failure, Auto Loader updates the schema stored in the schemaLocation to include the new column(s).     \u25e6 When you rerun the stream, it will succeed and process the file, adding the new column to the target table (assuming mergeSchema is enabled). 2. rescue     \u25e6 When a file arrives with new columns or mismatched data types, the stream does not fail.     \u25e6 Instead, the \"bad\" or new data is captured and placed into a JSON blob in a special column named _rescued_data.     \u25e6 This allows the stream to continue uninterrupted while preserving the problematic data for later analysis. 3. failOnNewColumns     \u25e6 As the name suggests, if a new column is detected in a source file, the stream will immediately fail.     \u25e6 You must then manually intervene, either by fixing the source file or updating the schema. 4. none     \u25e6 In this mode, any changes to the schema are completely ignored.     \u25e6 The stream will not fail, but new columns will not be added, and their data will be dropped. Nothing is sent to a rescued data column unless one is manually configured</p> <p>Introduction to the Medallion Architecture \u2022 The Medallion architecture is a multi-hop data architecture designed to logically organize data in a data lakehouse. \u2022 It consists of three primary layers: Bronze, Silver, and Gold. \u2022 The fundamental principle is that data moves progressively from left to right (from Bronze to Silver, then to Gold), becoming more validated, enriched, and refined with each hop. \u2022 This structure improves the overall quality and reliability of the data as it flows through the system. 2. The Bronze Layer (Raw Data) \u2022 Purpose: The Bronze layer is the first stop for data entering the lakehouse. Its main function is to capture raw data from various source systems. \u2022 Data Sources: It can ingest data from a wide variety of sources, including streaming systems, batch systems, and different file types like CSV, JSON, text, and Parquet. \u2022 Data State: The data in the Bronze layer is typically stored \"as is,\" mirroring the source system's structure and content. This raw format is crucial because it ensures the original data is always available for downstream processes to re-consume if needed. \u2022 Key Characteristics:     \u25e6 Stores Raw Data: It holds the unprocessed, original data.     \u25e6 Maintains History: The Bronze layer also stores the historical record of the data.     \u25e6 Audit Columns: It is common practice to add audit columns, such as ingestion timestamps, to track when the data was loaded.     \u25e6 Delta Lake Optimization: Storing this data in the Delta Lake format optimizes it for better read performance compared to traditional raw file storage. 3. The Silver Layer (Cleansed &amp; Conformed Data) \u2022 Purpose: The Silver layer takes the raw data from the Bronze layer and transforms it into a more reliable and queryable state. \u2022 Transformations: This is where data quality improvements are applied. Common operations include:     \u25e6 Filtering data to remove irrelevant records.     \u25e6 Deduplicating records.     \u25e6 Cleaning and refining the data to correct errors or inconsistencies. \u2022 Data Enrichment: Data is often enriched in this layer. This can involve joining multiple Bronze tables to create new, more comprehensive tables in the Silver layer. \u2022 Key Characteristics: The data in the Silver layer is cleansed, conformed, and ready for more complex business logic and analytics. 4. The Gold Layer (Curated Business-Level Data) \u2022 Purpose: The Gold layer is the final and most refined layer, specifically designed to serve business needs and analytics. \u2022 Data State: Data in the Gold layer is highly refined and aggregated according to specific business requirements. It represents the \"single source of truth\" for key business metrics. \u2022 Use Cases: This layer serves as the primary endpoint for various consumers:     \u25e6 Analytical Reports and Dashboards.     \u25e6 Artificial Intelligence (AI) and Machine Learning (ML) models.     \u25e6 Data Sharing initiatives. \u2022 Key Characteristics: The tables in the Gold layer are often organized in a way that is optimized for reporting and analysis, providing business-level insights. 5. Cross-Cutting Concerns \u2022 Data Quality and Governance: It is essential to apply data quality checks and data governance policies throughout all three layers (Bronze, Silver, and Gold) to ensure the integrity and security of the data at every stage of the architecture.</p> <ol> <li>Introduction to Delta Live Tables (DLT) \u2022 What is DLT? Delta Live Tables (DLT) is a declarative framework from Databricks designed to simplify the creation of ETL and data processing pipelines. \u2022 Declarative Framework: This means developers only need to focus on writing the data transformations, while the DLT pipeline automatically manages background tasks like orchestration, cluster management, data quality, and error handling. \u2022 Prerequisites:     \u25e6 Working with DLT requires a Premium plan in Databricks. If you are on a Standard plan, you will need to upgrade.     \u25e6 DLT pipelines are powered by Delta Lake, inheriting all of its capabilities.     \u25e6 The code for DLT can be written in either Python or SQL.</li> <li>DLT Dataset Types DLT pipelines use three main types of datasets:</li> <li>Streaming Table:     \u25e6 Designed to process incremental data and supports streaming.     \u25e6 It allows data to be appended to a table.     \u25e6 In DLT, a streaming table is typically created from a streaming source (e.g., spark.readStream).</li> <li>Materialized View (MV):     \u25e6 Generally used for transformations, aggregations, or computations.     \u25e6 In DLT, a materialized view is typically created from a batch source (e.g., spark.read).</li> <li>View:     \u25e6 Used for intermediate transformations that should not be stored permanently in the target schema.     \u25e6 Views in DLT are temporary and are not recorded at the target destination.</li> <li>Initial Setup for the Demonstration Before building the DLT pipeline, the video explains the setup process: \u2022 A new schema named ETL is created under the Dev catalog to store the pipeline's output. \u2022 Source data is prepared by performing a deep clone of the orders and customer tables from the sample tpch schema provided by Databricks. \u2022 The cloned tables, orders_raw and customer_raw, are stored in a bronze schema and will serve as the input for the DLT pipeline.</li> <li>Implementing a DLT Pipeline in Python The implementation uses Python decorators to define the different datasets within the DLT pipeline. The first step is to import the dlt module. \u2022 Creating a Streaming Table (orders_bronze)     \u25e6 The @dlt.table decorator is used.     \u25e6 The function name (orders_bronze) becomes the table name by default.     \u25e6 The function must return a Spark DataFrame.     \u25e6 The data is read using spark.readStream.table() from the orders_raw clone, making the resulting table a streaming table.     \u25e6 You can add comments and table properties (like quality: 'bronze') within the decorator. \u2022 Creating a Materialized View (customer_bronze)     \u25e6 The same @dlt.table decorator is used.     \u25e6 The source data is read using spark.read.table() (a batch read), which defines the output as a materialized view.     \u25e6 The table name can be explicitly set using the name parameter within the decorator (e.g., name='customer_bronze'). \u2022 Creating an Intermediate View (join_view)     \u25e6 The @dlt.view decorator is used to define a temporary view.     \u25e6 To read from other tables created within the same DLT pipeline, you use the dlt.read() function, which is implicitly available via the live keyword (e.g., live.customer_bronze).     \u25e6 This view joins the orders_bronze streaming table and the customer_bronze materialized view. \u2022 Creating a Silver Materialized View (joined_silver)     \u25e6 Another @dlt.table is created to represent the silver layer.     \u25e6 It reads from the intermediate join_view using live.join_view.     \u25e6 A new column, insert_date, is added using the current timestamp. \u2022 Creating a Gold Materialized View (orders_aggregated_gold)     \u25e6 The final @dlt.table represents the gold layer.     \u25e6 It reads from the silver table (live.joined_silver).     \u25e6 It performs a groupBy on the marketsegment column and aggregates the count of orders to create a summary table.</li> <li>Creating and Configuring the DLT Pipeline \u2022 Creation: You can create a DLT pipeline from the Databricks workspace sidebar by selecting New &gt; DLT Pipeline (sometimes referred to as ETL Pipeline). \u2022 Configuration Options:     \u25e6 Pipeline Name: A user-defined name for the pipeline.     \u25e6 Product Edition:         \u25aa Core: Basic functionality (streaming tables, MVs, aggregations) but no CDC or data quality checks.         \u25aa Pro: Adds support for Change Data Capture (CDC).         \u25aa Advanced: Includes all DLT features, including data quality checks.     \u25e6 Pipeline Mode:         \u25aa Triggered: The pipeline runs once as per a schedule and then stops.         \u25aa Continuous: The pipeline runs continuously to process streaming data without stopping.     \u25e6 Source Code: Path to the notebook containing the DLT code.     \u25e6 Destination: You must specify the target catalog and schema for the output tables. DLT can automatically create the schema if it doesn't exist.     \u25e6 Compute: You must configure the job compute for the pipeline. DLT cannot be run on an all-purpose compute.</li> <li>Running and Debugging the Pipeline \u2022 Development vs. Production Mode:     \u25e6 Development Mode (Default): If the pipeline succeeds or fails, the underlying compute cluster remains running. This is useful for debugging because you can fix your code and restart the pipeline quickly without waiting for a new cluster to spin up.     \u25e6 Production Mode: The cluster is terminated immediately after the pipeline succeeds or fails. \u2022 Pipeline Execution Graph: Once the pipeline starts, it renders a visual Directed Acyclic Graph (DAG) showing the data flow from source tables through views to the final aggregated tables. You can see record counts for each step as it completes. \u2022 Debugging:     \u25e6 If the pipeline fails, the event log at the bottom of the screen provides detailed error messages.     \u25e6 In development mode, you can fix the code in your notebook and simply click \"Start\" again to rerun the pipeline on the existing cluster, making the debugging process very efficient.</li> <li>Final Output \u2022 After the pipeline completes successfully, you can query the tables created in the target ETL schema. \u2022 The output will only contain the streaming tables and materialized views. The intermediate view (join_view) is not stored, as it's only used for temporary transformations during the pipeline run</li> </ol> <p>Introduction to Incremental Loads and DLT Internals The video builds upon a previous session where a basic Delta Live Tables (DLT) pipeline was created. This pipeline read from customer and orders datasets, joined them, and aggregated the data based on market segment. The main focus of this session is to: \u2022 Process incremental data using the existing pipeline. \u2022 Understand the declarative framework of DLT and how it simplifies data management. \u2022 Explore some internal workings of DLT pipelines. DLT Pipeline Management \u2022 Declarative Framework: DLT is a declarative framework, meaning you declare the desired state of your data, and DLT manages the underlying datasets (like materialized views or streaming tables) for you. \u2022 Pipeline-Managed Datasets: All datasets created within a DLT pipeline are managed by that specific pipeline, which is identified by a unique Pipeline ID. This ID is stored in the properties of each dataset, tying them to the pipeline's lifecycle. \u2022 Lifecycle Management: If you delete a DLT pipeline, all associated datasets it created are also automatically deleted. This simplifies cleanup and resource management. \u2022 Pipeline Updates: Each time a DLT pipeline is triggered, a new \"update\" is created. You can find details about the run, including the DLT cluster compute information, Spark UI, logs, and metrics under the \"Update Details\" section. All events that occur during a run are logged and can be inspected for more information. Processing Incremental Data The video demonstrates how to process new data incrementally using a streaming table. 1. Inserting New Data: 10,000 new records were inserted into the orders_raw table, which serves as the source for the orders_bronze streaming table. 2. Running the Pipeline: The DLT pipeline was started again to process the new data. 3. Incremental Read: During the run, the orders_bronze streaming table only read the new 10,000 records that were just added. It did not re-process the existing data. 4. Verification: After the pipeline completed, a query on the final aggregated gold table confirmed that the counts had increased, proving that the new 10,000 records were successfully processed incrementally. This demonstrates a key feature of streaming tables: they are designed to efficiently process incremental data. Modifying a DLT Pipeline The video explains how to add or modify columns and rename tables in a running DLT pipeline using development mode. Connecting to a Running Pipeline You can connect your notebook directly to a DLT pipeline running in development mode. This allows you to validate and debug your code directly from the notebook without stopping the pipeline. Modifying and Adding Columns The demonstration involved two changes to the aggregated gold table: 1. Renaming a Column: An existing column, mistakenly named sum, was renamed to count_orders to accurately reflect its aggregation (a count). 2. Adding a New Column: A new aggregated column, sum_of_total_price, was added to calculate the sum of order_total_price. After making these code changes, the Validate button was used to check the code for errors. An initial error was found because the sum function had not been imported. This was fixed directly in the notebook, showcasing the debugging capabilities. After a successful validation, the pipeline was started again. \u2022 Result: The pipeline run completed successfully. The streaming table did not read any new records because no new data had been added to the source. A query on the gold table confirmed that the column was renamed and the new aggregated column was added successfully. This illustrates how easy it is to modify table schemas in a declarative framework like DLT. Renaming a Table You can also rename tables managed by a DLT pipeline. 1. Code Change: The materialized view named joint_silver was renamed to order_silver in the code. The reference to this table in the downstream gold table was also updated. 2. Pipeline Execution: After validating the change, the pipeline was run again. 3. Automatic Management: DLT automatically handled the change. The new order_silver table was created, and because joint_silver was no longer defined in the code, DLT automatically removed it. This demonstrates that DLT manages the creation and removal of datasets based on the pipeline's code definition. DLT Internals and Data Storage \u2022 Internal Catalog: When a DLT pipeline runs, it creates an internal, hidden catalog named databrick_internal. The materialized views and streaming tables you define are abstractions built on top of underlying tables within this internal schema. \u2022 Table Location: The actual data for DLT tables is stored in a location specified by a Table ID found in the table's details. The video demonstrates finding the data for the orders_bronze streaming table in an Azure storage location by looking up this ID. \u2022 Checkpointing for Incremental Loads: The incremental behavior of streaming tables is managed through checkpointing. Within the storage location for a streaming table, a _dlt_metadata/checkpoint folder is created. This checkpoint tracks the changes in the source data, ensuring that only new or modified data is processed during each run. Data Lineage with Unity Catalog Data lineage, which tracks the flow of data, is a powerful feature powered by Unity Catalog. \u2022 Visualizing Lineage: Unity Catalog provides a visual lineage graph that shows the relationships between tables. For example, it showed that orders_aggregated_gold was loaded from order_silver, which in turn was loaded from customer_bronze and orders_bronze. You can explore the entire flow from the raw source tables to the final aggregated tables. \u2022 Column-Level Lineage: Lineage can be tracked down to the individual column level. You can select a column (e.g., count_orders) and see exactly which upstream columns were used to calculate it, tracing it all the way back to the source tables. \u2022 General Unity Catalog Feature: Lineage is not exclusive to DLT; it is a feature of Unity Catalog that works for any table, allowing you to track data and metadata flow across your entire workspace. It can also show which notebooks use a particular dataset.</p> <p>ntegrating Autoloader into a Delta Live Tables (DLT) Pipeline The primary goal is to integrate Autoloader into an existing DLT pipeline to ingest file-based data alongside data coming from a Delta table. 1. Setup for Autoloader Before writing the DLT code, some initial setup is required to handle the incoming files and schemas for Autoloader. \u2022 Create a Managed Volume: A managed volume named Landing is created under the Dev catalog and ETL schema. \u2022 Create Folders: Within this volume, two folders are created:     \u25e6 files: This is the landing zone where the source files for ingestion will be placed.     \u25e6 autoloader/schemas: This location is required by Autoloader to store and manage the schema information of the ingested files. 2. Reading Data with Autoloader in DLT The next step involves writing the Python code within the DLT notebook to read data using Autoloader. \u2022 Create a Streaming Table: Since Autoloader's cloudFiles format is a streaming source, the data must be read into a streaming table. \u2022 Code Implementation:     \u25e6 A new function is defined with a @dlt.table decorator to create a streaming table named orders_autoloader_bronze.     \u25e6 The core logic uses spark.readStream.format(\"cloudFiles\") to configure Autoloader. \u2022 Autoloader Configuration Options:     \u25e6 cloudFiles.schemaHints: A schema string is provided to ensure the data from the files matches the schema of the existing orders_bronze Delta table. This is crucial for the subsequent union operation.     \u25e6 cloudFiles.schemaLschema locationocation: This is set to the autoloader/schemas path created during the setup phase.     \u25e6 cloudFiles.format: Specified as csv for the incoming files.     \u25e6 pathGlobFilter: Set to *.csv to ensure only CSV files are processed.     \u25e6 cloudFiles.schemaEvolutionMode: This is explicitly set to None to prevent any schema changes. If a file with a different schema arrives, it will be discarded.     \u25e6 .load(): This function points to the files directory where the source files will land. \u2022 Checkpoint Location Management: The video notes that you do not need to specify a checkpoint location for Autoloader when using it within DLT. DLT automatically manages the checkpointing at the location of the streaming table itself. Unioning Streaming Tables with Append Flow After setting up two separate streaming sources (one from a Delta table and one from Autoloader), the data needs to be combined. \u2022 The Challenge with a Standard Union: A standard union operation on two streaming tables would cause the entire dataset from both sources to be re-read and processed during each pipeline run, which is inefficient and defeats the purpose of incremental processing. \u2022 The Solution: dlt.append_flow():     \u25e6 To perform an incremental union, DLT provides a specific decorator called dlt.append_flow.     \u25e6 This feature allows data from multiple streaming sources to be incrementally appended into a single target streaming table. \u2022 Implementation Steps:     1. Create a Target Table: First, an empty streaming table is created to serve as the destination for the union. This is done using dlt.create_streaming_table(\"orders_union_bronze\").     2. Append the First Source: A function is defined using the @dlt.append_flow(target=\"orders_union_bronze\") decorator. This function reads incrementally from the first streaming table (live.orders_bronze) and appends the new data to the target union table.     3. Append the Second Source: The process is repeated for the second source. Another function with the same @dlt.append_flow decorator reads from the Autoloader streaming table (live.orders_autoloader_bronze) and appends its data to the same target union table. \u2022 Benefit: This approach ensures that only new, incremental data from each source stream is processed and added to the union table in each pipeline run, making the pipeline highly efficient. Generating Dynamic Tables with Parameters The video also demonstrates how to make the DLT pipeline more flexible by using parameters to dynamically create tables. \u2022 The Use Case: The client wants to create separate gold tables for each order status (e.g., one for 'O' and one for 'F') based on a parameter passed to the pipeline. \u2022 Implementation Steps:     1. Add a Parameter in Pipeline Settings: A custom configuration is added in the DLT pipeline's UI settings. The key is set to custom.order_status and the value is a comma-separated string, O,F.     2. Read the Parameter in Code: Inside the DLT notebook, the parameter is read from the Spark configuration using spark.conf.get(\"custom.order_status\", \"na\").     3. Create Tables in a Loop:         \u25aa The comma-separated string of statuses is split into a list.         \u25aa A for loop iterates through each status ('O' and 'F').         \u25aa Inside the loop, the code for creating a materialized view (dlt.table) is placed.         \u25aa The table name and the filtering logic are made dynamic using f-strings. This results in tables like orders_agg_O_gold and orders_agg_F_gold, each containing data filtered for its respective status. \u2022 Outcome: When the pipeline runs, it dynamically generates a separate materialized view for each status provided in the configuration, demonstrating how to build powerful and adaptable pipelines.</p> <p>Introduction to Change Data Capture (CDC) in Delta Live Tables (DLT) The video focuses on a key feature of Delta Live Tables (DLT) called Change Data Capture (CDC). It demonstrates how to create Slowly Changing Dimension (SCD) Type 1 and Type 2 tables using the APPLY CHANGES API. Key topics covered include: \u2022 Designing SCD Type 1 and Type 2 tables. \u2022 Inserting new records and handling updates. \u2022 Deleting records based on a condition. \u2022 Truncating an entire table based on a condition. \u2022 Backloading out-of-order data into an SCD Type 2 table, a process that is typically difficult but made much easier with DLT. The goal is to convert an existing customer_bronze materialized view from a previous pipeline into two new tables: one for SCD Type 1 and another for SCD Type 2. 1. Prerequisites: Modifying the Source Table Before implementing the CDC logic in the DLT pipeline, the source table (customer_raw) needs to be modified to support the CDC process. \u2022 Adding New Columns: Two columns are added to the customer_raw table:     \u25e6 Source_action: This column indicates the action taken at the source (e.g., insert, delete, or truncate).     \u25e6 Source_insert_date: This column is a timestamp representing when the action occurred. \u2022 Updating Existing Data: For the records that already exist in the table, these new columns are populated with default values:     \u25e6 Source_action is set to 'I' (for insert).     \u25e6 Source_insert_date is set to the current timestamp minus three days. These columns are crucial for the APPLY CHANGES API to determine how to process each incoming record. 2. Setting up the DLT Pipeline for CDC The DLT notebook is updated to read the modified source data and prepare it for the SCD tables. \u2022 Create a Streaming View: A streaming view named customer_bronze_view is created to read data from the customer_raw table using spark.readStream. Using a streaming table or a streaming view as the source is a requirement for the APPLY CHANGES API. This view will serve as the input for both the SCD Type 1 and SCD Type 2 tables. 3. Implementing SCD Type 1 Table An SCD Type 1 table is created to handle upserts (inserting new records and updating existing ones). \u2022 Create a Streaming Table: A target streaming table is defined using dlt.create_streaming_table(\"customer_scd1_bronze\"). \u2022 Use dlt.apply_changes: The core logic is defined using the dlt.apply_changes API.     \u25e6 target: Specifies the destination table, customer_scd1_bronze.     \u25e6 source: Specifies the input streaming view, customer_bronze_view.     \u25e6 keys: Defines the primary key(s) to identify records for updates. In this case, it's c_custkey.     \u25e6 stored as SCD TYPE 1: This explicitly defines the table type. However, SCD Type 1 is the default behavior for apply_changes, so this line is optional.     \u25e6 apply as deletes: Defines the condition for deleting records. The expression expr(\"SRC_action = 'D'\") tells DLT to delete a record when the Source_action is 'D'.     \u25e6 apply as truncate: Defines the condition for truncating the table. The expression expr(\"Source_action = 'T'\") will cause the entire table to be truncated.     \u25e6 sequence by: This is the most important parameter. It specifies the column that determines the correct order of events, which is SRC_insert_date. This ensures that older changes do not overwrite newer ones. 4. Implementing SCD Type 2 Table An SCD Type 2 table is designed to track and maintain a full history of changes. \u2022 Implementation: The code is very similar to the SCD Type 1 table, with a few key differences:     \u25e6 The target table is named customer_scd2_bronze.     \u25e6 stored as SCD TYPE 2: This is a mandatory parameter to enable history tracking.     \u25e6 No Deletes or Truncates: The apply as deletes and apply as truncate clauses are removed because the purpose of SCD Type 2 is to preserve history, not delete it.     \u25e6 except_column_list: This parameter is used to exclude columns from being considered when detecting changes. The SRC_action and SRC_insert_date columns are excluded because they are metadata for the CDC process and not actual data attributes that need to be tracked for historical changes. \u2022 Connecting to Downstream Tables: The downstream logic is updated to read from the new customer_scd2_bronze table. A filter where __end_at is null is added to ensure that only the current, active records are used for joins and further processing. The __end_at column is automatically managed by DLT in SCD Type 2 tables to track the historical validity of each record. 5. Demonstrating CDC in Action After an initial full load, the video demonstrates several scenarios: \u2022 Handling Updates (SCD Type 1 vs. Type 2):     \u25e6 A new record for an existing customer key (c_custkey = 6) is inserted into the source with an 'I' action.     \u25e6 SCD Type 1 Result: The existing record is updated in place. The data quality metrics show one upserted record.     \u25e6 SCD Type 2 Result: The old record is expired by populating its __end_at timestamp, and a new record is inserted, becoming the new active record (where __end_at is null). The metrics show two upserted records (one for the update, one for the new insert). \u2022 Handling Backloaded Data:     \u25e6 A record is inserted with a timestamp that falls between two existing historical records for the same key.     \u25e6 SCD Type 1 Result: No change occurs. Since SCD Type 1 only cares about the latest state, the backloaded data is older than the current record and is therefore ignored.     \u25e6 SCD Type 2 Result: DLT effortlessly handles the backfill. It correctly adjusts the __start_at and __end_at timestamps of the surrounding records to insert the backloaded record in its correct historical sequence. The metrics show three upserted records as DLT reorganizes the history. This demonstrates a powerful feature of DLT's CDC implementation. \u2022 Handling Deletes:     \u25e6 A record is sent to the source with the Source_action set to 'D'.     \u25e6 SCD Type 1 Result: The corresponding record is permanently deleted from the table, as defined by the apply as deletes rule.     \u25e6 SCD Type 2 Result: No change occurs. The history remains intact because no delete rule was defined for this table. \u2022 Handling Truncates:     \u25e6 A record is sent with Source_action as 'T'. For a truncate operation, only the action column matters; the other data can be null.     \u25e6 SCD Type 1 Result: The entire table is truncated, deleting all records.     \u25e6 SCD Type 2 Result: The table remains untouched, as no truncate rule was defined.</p> <ol> <li>Introduction to Data Quality in Delta Live Tables (DLT) Data quality in DLT is managed through an optional clause called expectations. These are rules applied to DLT datasets to perform a data quality check on every record that passes through a query. There are three main actions you can perform using expectations: \u2022 Warning (Default): This is the default behavior. If a record fails a data quality rule, a warning is logged, but the record is still processed and loaded into the target table. The pipeline does not stop. \u2022 Drop: If a record fails the data quality rule, it is dropped and not loaded into the target table. The pipeline continues to run with the valid records. \u2022 Fail: If any record fails the data quality rule, the entire pipeline stops and fails at that step. To define an expectation, you need three components: a unique rule name, a valid rule (the condition to check), and the action to take (warn, drop, or fail).</li> <li>Setting Up Data Quality Rules (Expectations) The first step is to define the data quality rules, which can be done using a Python dictionary. Rule Definition Steps:</li> <li>Create a Python dictionary to hold your rules.</li> <li>Inside the dictionary, each rule needs two parts:     \u25e6 A unique name for the rule (e.g., valid_order_status).     \u25e6 The rule itself, which is a SQL-like condition (e.g., o_order_status in ('O', 'F', 'P')). Example Rules: \u2022 For an orders dataset:     \u25e6 valid_order_status: The order status must be 'O', 'F', or 'P'.     \u25e6 valid_order_price: The order total price must be greater than zero (o_total_price &gt; 0). \u2022 For a customer dataset:     \u25e6 valid_market_segment: The customer market segment must not be null (c_mkt_segment is not null).</li> <li>Applying Expectations in a DLT Pipeline Expectations are applied using a DLT decorator in your pipeline code. \u2022 For a single rule, you can use @dlt.expect(\"rule_name\", \"rule_condition\"). \u2022 For multiple rules defined in a dictionary, it's more efficient to use @dlt.expect_all() and pass the dictionary name. Applying Actions: \u2022 Warning (Default): Simply apply the decorator. \u2022 This will log any rule violations but allow the records to pass through. \u2022 Drop: Append _or_drop to the decorator. Records that fail the rules will be dropped. \u2022 Fail: Append _or_fail to the decorator. The pipeline will fail if any record violates the rules. Important Note: Expectations can be applied to both tables and views (including temporary views) in your DLT pipeline. You can also apply multiple expectations with different actions (e.g., one to warn, another to drop) on the same dataset, such as a joined view.</li> <li>Pipeline Configuration and Execution \u2022 Product Edition: To use the data quality and expectations features, the DLT pipeline's \"Product Edition\" must be set to Advanced in the pipeline settings. \u2022 Testing: To test the rules, you can insert records that intentionally violate them. For example, insert an order with a negative price or a customer with a null market segment. Execution Results: \u2022 With Warning: The pipeline completes successfully. The UI's \"Data Quality\" tab for the dataset shows how many records failed each rule. The action is listed as \"Allow,\" and the invalid data will be present in the target table. \u2022 With Fail: The pipeline fails at the step where the invalid data is processed. The event log will show the failure was due to an expectation check and may provide details on the failing record. \u2022 With Drop: The pipeline completes successfully. The \"Data Quality\" tab shows that records were dropped. The invalid data will not be present in the target table.</li> <li>Monitoring Data Quality with SQL You can directly query the DLT pipeline's event logs to monitor data quality metrics using SQL. This allows for the creation of custom monitoring dashboards. Steps to Monitor via SQL:</li> <li>Find the Pipeline ID: Copy the Pipeline ID from the DLT pipeline's main view.</li> <li>Query the Event Log: Use the event_log function in a SQL query to access the raw logs.</li> <li>Use Pre-built Queries: Databricks provides documentation with helpful queries to parse the event log and create views for easier monitoring. These queries can help you create views like event_log_raw and latest_updates.</li> <li>Query Data Quality Metrics: You can then run specific queries against these views to see a summary of data quality, including the number of passing and failing records for each rule and the type of expectation applied. This SQL-based monitoring provides a powerful way to track the data quality of your DLT pipelines over time and can be integrated into dashboards for better observability</li> </ol>"},{"location":"cloud/overview/","title":"Overview","text":"Service Azure AWS GCP CI/CD Azure DevOps, GitHub Enterprise AWS Code Build, AWS Code Deploy, AWS Code Pipeline Google Cloud Build, Google Cloud Deploy Data warehouse Azure Synapse Analytics Amazon Redshift BigQuery Data Integration Azure Data Factory AWS Glue, Amazon Data Pipeline Google Cloud Data Fusion Messaging Azure Service Bus, Azure Event Hubs AWS Kinesis, Amazon SNS, Amazon SQS Google Pub/Sub Workflow orchestration Azure Data Factory Amazon Data Pipeline, AWS Glue, Apache Airflow Cloud Composer Document data Azure Cosmos DB Amazon DocumentDB Firestore NoSQL - Key/Value Azure Cosmos DB Amazon DynamoDB Cloud Bigtable RDBMS Azure SQL Database Amazon Aurora, Amazon RDS Cloud SQL Storage Transfer Azure Data Factory, Azure Storage Mover AWS Storage Gateway, AWS Data Sync Storage Transfer Service Network connectivity Azure Virtual Private Network AWS Virtual Private Network Cloud VPN Audit logging Azure Audit Logs AWS CloudTrail Cloud Audit Logs Key management Azure Key Vault AWS KMS Cloud KMS Identity Azure Identity Management AWS IAM Google Cloud IAM Storage Service Azure Blob Storage Amazon S3 Google Cloud Storage"},{"location":"delta/deltalake/","title":"Delta Lake","text":""},{"location":"delta/deltalake/#data-lake-problems-and-the-delta-lake-solution","title":"Data Lake Problems and the Delta Lake Solution","text":"<p>Data lakes offered flexible data storage, allowing ingestion of structured, semi-structured, and unstructured data like audio, video, images, or logs, and storing this high volume data cheaply and scalably on services like S3 or ADLS.</p> <p>However, data lakes presented severe challenges that Delta Lake was designed to solve, including a lack of ACID support, lack of support for update, merge, and delete operations, and issues related to data reliability and quality due to the absence of schema enforcement.</p> <p>Traditional updates or deletes required reading all the data into memory, applying the changes, and writing it back; if the system failed during this process, it resulted in inconsistent or corrupt data. Delta Lake addresses these issues and provides additional features such as time travel, unified batch and streaming capabilities, schema evolution and enforcement, and audit history</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#delta-log-internals-and-scaling","title":"Delta Log Internals and Scaling","text":"<p>The Delta Log acts as the transaction layer on top of Parquet files, recording transactions atomically in JSON files (e.g., 0.json, 1.json). The current state of the table is calculated by \"summing\" all transaction logs, applying additions and removals to determine the set of active Parquet files. </p> <p></p> <p>Delta Lake computes the latest state of a table by reading the Delta Log and performing a summation of all recorded transactions. This approach ensures data consistency and provides features like time travel.</p> <p>Core Components of a Delta Table</p> <p>A Delta Lake table is composed of two main elements</p> <ol> <li>Data Files (Parquet): The actual data is stored in immutable Parquet files.</li> <li>Transaction Layer (Delta Log): A transaction layer, called the Delta Log, resides on top of the data files. This log records every operation performed on the table.</li> </ol> <p>The Delta Log and Transaction Recording</p> <p>Each transaction or operation (such as an insert, update, or delete) on a Delta table is treated as an atomic unit of work.</p> <p>Recording Transactions: Transactions are recorded as sequential JSON files (e.g., 0.json, 1.json, 2.json) within the hidden _delta_log folder.</p> <p>Operation Details: Each JSON file contains details about the operation performed (e.g., create or replace table, write, update, delete) and tracks which Parquet files were added or logically removed. A transaction groups one or more operations together. The commit info section contains details about the operation and the user who performed it. The add section notes a new Parquet file that was added to the table state. The remove section logically marks a Parquet file as no longer part of the current table state (a soft delete).</p> <p>Computing the Latest State via Summation</p> <p></p> <p>When a user executes a query (e.g., SELECT * FROM table), Delta Lake processes the Delta Log to construct the latest valid view of the data:</p> <ol> <li> <p>Applying Transactions in Order: Delta Lake starts from the beginning of the transaction history and sequentially applies each transaction record (JSON file).</p> </li> <li> <p>Summation Principle: The system performs a \"summation\" of the transactions. It tracks all file additions (+) and removals (-). Files that are added and subsequently removed in later transactions cancel each other out.</p> </li> <li> <p>Handling Updates and Deletes: When an update or delete occurs, the original Parquet file containing the modified data is logically marked as REMOVE, and a new file (or files) containing the revised records is ADDed. For small changes, especially when deletion vectors are enabled, Delta Lake records the changes in the deletion vector file instead of rewriting the entire Parquet file immediately (Merge on Read approach). However, the log still records the necessary file additions and removals to reflect the logical state change.</p> </li> <li> <p>Final State Construction: The final, latest state of the table is computed by reading only the Parquet files that remain after all logical removals have been processed in the summation.</p> </li> </ol> <p>Scaling the Delta Log (Compacted JSONs and Checkpoints)</p> <p>For tables with millions of transactions, reading every JSON file in the Delta Log to compute the latest state would be computationally expensive and slow. </p> <p>To optimize this, Delta Lake uses two key scaling mechanisms:</p> <ol> <li> <p>Compacted JSONs: Compacted JSON files are used to reduce the overhead associated with reading many small JSON transaction files. Delta Lake periodically creates a compacted JSON file (e.g., a .compacted.json file). This compacted file picks up and dumps all the information (transactions and details) contained within a set of smaller JSON log files into one larger file, avoiding the need to open and close many individual files. If a transaction is committed, Delta can read the already compacted file plus any subsequent uncompacted files to compute the latest state. It avoids having to read all the historical individual JSON files.</p> </li> <li> <p>Checkpoint Parquet Files: Checkpoint files provide a mechanism to drastically reduce the number of files Delta must scan to determine the latest state of a table, offering a \"tremendous amount of optimization\". After a predetermined number of JSON files are created (e.g., 36 files in the Databricks environment used in the example), a checkpoint.parquet file is generated. This checkpoint file condenses all the information from the very first transaction (version zero) up to the point of the checkpoint.</p> </li> </ol> <p>To compute the latest state of the table, Delta only needs to find and read the latest checkpoint.parquet file and then apply any subsequent JSON files (or compacted JSONs) that have been created since that checkpoint. This allows Delta to skip reading potentially hundreds or thousands of older historical JSON log files. The gap at which a checkpoint file is created varies; it is 36 transactions in the Databricks version shown, but often 10 in the open-source version. This difference is due to Databricks implementing performance optimizations to relax this gap.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#pessimistic-concurrency-control","title":"Pessimistic concurrency control","text":"<p>Core Assumption</p> <p>PCC gets its name because the DBMS operates under the assumption that conflicts are bound to happen and are likely to occur. A conflict occurs when two or more transactions attempt to modify the same data simultaneously, resulting in an invalid state. It is a strategy used by database management systems (DBMS) to ensure that transactions maintain the correctness and consistency of the database.</p> <p>Mechanism: Exclusive Locking</p> <p>To prevent these conflicts, PCC uses a locking mechanism:</p> <p>\u2022 When a transaction attempts to make a change to a database, an exclusive lock is placed on that data.</p> <p>\u2022 This lock ensures that no other transaction can come in and make changes while the first transaction is running.</p> <p></p> <p>Example Walkthrough (T1 and T2)</p> <p>Consider an account with a starting balance of $1,000, and two simultaneous transactions: T1 (withdraw $100) and T2 (deposit $400).</p> <ol> <li>T1 Starts: Transaction T1 begins first.</li> <li>Lock Acquired: The moment T1 starts, it acquires an exclusive lock on the relevant row (the account balance).</li> <li>T2 Waits: Because T1 holds the lock, T2 cannot proceed and must wait until T1 completes.</li> <li>T1 Processing: T1 reads the balance ($1,000), subtracts $100, and computes the new balance as $900.</li> <li>T1 Updates and Releases: T1 updates the account balance to $900 and then releases the exclusive lock.</li> <li>T2 Starts: T2 now gets a chance and places its own exclusive lock on the row.</li> <li>T2 Processing: T2 reads the latest balance ($900), adds $400, computes the final balance as $1,300, and updates the account.</li> <li>Consistency Achieved: T2 then releases its lock, and the database is left in the correct state.</li> </ol> <p>Major Drawback</p> <p>While PCC achieves consistency, its major downside is performance when dealing with high concurrency:</p> <p>\u2022 Waiting: T2 had to wait until T1 was completed.</p> <p>\u2022 Scalability Issue: If a system handles millions of transactions, the waiting involved means the entire system is going to \"come to a halt\" because transactions wait endlessly for locks to be released, making the system slow and inefficient. This performance issue is why Delta Lake primarily achieves isolation using Optimistic Concurrency Control (OCC), which assumes conflicts are rare and avoids explicit locks</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#optimistic-concurrency-control","title":"Optimistic concurrency control","text":"<p>Core Assumption (Optimism)</p> <p>The name \"Optimistic\" comes from its fundamental assumption: conflicts are very unlikely to occur. Conflicts, where two or more transactions try to modify the same data and lead to an invalid state, are assumed to be rare. This assumption allows the system to prioritize speed and concurrency over preemptive conflict avoidance.</p> <p>Mechanism: Avoiding Locks</p> <p>With OCC, transactions do not obtain locks when they read or write data. This is the \"beautiful part\" of OCC. Because no locks are used, multiple transactions can read the same row simultaneously.</p> <p>When transactions (like T1 and T2) begin, they read the account balance, and crucially, they also read the associated version number and time frame of the data. This version number tracks changes within the table.</p> <p>Conflict Resolution and Validation</p> <p></p> <p>When transactions are finished processing and attempt to commit their changes, a validation procedure occurs:</p> <ol> <li>Concurrent Execution: T1 and T2 perform their calculations independently (e.g., T1 computes $900, T2 computes $1,400).</li> <li>First Commit Wins (T1): T1 attempts to commit first. It checks the current version number (let's say V1) against the version number it read (also V1). Since they match, T1 knows no one made changes while it was working. T1 succeeds, updates the balance (to $900), and updates the version number (to V2).</li> <li>Second Commit Fails (T2): T2 attempts to commit next. It checks the current version number (now V2) against the version it originally read (V1). Because V2=V1, T2 immediately understands that a person came before it and made changes to the table. T2 was working on stale data.</li> <li>The Retry: The conflicting transaction (T2) does not block; instead, it is designed to fail. It must then retry the transaction. It reads the latest state of the table (V2), re-applies its operation (adding $400 to 900),computesthecorrectfinalbalance(1,300), and attempts to commit again.</li> <li>Successful Second Attempt: T2 checks the version again (V2 = V2), finds it is working on the latest data, and updates the table (to $1,300) and the version number (to V3).</li> </ol> <p>High Concurrency Benefits</p> <p>This approach has significant advantages for modern data systems:</p> <p>\u2022 No Interference: Multiple simultaneous transactions can proceed \"without interfering with each other\". Each transaction operates as if it is the only one running. Intermediate or uncommitted changes are not visible to other transactions.</p> <p>\u2022 High Concurrency: This mechanism allows for a very high level of concurrency, as transactions avoid endless waiting caused by locks.</p> <p>\u2022 Suitability for Read-Heavy Systems: OCC works best in systems where writes (and therefore conflicts) happen very rarely. This efficient conflict resolution is why OCC, and the versioning ledger it relies on, is central to Delta Lake\u2019s architecture.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#time-travel-versioning","title":"Time Travel &amp; Versioning","text":"<p>Versioning</p> <p>It is the mechanism that helps Delta Lake track the different states of a table over time.</p> <p>\u2022 Tracking Changes: Every operation performed on a Delta table (such as a create, delete, update, or insert) results in the creation of a new version of the table.</p> <p>\u2022 Zero-Indexed: The first operation performed (like a CREATE OR REPLACE TABLE AS SELECT) creates version zero of the table. Subsequent operations increment this version number. For example, after version zero, a delete operation might create version one, an update might create version two, and a write (insert) might create version three.</p> <p>\u2022 Audit History: This version history allows users to see an audit trail of operations performed, including the timestamp, the user who performed the operation, and the operation details.</p> <p>Time Travel (Accessing Past States)</p> <p>Time travel is the ability to query or restore a Delta table to any of its previous versions. This is a core feature for data reliability, testing, and compliance.</p> <p>\u2022 Accessing Specific Versions: Users can select data from a specific historical version using either the version number or the timestamp of that version.</p> <p>Note</p> <p>Using Version Number: You can use the syntax VERSION AS OF [version_number] </p> <p>In a SELECT query (e.g., SELECT * FROM table VERSION AS OF 0).</p> <p>Using Timestamp: You can use the syntax TIMESTAMP AS OF [timestamp] in a SELECT query. The timestamp must be a time at which a version existed.</p> <p>\u2022 Handling Non-Exact Timestamps: If a user specifies a timestamp that doesn't exactly match a version's commit time, Delta Lake finds the latest version that existed before that specified time. For instance, if versions existed at 9:31 and 9:57, querying for 9:35 will return the state of version 9:31.</p> <p>\u2022 Restoring the Table: Beyond querying, a user can permanently roll the table back to a specific version using the RESTORE TABLE command (e.g., RESTORE TABLE [table_name] TO VERSION AS OF 0). This restoration itself is recorded as a new version in the table's history.</p> <p>Impact of Vacuum on Time Travel</p> <p>It is crucial to understand that the VACUUM command limits time travel capability.</p> <p>\u2022 File Deletion: When you run VACUUM, especially with a retention duration of zero, Delta Lake permanently deletes the physical data files that are no longer referenced by the current version (i.e., files that have been \"tombstoned\" or marked for soft delete by update/delete operations).</p> <p>\u2022 Version Incompleteness: If a file required to construct an older version is removed by a VACUUM operation, time travel back to that specific version will fail because the data required to fully reconstruct that historical state is incomplete or missing.</p> <p>For instance, in a demonstration, attempting to access version one of a table after a vacuum failed because the file written in version one was deleted, but accessing version zero worked because its necessary file was still present</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#schema-validation","title":"Schema Validation","text":"<p>Schema-on-Read (Traditional Data Lakes)</p> <p>The traditional approach used in standard data lakes is schema-on-read.</p> <p>Data is dumped into the lake in any format (structured, semi-structured, unstructured) and resides there. The application of the schema happens later, after the data has been stored, typically when a user runs a query to read the files.</p> <p>This flexibility leads to inconsistency. If different files are ingested with varying formats or column structures, writing a query becomes difficult, requiring complex logic to handle different fields and schemas.</p> <p>Schema-on-Write (Delta Lake Validation)</p> <p>Delta Lake uses a schema-on-write approach, which is essential for ensuring that every transaction brings the database from one valid state to another.</p> <p>Incoming data records are checked to validate their schema against the target Delta table's existing schema before ingestion occurs. This is done to prevent \"garbage data\" from corrupting the table.</p> Scenario Description INSERT Behavior MERGE Behavior 1. Column Order The source data columns are reordered relative to the target table. Matches columns by position. If data types are compatible, data corruption occurs as values are mapped incorrectly. Matches columns by name. It correctly identifies columns even if the physical order is changed. 2. Data Type Incoming data types do not match the target schema's definitions. Attempts to cast (convert) the incoming value to the table's data type (e.g., string '99499' to an integer). Fails if the string value cannot be cast (e.g., 'ABC' to integer), preventing data garbage. Similar automatic casting attempts are made. 3. Column Name Source data column names differ from the target table column names. Ignores name differences, as it matches columns by position. The write succeeds if positions align. Is strictly based on matching column names. If names are mismatched, the operation fails because it cannot resolve the column. 4. Nullability/Constraints The incoming data violates constraints (e.g., <code>NOT NULL</code> for a primary key). Fails if a <code>NOT NULL</code> constraint is violated by inserting a <code>NULL</code>. Applies to other rules like ensuring <code>price</code> is greater than zero. Fails if constraints are violated. 5. Extra Columns The source data contains more columns than the target table schema. Fails with a \"schema mishmash detected\" error. Is robust; it successfully inserts the data by focusing only on the columns defined in the target schema, ignoring the extra columns. <p>Since schema validation is critical for ensuring consistency and preventing corruption, the choice between <code>INSERT</code> and <code>MERGE</code> is often determined by how robustly you need to handle potential schema changes.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#schema-evolution","title":"Schema Evolution","text":"<p>Schema evolution is Delta Lake's capability to handle changes to a table's schema, such as adding, reordering, or modifying columns, without having to completely rewrite the underlying table or data. This functionality is crucial for maintaining flexibility in data pipelines.</p> <p>The ability to evolve a schema accommodates several common changes:</p> <ol> <li>Adding new columns.</li> <li>Upgrading a data type to a larger, more accommodating type (type widening).</li> <li>Handling changes within nested structures.</li> <li>Changing the physical position of columns.</li> </ol> <p>Adding New Columns (Scenario 1)</p> <p>New columns can be added using two main approaches:</p> <p>Manual Approach (<code>ALTER TABLE ADD COLUMN</code>): The user explicitly runs the <code>ALTER TABLE ADD COLUMN</code> statement to update the table's schema. Subsequent <code>INSERT</code> or <code>WRITE</code> operations that include this new column will succeed. Rows that existed before the new column was added will contain <code>NULL</code> values for that column.</p> <p>Automatic Schema Evolution: This approach is enabled by setting a specific table property to <code>true</code>. If the incoming data contains a new column that does not exist in the target Delta table, the system automatically accommodates it without requiring a manual <code>ALTER</code> statement.</p> <p>Type Widening (Scenario 2)</p> <p>Type widening refers to the ability to upgrade a data type to a \"bigger type\" to handle larger values, such as converting an <code>INT</code> to a <code>BIGINT</code> or a <code>FLOAT</code> to a <code>DOUBLE</code>.</p> <p>Enforcement: This capability was introduced in Delta version 3.2 (which corresponds to Databricks Runtime 15.4 used in the demonstration).</p> <p>Manual Change Required: Even with automatic schema evolution enabled, type widening often does not happen automatically. If an incoming value exceeds the capacity of the current column type (e.g., a huge number for an <code>INT</code> column), the system fails with an error. The user must manually run an <code>ALTER COLUMN</code> statement to change the type (e.g., <code>ALTER COLUMN customer ID TYPE BIGINT</code>) before the write will succeed.</p> <p>Nested Structure Evolution (Scenario 3)</p> <p>Delta Lake supports schema changes within nested <code>STRUCT</code> data types.</p> <p>Adding Attributes: You can add a new attribute to an existing <code>STRUCT</code> column using <code>ALTER TABLE ADD COLUMN</code> with the dot notation (e.g., <code>ALTER TABLE... ADD COLUMN purchase_details.store_location STRING</code>).</p> <p>Changing Nested Types: You can also manually change the data type of an attribute inside a <code>STRUCT</code> (e.g., changing <code>mall_pin_code</code> from <code>INT</code> to <code>BIGINT</code>) using <code>ALTER COLUMN purchase_details.mall_pin_code TYPE BIGINT</code>.</p> <p>Automatic Nested Evolution: To automatically add a new attribute to a nested <code>STRUCT</code> during an insert, the user must utilize a named struct. This is necessary because if a simple positional insert is used, the system won't know the key of the new attribute, leading to inconsistencies.</p> <p>Column Position Changes (Scenario 4)</p> <p>Delta Lake provides methods to control the physical placement of columns:</p> <p>Manual Positioning: When manually adding a column, you can specify its exact location using <code>FIRST</code> (to place it as the first column) or <code>AFTER [column_name]</code> (to place it after a specific column).</p> <p>Automatic Behavior: When automatic schema evolution is enabled (especially with a <code>MERGE</code> operation), if a new column is encountered in the source, Delta Lake generally appends that column as the last column in the table, even if the user intended a different position.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#parquet-to-delta","title":"Parquet to delta","text":"<p>Converting an existing Parquet file (or a folder containing Parquet files) into a Delta Lake table format is necessary to enable features like ACID transactions, time travel, and schema enforcement.</p> <p>The fundamental difference between a standard Parquet file and a Delta table is the presence of the transaction layer, or the Delta Log. When a directory contains Parquet data but no Delta Log folder, it is not yet a Delta table.</p> <p>The conversion process essentially creates this transactional layer and registers the existing data. There are two primary methods detailed in the video for converting Parquet to Delta:</p> <p>Using SQL / Shell Command</p> <p>The first method involves using the <code>CONVERT TO DELTA</code> command directly on the path of the Parquet data.</p> <p>This approach is straightforward and typically executed using Spark SQL or a shell command that interacts with the Delta Lake framework.When executed, this command instructs Delta Lake to perform the conversion.</p> <p>Using the Delta Table API</p> <p>The second method utilizes the Delta Lake API, typically within a Python or Scala environment, using the <code>DeltaTable</code> class.</p> <p>This requires importing the necessary library (e.g., <code>from delta.tables import DeltaTable</code>). The function used is <code>convertToDelta</code>. For example: <code>DeltaTable.convertToDelta(path, format='parquet')</code>.</p> <p>The Core Conversion Mechanism</p> <p>Regardless of the method used, the underlying process to transform the Parquet files into a Delta table is the same:</p> <ol> <li>Delta Log Creation: A new folder named <code>_delta_log</code> is created in the directory containing the Parquet files.</li> <li>Transaction Registration: The Delta Log is populated with its first transaction file, a JSON file (version <code>0.json</code>), which records the initial operation.</li> <li>Data Registration (The <code>ADD</code> Operation): This initial JSON file contains an ADD operation, which explicitly registers the existing Parquet file(s) as the contents of the newly created Delta table. This transaction registers the data.</li> </ol> <p>Once this transaction is recorded, the folder structure officially functions as a Delta Lake table.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#copy-on-write","title":"Copy on Write","text":"<p>Copy on Write is defined by the underlying architecture of data storage in Delta Lake: Parquet files.</p> <p>Immutable Files: Parquet files are immutable, meaning they cannot be modified directly once they are written to disk.</p> <p></p> <p>The Copy on Write Mechanism</p> <p>Since files cannot be changed directly, any operation that modifies the data must result in new files being created. Delta Lake follows the Copy on Write approach when deletion vectors are disabled.</p> <p>The mechanism involves three steps for any change (update, delete, or merge):</p> <ol> <li>Read: The system must read the entire existing Parquet file that contains the record(s) intended for modification.</li> <li>Apply Operation: The necessary changes (delete or update) are applied in memory.</li> <li>Rewrite: The entire file is then rewritten back to storage as a completely new file, either omitting the deleted records or including the updated records.</li> </ol> <p>For instance, to delete rows 1 and 6 from a file named <code>1.parquet</code>, Copy on Write rewrites the whole file into a new file (e.g., <code>2.parquet</code>) which omits those two rows. This new file (<code>2.parquet</code>) is then the one referenced for the latest version of the data.</p> <p>Computational Expense (The Drawback)</p> <p>The Copy on Write approach has a significant drawback: it is \"tremendously computationally expensive\".</p> <p>Costly Rewriting: The read and rewrite process is very costly. If a Parquet file contains 10 million rows and you only need to delete a \"few couple of rows,\" you still must read the whole file and rewrite the whole file back.</p> <p>Optimal Use Case</p> <p>Because Copy on Write involves expensive rewrites for frequent changes, it is not ideal for high-write scenarios.</p> <p>Read-Heavy Focus: CoW works best for use cases that are read heavy and where the frequency of writes (updates, deletes) is very low. If rights are high, the system will read and rewrite the whole file repeatedly, making the system slow.</p> <p>Copy on Write's performance limitations during frequent writes is the reason Delta Lake later introduced Deletion Vectors to enable the Merge on Read paradigm, which avoids these costly full file rewrites.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#merge-on-read","title":"Merge on Read","text":"<p>Merge on Read (MoR) is a critical concurrency paradigm in Delta Lake that is enabled when deletion vectors are turned on. It provides a high-performance alternative to the traditional Copy on Write approach.</p> <p></p> <ol> <li>The Core Problem Solved by MoR</li> </ol> <p>The fundamental challenge in data systems using immutable files (like Parquet, which Delta Lake uses under the hood) is how to handle updates and deletes efficiently. In the Copy on Write approach, any change requires reading the entire affected file, applying the change, and then rewriting the entire file back to storage. This rewrite process is \"tremendously computationally expensive,\" even if only a \"few couple of rows\" are modified in a file containing millions of records.</p> <ol> <li>The Mechanism: Deletion Vectors</li> </ol> <p>Merge on Read resolves this expensive rewrite problem by keeping the original Parquet files untouched.</p> <p>Recording Changes: Instead of modifying the data file, any changes (such as deletions) are recorded in a separate, much smaller file known as a deletion vector (DV).    DV Content: The deletion vector records the row number or index of the records that are intended for deletion.    Updates as Two Operations: An update operation is logically treated as a \"delete plus insert\". The old row is marked for deletion in the DV, and the newly updated row is written as a small, new Parquet file.</p> <ol> <li>The Read Process (The \"Merge\" Step)</li> </ol> <p>When a user runs a query to read the latest state of the table, the system uses the Merge on Read approach to combine the data:</p> <ol> <li>Read Files: The system reads the underlying Parquet file(s).</li> <li>Check DV: For each row read from the Parquet file, the system checks the deletion vector.</li> <li>Soft Delete: If the row's index is present in the DV, that row is treated as a soft delete and is skipped from the final output. If the row is not present in the DV, it is displayed.</li> <li> <p>Final State: The result is the latest state of the table, which is constructed from the original file combined with the information in the small DV bitmap file.</p> </li> <li> <p>Benefits and Optimal Use</p> </li> </ol> <p>Deletion vectors, and thus Merge on Read, bring significant performance advantages:</p> <p>Increased Write Performance: MoR avoids the costly process of rewriting the entire Parquet file for small changes. This dramatically reduces write latency.    Efficiency: It increases the performance of deletes, updates, and merge operations.    Best Use Case: MoR works best for use cases where data is updated frequently.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#deletion-vectors","title":"Deletion Vectors","text":"<p>The Problem Deletion Vectors Solve</p> <p>Underlying Delta Lake, data is stored in Parquet files, which are immutable.</p> <p>In the traditional approach, known as Copy on Write: if you want to update or delete just one record in a Parquet file containing millions of rows, the system must:</p> <ol> <li>Read the entire Parquet file.</li> <li>Apply the update or delete operation.</li> <li>Rewrite the entire file back, omitting the deleted row or including the updated row.</li> </ol> <p>This read-and-rewrite process is computationally expensive and is the bottleneck that deletion vectors aim to resolve.</p> <p>The Solution: Merge on Read and Deletion Vectors</p> <p>Deletion vectors enable Delta Lake to use the Merge on Read approach when they are enabled.</p> <p>Mechanism: Instead of rewriting the massive Parquet file, the original file remains untouched. The changes (deletions or updates) are recorded in a separate, small file called the deletion vector (DV).</p> <p>Content: The deletion vector acts as a bitmap, recording the row number or index of the records that are supposed to be removed or soft-deleted.</p> <p>Operation with Deletion Vectors</p> <p>When a transaction occurs, the transaction log is updated, often including the deletion vector:</p> <p>Delete Operation: A delete operation simply records the row numbers of the deleted records in the deletion vector file. For example, deleting row 1 and row 6 results in those numbers being recorded in the DV.    Update Operation: An update is treated as a \"delete plus insert\". When updating a row, the old version of the row is marked for deletion in the deletion vector, and the newly updated row is written as a new, small Parquet file.</p> <ol> <li>Reading the Data</li> </ol> <p>When a user reads the latest state of the table, the system checks both the Parquet data and the deletion vector:</p> <p>The system reads the Parquet file.    For each row read, it checks the deletion vector.    If the row's index is present in the DV, it is treated as a soft delete and is skipped from the final output, ensuring the file doesn't have to be rewritten for every minor change.</p> <ol> <li>Benefits and Tradeoffs</li> </ol> <p>Deletion vectors provide significant advantages:</p> <p>Performance: They reduce write latency by avoiding the rewriting of the whole file for small changes.    Efficiency: They bring Merge on Read capability to Delta Lake, increasing the performance of deletes, updates, and merge operations.</p> <p>However, the Merge on Read approach (with DVs enabled) works best for use cases where data is updated frequently. Conversely, the Copy on Write approach (DVs disabled) is generally better for read-heavy use cases.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#shallow-vs-deep-clone","title":"Shallow v/s Deep Clone","text":"<p>Cloning is a functionality in Delta Lake that allows you to create a snapshot of your Delta tables at a specific point in time [i, 148]. There are two main approaches or \"flavors\" to cloning: shallow clone and deep clone.</p> <p>Shallow Clone</p> <p></p> <p>A shallow clone involves taking a snapshot primarily of the table's metadata at a specific point in time.</p> <p>Mechanism and Data Storage:    When a table is shallow cloned, Delta Lake does not copy the underlying data files (such as Parquet files).    Instead, the shallow clone table simply references the underlying data files of the source table.    The target shallow clone table will start with a fresh history, beginning at version zero.    The metadata itself is duplicated, but not via a one-to-one copy of the source JSON transaction files. All information in the source table's delta log is condensed into a <code>0.checkpoint.parquet</code> file, and the latest state of the source table is computed and recorded in <code>0.json</code> in the target table's delta log.</p> <p>Performance and Cost:    The shallow clone operation is super fast and computationally cheap.    It does not use a lot of storage because the data files are only referenced, not copied.</p> <p>Independence and Limitations:    Once a shallow clone is created, it maintains its own history and set of operations.    Any changes, updates, or modifications made to the shallow clone table will not affect the source table, and similarly, changes to the source table will not affect the shallow clone table.    You can clone a table using a specific timestamp or version number from the source table, using <code>VERSION AS OF</code> or <code>TIMESTAMP AS OF</code>.    Since the shallow clone starts with a fresh version zero representing the latest state of the source, you cannot access the full history of the source table (e.g., versions 1 or 2) through the shallow clone table.    If a <code>VACUUM</code> operation is run on the source table after a shallow clone has been created, any underlying data files that are still being referenced by the shallow clone will not be removed from the source storage until those references are deleted in the shallow clone(s) as well.</p> <p>Deep Clone (or Copy)</p> <p> A deep clone, or deep copy, creates a completely independent copy of both the metadata and the data files.</p> <p>Mechanism and Data Storage:    A deep clone performs an exact replication of the underlying Parquet files from the source to the target location.    The target deep clone table is self-contained and independent.    Similar to shallow clones, the metadata starts at version zero in the target table, with the source transaction history being condensed into a checkpoint file (<code>0.checkpoint.parquet</code>) and the latest state computed and placed in <code>0.json</code>.</p> <p>Performance and Cost:    This operation takes a little bit longer and uses more storage compared to a shallow clone, because it copies the data files instead of referencing them.</p> <p>Independence and Advantages:    The deep clone is fully independent: changes in the deep clone table will not affect the source table, and changes in the source table will not affect the deep cloned table.    Deep clone operations are incremental. If you perform a subsequent deep clone (e.g., for disaster recovery synchronization), it does not copy the entire source table again. Instead, it only copies or syncs the incremental changes (such as updates or deletes) that have occurred since the last clone, making the operation robust and performant.    Deep cloning is considered a more robust way to clone than using a <code>CREATE TABLE AS SELECT (CTAS)</code> statement. A CTAS statement only uses the resulting rows to create a new table, often losing table properties, constraints, and partitioning details. A deep clone, however, also clones the metadata and properties of the table.</p> Feature Shallow Clone Deep Clone Data Files Referenced (not copied) Independent copy (copied one-to-one) Metadata Copied/Replicated Copied/Replicated Speed Super fast Takes a little bit longer Storage Use Computationally cheap, low storage Uses more storage Independence Independent history/changes post-clone Completely self-contained and independent <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#ctas-vs-deep-clone","title":"CTAS vs Deep Clone","text":"<p>The distinction between creating a table using a CTAS (CREATE TABLE AS SELECT) statement and performing a Deep Clone lies primarily in how the metadata, properties, and incremental changes are handled.</p> <p>Output Similarity</p> <p>Initially, if you were to look at the resulting tables at the output level (for example, by running a <code>SELECT *</code> query), the table generated using CTAS and the table generated using a deep clone would look exactly identical. They both contain the same underlying rows of data.</p> <p>Key Differences (Metadata and Properties)</p> <p>The crucial difference is what information is carried over from the source table to the new table:</p> <ol> <li>Handling of Properties: A CTAS statement creates a table just using the output of a select query. It takes the resulting set of rows and uses those rows to create the new table. Consequently, all of the properties are lost when using CTAS. These lost properties can include partitioning configurations and constraints.</li> <li>Robustness of Deep Clone: Deep clone, conversely, is described as a robust way to clone the metadata, the data, and the properties of the table. When you perform a deep clone, you do not need to respecify things like partitioning properties and constraints, because they are automatically cloned along with the data.</li> </ol> <p>Incremental Feature (Deep Clone Advantage)</p> <p>Another significant advantage of deep clone over CTAS is its ability to operate incrementally:</p> <ul> <li>Incremental Synchronization: Deep cloning is designed to work in an incremental manner. This capability is useful for scenarios like disaster recovery, where you need to maintain a synchronous replica of a source table.</li> <li>Performance on Resync: If you perform a deep clone to create a replica initially, and then later run the deep clone again to bring the tables back in sync (after updates or deletes occurred on the source), Delta Lake does not copy the whole source table again. Instead, it only copies or syncs the incremental changes that have occurred since the last clone. This ensures that the operation is performant and robust because only the necessary operations (updates or deletes) are applied to the replica.</li> </ul> <p>In summary, while CTAS is good for creating a simple, static snapshot of the rows, Deep Clone is a more sophisticated mechanism that ensures the integrity of the table structure and settings (metadata and properties) is maintained, and it allows for efficient, incremental synchronization.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#small-file-problem","title":"Small File Problem","text":"<p>The small file problem is a phenomenon that silently kills your Spark performance. It occurs when data intended to be part of a large dataset is scattered over thousands of smaller files.</p> <p>What is the Small File Problem?</p> <p>The small file problem arises because, although the total volume of data might be reasonable, the large number of small files introduces immense overhead for the processing engine.</p> <p>The source uses the analogy of a 300-page novel: instead of being a single 300-page PDF, someone saves each of the 300 pages as a separate PDF. To read the novel, the system would have to perform thousands of time-consuming operations:</p> <ol> <li>Find/Look up the file.</li> <li>Open the file.</li> <li>Read the file.</li> <li>Close the file.</li> </ol> <p>When reading a table scattered across thousands of smaller files, the thousands of open, close, and metadata lookup operations lead to wasted compute and poor I/O.</p> <p>Root Causes of the Small File Problem</p> <p>The video identifies three primary root causes that lead to the creation of excessive small files:</p> <ol> <li>Repartitioning to a very large number: If you have a file that is 10 GB in size and you use <code>.repartition(10000)</code>, you will end up with 10,000 resulting files, each only about 1 megabyte in size. These undersized files cause the small file problem.</li> <li>Partitioning on a high cardinality column: When partitioning data (using <code>partitionBy</code>) on a column that has a large number of distinct values (high cardinality, e.g., thousands of distinct values in a category column), you create many separate folders, each containing very small amounts of data.</li> <li>Frequently updated data sets: Data streams that receive continuous updates, perhaps every 5 minutes, write small updates in small chunks. These small chunks manifest as small files, leading to the small file problem.</li> </ol> <p>Solution: The <code>OPTIMIZE</code> Command</p> <p>Delta Lake provides a built-in operation called <code>OPTIMIZE</code> to solve the small file problem.</p> <p>The <code>OPTIMIZE</code> command works by compacting all of these small files into larger, more appropriately sized ones.</p> <p>Bin Packing Algorithm</p> <p>The compaction relies on an algorithm known as bin packing. This algorithm works as follows:</p> <ol> <li>It collects all file sizes and sorts them from high to low.</li> <li>It then places each file into a \"bin,\" where each bin represents a new, large consolidated file.</li> <li>The goal is to fill the bins up to a default target file size of 1 GB. This 1 GB target size is considered robust and should not be changed unless there is a compelling reason to do so.</li> </ol> <p>Post-Optimization Cleanup</p> <p>When <code>OPTIMIZE</code> is run, it creates the new, larger files but does not immediately remove the original small files.</p> <p>The small files that were compacted are tombstoned (marked for soft delete).    To physically remove the tombstoned files and recover storage space, you must run the <code>VACUUM</code> command.</p> <p>Incremental Compaction Approaches</p> <p>Beyond manual optimization, Delta Lake offers two automatic approaches to address small files generated during writes:</p> <ol> <li>Optimize Write: This feature combines all small writes intended for a partition into a single write command. It shuffles all the data and executes a single write command, which produces appropriately sized files for every partition. While effective for creating appropriate file sizes, it incurs a shuffle, which can be costly in terms of data transfer and right latency.</li> <li>Auto Compaction: This runs a compaction operation (similar to <code>OPTIMIZE</code>) automatically after a write finishes, if the number of small files exceeds a minimum threshold (the default minimum number of files to trigger auto compaction is 50, though this can be configured). This approach combines the small files into appropriately sized files.</li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#optimize-write","title":"Optimize Write","text":"<p>The feature Optimize Write is an automatic optimization technique used in Delta Lake, specifically designed to address one of the root causes of the small file problem.</p> <p>Here is a detailed explanation of how Optimize Write works and its implications, as described in the video:</p> <p>The Problem it Solves (Traditional Write)</p> <p>In a traditional write scenario, multiple processes or executors (e.g., Executor 1, 2, and 3) often try to write data concurrently to the same partition (for example, a date partition like <code>date=2025-05-01</code>). Since these writes happen simultaneously in small chunks, they result in the creation of many small Parquet files (e.g., <code>1.pk</code>, <code>2.pk</code>, <code>3.pk</code>) within that single partition folder. This proliferation of tiny files leads directly to the small file problem, which silently reduces Spark performance due to excessive file opening, closing, and metadata lookups.</p> <p>Mechanism of Optimize Write</p> <p>Optimize Write operates by transforming multiple intended small writes into a single, cohesive write operation.</p> <ol> <li>Data Shuffle: When Optimize Write is enabled, all data coming from different processes (Executor 1, 2, and 3) is first shuffled.</li> <li>Single Write Command: After the shuffle, the entire dataset intended for the partition is executed as a single write command.</li> </ol> <p>This process combines all the data together before it is physically written to the storage location.</p> <p>Outcome and Benefits</p> <p>The key benefit of using Optimize Write is that it produces appropriately sized files for every partition. By consolidating the output, it ensures that instead of scattering data across numerous small files, the data is packaged efficiently into large files (up to the target size, typically 1 GB), effectively avoiding the small file problem right at the source.</p> <p>Trade-offs and Costs</p> <p>While highly effective at preventing small files, Optimize Write introduces a notable trade-off:</p> <p>Shuffle Cost: The operation necessarily incurs a shuffle. Shuffles are costly because they involve data transfer across the network.    Write Latency: If your primary concern is optimizing for write latency (the time it takes for data to be written and become available), Optimize Write might not be the best solution because the involved shuffle will add time to the overall writing process.</p> <p>Therefore, Optimize Write is generally viewed as a very good solution for optimizing for the small file problem, but users must consider the added shuffle overhead. The source demonstrated this, showing that a query operation ran much quicker (1.82 seconds) on a table created with Optimize Write compared to a table created using traditional, small-file-generating methods (6.97 seconds).</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#vacuum","title":"Vacuum","text":"<p>The <code>VACUUM</code> command in Delta Lake is a crucial optimization technique used for managing the physical storage of data files associated with a Delta table.</p> <p>Primary Purpose and Mechanism</p> <p>The core function of <code>VACUUM</code> is to physically remove files from cloud storage or disk. Delta Lake employs a soft delete mechanism for data that is no longer part of the current table state:</p> <ol> <li>Soft Deletion: Whenever records are deleted, updated, or merged in a Delta table, the older, irrelevant underlying records or files are tombstoned or marked for soft deletion. This is done to maintain time travel and ACID properties without immediate costly file rewriting or removal.</li> <li>Physical Deletion: These tombstoned files are not physically deleted from storage until the <code>VACUUM</code> command is explicitly run. Running <code>VACUUM</code> removes all such files that are no longer referenced in the transaction log and have exceeded the retention duration.</li> <li>Storage Savings: The key benefit of running <code>VACUUM</code> is that it helps save on storage cost by permanently removing unnecessary files.</li> </ol> <p>Key Considerations</p> <ol> <li> <p>Query Performance: A popular misconception is that running <code>VACUUM</code> makes queries run faster. This is not the case. Query performance is determined by file filtering and pruning handled by the Delta Log, which directs the engine to scan only the necessary active files. <code>VACUUM</code> only cleans up the retired physical files, and the data being scanned remains the same.</p> </li> <li> <p>Retention Duration and Safety Checks: <code>VACUUM</code> respects a retention duration setting to ensure that time travel capabilities are not compromised prematurely.</p> </li> </ol> <p>Default Duration: The default retention duration is 168 hours (7 days). Files created within this period are generally not deleted.    Overriding Retention: To remove files that have been marked for soft delete even if they fall within the default retention period (e.g., immediately after an <code>OPTIMIZE</code> operation), you must specify a shorter duration, such as <code>RETAIN 0 HOURS</code>.    Safety Warning: Delta Lake usually issues a warning when a user attempts to run <code>VACUUM</code> with a low retention duration (e.g., zero hours). To proceed with low retention, a configuration check must be disabled: <code>set spark.databicks.delta.retentiondurationcheck.enable = false</code>.</p> <ol> <li>Limitation on Time Travel: Running <code>VACUUM</code> limits your ability to time travel. If a file associated with a previous version is physically deleted by the <code>VACUUM</code> operation, attempting to access that specific old version (<code>VERSION AS OF</code> or <code>TIMESTAMP AS OF</code>) will fail because the required underlying file data is missing.</li> </ol> <p>Interaction with Other Operations</p> <p><code>OPTIMIZE</code> Compaction: When the <code>OPTIMIZE</code> command is run to compact small files into one large file, the original small files are tombstoned. These small files remain physically present until <code>VACUUM</code> is run to remove them and recover the storage space.    Shallow Clones: <code>VACUUM</code> operations on a source table are constrained by active references from shallow clones. If a file in the source table is deemed obsolete (orphaned) but is still being referenced by a shallow clone, running <code>VACUUM</code> on the source table will not delete that file. The file will only be deleted after all references, including those in all shallow clones, are removed (e.g., by running a delete operation on the relevant data in the shallow clone tables) and then <code>VACUUM</code> is re-run on the source.    Deep Clones: <code>VACUUM</code> runs independently on deep clone tables. Since a deep clone is a self-contained and independent copy of the source data with no references linking back, running <code>VACUUM</code> on the source table will not affect the deep clone, and vice versa.</p> <p>The <code>VACUUM</code> command is necessary housekeeping in Delta Lake, ensuring that while the transaction log maintains a consistent view of the data, the storage layer only retains necessary files, similar to how recycling services remove old, unnecessary containers to free up space.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#z-ordering","title":"Z-ordering","text":"<p>The concept of Z-Ordering is one of the optimization techniques provided by Delta Lake, designed primarily to enhance query performance by maximizing data skipping and minimizing costly data transfers.</p> <p>The Problem Z-Ordering Solves (Costly Data Transfer)</p> <p>Before Z-ordering is applied, Delta Lake tables, like standard Parquet tables, store statistics (such as minimum and maximum values) for columns within the Delta Log.</p> <p>When a user submits a query with a filter (e.g., <code>WHERE age &gt;= 5 AND age &lt;= 10</code>), the query engine uses these statistics to determine which underlying data files need to be scanned and transferred into the memory of the compute cluster.</p> <p>If the required filter range overlaps with the minimum and maximum values recorded for a column in a specific Parquet file, that entire file must be transferred over the wire.    In poorly organized data, the statistics in different files often overlap significantly. This means that a filtered query may result in the necessary transfer and reading of all underlying data files, even if only a few records within those files actually match the criteria. Transferring all these files is a costly operation that consumes compute resources and leads to poor I/O performance.</p> <p>Mechanism of Z-Ordering</p> <p>Z-Ordering fundamentally optimizes the physical layout of data to minimize these file overlaps. It achieves this through a process often described as a sort and repartition.</p> <ol> <li>Collocating Similar Data</li> </ol> <p>The central idea is to collocate similar data points into the same physical data files. By grouping data together, the statistical ranges (min/max) of the resulting files become much tighter, meaning they are less likely to overlap.</p> <p>When a query is run after Z-ordering, the tight statistical ranges allow the engine to prune (filter out or skip) files that clearly do not contain the relevant data, avoiding their transfer over the network.</p> <ol> <li>Multi-Dimensional Mapping and Locality</li> </ol> <p>Z-Ordering is particularly powerful because it can apply this sorting principle across multiple columns simultaneously (multi-dimensional filtering).</p> <p>It works by taking the values from the selected columns and mapping this multi-dimensional space into a single dimension.    This mapping is done using a mathematical technique called a space-filling curve.    The curve ensures that data points which are close to each other in the original multi-dimensional space remain close to each other after being mapped to the single dimension (this concept is called preserving locality).</p> <p>This single, optimized dimension is then used to physically sort and arrange the data within the files, ensuring similar data points are grouped together.</p> <p>Using Z-Order with <code>OPTIMIZE</code></p> <p>The <code>ZORDER BY</code> command is not run in isolation; it must be executed together with the <code>OPTIMIZE</code> command.</p> <p>The two operations work hand in hand:</p> <ol> <li><code>OPTIMIZE</code> Compaction: The <code>OPTIMIZE</code> command\u2019s role is to solve the small file problem by merging many small files into fewer, larger files (called \"bins\"), typically targeting a size of 1 GB.</li> <li>Z-Ordering Collocation: While <code>OPTIMIZE</code> is consolidating the small files, Z-Order simultaneously performs the necessary collocation (sorting and rearranging) of the data into the new, larger files.</li> </ol> <p>Running <code>OPTIMIZE ZORDER BY [columns]</code> leads to significant performance improvements, often seeing query runtimes decrease substantially because the number of files scanned is drastically reduced.</p> <p>Z-Ordering with Partitions</p> <p>Z-Ordering can be applied incrementally and selectively, particularly when dealing with Hive-style partitions (e.g., partitioning by date). If a table is partitioned (e.g., by <code>invoice date</code>), you can use a predicate or <code>WHERE</code> condition to apply Z-Ordering only to specific, recently updated partitions (e.g., <code>OPTIMIZE ... WHERE category = 'fruit'</code>). This means the expensive Z-Ordering operation is only performed on the small subset of new data, rather than the entire table.</p> <p>Z-Ordering is like organizing a massive library collection not just by author (which is like partitioning) but also simultaneously by subject, genre, and length, so that when a reader asks for \"short sci-fi books from 2020,\" the librarian (the query engine) only needs to walk to one aisle and look at two shelves, instead of checking every aisle and every shelf just in case a relevant book overlapped into that section.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#liquid-clustering","title":"Liquid Clustering","text":"<p>Liquid Clustering is an advanced optimization technique in Delta Lake designed to provide flexibility and robust query performance by dynamically managing the physical data layout.</p> <p>It solves the primary problem of inflexibility inherent in traditional Hive-style partitioning and Z-Ordering.</p> <p>Limitations of Traditional Methods Solved by Liquid Clustering</p> <p>Hive-style partitioning and Z-Ordering require the user to decide the columns upfront when defining the table.</p> <p>If your query filter pattern changes over time (e.g., switching from filtering by <code>country</code> to filtering by <code>category</code>), the original data layout becomes unhelpful.    To address the new filter pattern, you would typically need to rewrite the entire dataset according to the new schema or layout.</p> <p>Liquid Clustering solves this by allowing you to change the clustering columns anytime, making the approach very flexible. It is also incremental, meaning that when you change clustering columns later on, the data layout changes going ahead to reflect the new desired pattern.</p> <p>Mechanism and Core Principles</p> <p>The liquid clustering algorithm is designed to maintain a balanced layout in the data files. It ensures several key outcomes simultaneously:</p> <ol> <li>Uniform File Size: It ensures that files are of uniform or appropriate file size. It avoids creating large files that skew processing or very small files that harm performance.</li> <li>Appropriate Number of Files: It manages the data layout to ensure an appropriate number of files, thereby helping to avoid the small file problem.</li> <li>Data Collocation: Similar data is collocated (placed close together) within the same physical files, which maximizes data skipping during query execution.</li> </ol> <p>How Liquid Clustering Addresses Small Files and Data Skew</p> <p>Liquid Clustering directly addresses two significant performance inhibitors: the small file problem and data skew.</p> <ol> <li>Avoiding the Small File Problem</li> </ol> <p>By ensuring uniform and appropriate file sizes, LC avoids the small file problem. It achieves this by:</p> <p>Merging Small Files: It combines many small files into appropriately sized files.    Breaking Down Large Files: Conversely, it can also break down bigger files that are too large (which might otherwise cause data skew) into appropriately sized chunks.</p> <ol> <li>Avoiding Data Skew</li> </ol> <p>Data skew occurs when some partitions contain a disproportionately large amount of data compared to others, creating bottlenecks and leaving compute resources idle.</p> <p>In a real-world scenario where partitions (e.g., partitioned by year) might vary drastically in size, the tasks processing the largest partitions must complete before the job finishes (these are on the \"critical path\").    Liquid Clustering works by dividing the larger chunks into smaller buckets. This ensures that all processing tasks or cores receive a uniform amount of data to process, avoiding the skew problem and resulting in higher resource utilization and faster completion times.</p> <p>Relationship with Optimization Techniques</p> <p>Liquid clustering cannot be used together with Z-Ordering or Hive-style partitioning; it serves as the independent, primary mechanism for organizing the data layout.</p> <p>When choosing clustering columns, best practices suggest:    Select the columns most frequently used in your query filters.    If transitioning from a table that used both Hive partitioning and Z-Ordering, use both the partition column and the Z-Order column as the new clustering key.    If certain columns are highly correlated (meaning choosing one implies the value of the other), only include the primary one as the clustering key.</p> <p>Liquid clustering provides the flexibility to change clustering columns down the line, avoiding the need to fix partitioning or Z-Order upfront, which protects against changing query patterns.</p> <p>Liquid clustering functions like a smart warehouse management system that doesn't just sort inventory based on a single, fixed attribute (like traditional partitioning/Z-Ordering). Instead, it continuously and dynamically rearranges the goods into uniformly sized boxes, grouping similar items together, and automatically breaking down massive shipments into manageable loads, ensuring that no worker is overwhelmed by one oversized box and that retrieval remains efficient even if customer demand shifts to a different set of categories.</p>"},{"location":"devops/docker/","title":"Docker","text":""},{"location":"devops/docker/#what-is-docker","title":"What is Docker?","text":"<p>Definition: Docker is an open-source containerization platform designed to build, deploy, and run applications in isolated environments called containers.</p> <p>Origin: Docker was first publicly released in March 2013, developed by Solomon Hykes and his team.    Motto/Logo: Docker's logo features a whale carrying containers, symbolizing its ability to transport and deploy applications easily from one place to another, much like ships carry physical containers across the ocean.</p> <p>Open Source: Docker is open-source, meaning it's free to use, though enterprise versions with added support are available.    Platform as a Service (PaaS): Docker operates as a Platform as a Service (PaaS) in the cloud. It provides a platform to run your applications, similar to how a railway track (platform) allows a train (code) to run.</p>"},{"location":"devops/docker/#docker-architecture-core-concepts","title":"Docker Architecture &amp; Core Concepts","text":"<p>Docker's core functionality revolves around OS-level virtualization, also known as containerization, which is an advanced form of virtualization.</p>"},{"location":"devops/docker/#docker-vs-traditional-virtualization-vmwarehypervisor","title":"Docker vs. Traditional Virtualization (VMware/Hypervisor)","text":"Feature Traditional Virtual Machine (VMware/Hypervisor) Docker (Containerization) Virtualization Level Hardware-level virtualization. A hypervisor (e.g., ESXi, VMware Workstation) is installed directly on the physical hardware or on top of an OS. OS-level virtualization. Docker Engine runs on top of the host operating system. Operating System Each VM requires its own full guest operating system (OS) (e.g., Windows, Linux). This consumes significant storage and RAM. Containers do not have their own full guest OS. They share the host OS's kernel. Resource Allocation Pre-allocated resources (RAM, storage, CPU) for each VM. These resources are dedicated to the VM whether it's actively using them or not. Resources are consumed on demand. Containers take resources from the host OS as needed and release them when done. Size VMs are heavyweight (e.g., 2-4 GB for an OS, plus application data). Containers are lightweight (e.g., 33MB for an Ubuntu image). Boot Time Slower boot times as each VM's OS needs to boot up. Extremely fast creation and startup. Portability VMs are portable but larger and less efficient to move. Highly portable. Containers can run on physical hardware, virtual hardware, or cloud platforms (e.g., AWS, Azure, Google Cloud). Dependency Management Manually manage dependencies within each VM's OS. Docker automatically pulls required dependencies from Docker Hub when creating containers or images. Cost Can be more expensive due to higher resource consumption and potential licensing costs for each OS. Cost-effective due to less resource consumption and no need for separate OS licenses. Isolation Provides strong isolation at the hardware level. Provides OS-level isolation. Processes within a container are isolated from other containers and the host system. <p>Core Difference Explained: In Docker, 95% of the operating system code needed by a container is taken from the host operating system's Linux kernel (since Docker's native design is for Linux). The remaining 5% are specific files for the desired OS distribution (like Ubuntu or Kali Linux), which are part of the Docker image. This makes containers extremely lightweight and efficient.</p>"},{"location":"devops/docker/#docker-ecosystem-components","title":"Docker Ecosystem Components","text":"<p>The Docker ecosystem is a set of software tools and packages that work together to create and manage containers.</p> <p>Docker Client: This is where Docker users interact with the Docker Daemon (server). Users write commands in the CLI (Command Line Interface) or use REST APIs to communicate with the Daemon. A single Docker client can communicate with multiple Docker servers.</p> <p>Docker Daemon (Docker Engine/Docker Server): This runs on the host operating system and is responsible for building images, running containers, and managing Docker services. It converts Docker images into running containers and executes commands from the Docker Client.</p> <p>Docker Hub (Registry): This is a cloud-based storage and management service for Docker images. It stores and categorizes images. There are two types of registries:        Public Registry: Images are publicly accessible to everyone (e.g., Docker Hub itself).        Private Registry: Companies use this to store images exclusively for their internal use and employees.</p> <p>Docker Images: These are read-only binary templates used to create Docker containers. They are like templates or Amazon Machine Images (AMIs). An image contains all the dependencies and configurations required to run a program or application.        Images are read-only: You cannot make changes directly to an image. Any modifications require creating a new image from a modified container.        Image states: When an image is running, it's called a container. When a container is stopped or paused, it becomes an image.</p> <p>Docker Containers: These are running instances of Docker images. They hold the entire package needed to run an application, including the OS files (the 5% specific ones) and supported software.        Containers act like virtual machines but are much lighter.        Layered File System: Containers are built in layers. When files or software are added to a container, they form new layers on top of previous ones. This layering allows for efficient storage and reusability.        Changes in Container: You can make modifications within a container, and then create a new image from that modified container.</p> <p>Docker Host: This refers to the physical hardware (laptop, server, cloud instance) on which the Docker Engine runs. It provides the environment and resources (CPU, RAM, storage) for Docker and its containers.</p>"},{"location":"devops/docker/#advantages-of-docker","title":"Advantages of Docker","text":"<p>Lightweight: Containers consume minimal resources (CPU, RAM, storage) compared to VMs because they don't include a full operating system. They only include the necessary components and share the host OS kernel.</p> <p>Cost-Effective: Due to lower resource consumption, Docker is cheaper than VMs. There's no pre-allocation of RAM, and no need for a separate OS installation for each application.</p> <p>No Pre-allocation of RAM: Docker only allocates RAM to a container when it's needed for an application, releasing it once the task is complete. This avoids resource wastage, unlike VMs where RAM is pre-reserved.</p> <p>Continuous Integration/Continuous Deployment (CI/CD) Efficiency: Docker simplifies the CI/CD pipeline. Developers build a container image, which can then be used consistently across every step of the deployment process (development, testing, production). This eliminates \"it works on my machine\" issues.</p> <p>Portability: Docker containers can run on any physical hardware, virtual hardware, or cloud platform (AWS, Azure, Google Cloud). This flexibility ensures applications run consistently across different environments.</p> <p>Image Reusability: Once an image is created (either from Docker Hub, a Dockerfile, or an existing container), it can be reused to create multiple containers or shared with other teams. This saves time and effort.    Faster Container Creation: Containers can be created in seconds or milliseconds, significantly faster than VMs, which can take minutes.</p>"},{"location":"devops/docker/#limitations-of-docker","title":"Limitations of Docker","text":"<p>Cross-Platform Compatibility Issues:        Applications designed to run in Docker containers on Windows generally will not run on Linux Docker containers, and vice-versa.        Even within Linux distributions, for optimal performance and compatibility, it's recommended that the development and testing/production environments use the same Linux distribution (e.g., Ubuntu development should be tested/deployed on Ubuntu). While a Linux container might run on a different Linux distribution (e.g., Ubuntu on CentOS), it might require downloading additional dependency files from Docker Hub.        Docker was originally designed for Linux, and 99% of industry usage is on Linux. While Windows 10 (Pro, Enterprise, not Home) now supports Docker with Hyper-V enabled, it still uses Linux files within the Docker Desktop tool.</p> <p>Not Ideal for Rich Graphical User Interface (GUI) Applications: Docker is better suited for command-line interface (CLI) based applications rather than those requiring extensive graphical user interfaces. For rich GUI applications, VMs might be a better solution.</p> <p>Difficulty Managing Large Number of Containers: While Docker Swarm and Kubernetes help, managing a very large number of Docker containers can become complex.</p>"},{"location":"devops/docker/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text file that contains a set of instructions for Docker to automatically build a Docker image. It's a method for automating Docker image creation and ensures consistent builds.</p> <p>Format: The filename must be exactly <code>Dockerfile</code>, with the 'D' capitalized.</p> <p>Instructions: Instructions within the Dockerfile must be in capital letters.</p> <p>Reusability: You can change instructions in a Dockerfile multiple times to create different new images. Each build from a Dockerfile creates a new, independent image.</p>"},{"location":"devops/docker/#dockerfile-instructions-commands","title":"Dockerfile Instructions &amp; Commands","text":"<p>Here are common Dockerfile instructions:</p> <p><code>FROM &lt;image&gt;:&lt;tag&gt;</code>: (Mandatory, must be the first instruction) Specifies the base image for your new image (e.g., <code>FROM ubuntu</code>). This is the foundation upon which your custom image will be built.</p> <pre><code>   Example: `FROM ubuntu`\n</code></pre> <p><code>RUN &lt;command&gt;</code>: Executes commands during the image build process. Each <code>RUN</code> instruction creates a new layer in the image.</p> <pre><code>   Example: `RUN echo \"Subscribe Technical Guftgu\" &gt; /tmp/test_file`\n</code></pre> <p><code>MAINTAINER &lt;name&gt;</code>: (Deprecated, use <code>LABEL</code> instead) Provides information about the maintainer of the Dockerfile.</p> <p><code>COPY &lt;source&gt; &lt;destination&gt;</code>: Copies files/directories from the local machine (where the Dockerfile is located) into the Docker image.</p> <pre><code>   Example: `COPY test_file_1 /tmp/`\n</code></pre> <p><code>ADD &lt;source&gt; &lt;destination&gt;</code>: Similar to <code>COPY</code>, but has additional functionalities:        Can fetch files from a URL.        Can automatically extract (unzip) compressed files (e.g., <code>.tar</code>, <code>.zip</code>) into the destination directory.</p> <pre><code>   Example: `ADD test.zip /tmp/`\n</code></pre> <p><code>EXPOSE &lt;port&gt;</code>: Informs Docker that the container listens on the specified network ports at runtime. It's a declaration, not an actual publishing of the port. For public access, ports must be published (e.g., using <code>-p</code> option with <code>docker run</code>).</p> <pre><code>   Example: `EXPOSE 80` (for HTTP) or `EXPOSE 8080` (for Jenkins).\n</code></pre> <p><code>WORKDIR &lt;path&gt;</code>: Sets the working directory for any <code>RUN</code>, <code>CMD</code>, <code>ENTRYPOINT</code>, <code>COPY</code>, or <code>ADD</code> instructions that follow it. If the directory doesn't exist, it will be created.</p> <pre><code>   Example: `WORKDIR /tmp`\n</code></pre> <p><code>CMD &lt;command&gt;</code>: Provides defaults for an executing container. It's the primary command that runs when a container starts. There can only be one <code>CMD</code> instruction per Dockerfile. If multiple <code>CMD</code> instructions are present, only the last one takes effect.</p> <p><code>ENTRYPOINT &lt;command&gt;</code>: Similar to <code>CMD</code>, but it sets the main command for the container's execution. Arguments to <code>ENTRYPOINT</code> are always run, even if <code>CMD</code> is also provided. It has higher priority than <code>CMD</code>.</p> <p><code>ENV &lt;key&gt;=&lt;value&gt;</code>: Sets environment variables within the Docker image. These variables persist when the container is run.</p> <pre><code>   Example: `ENV MY_NAME=\"Bhupender Rajput\"`\n</code></pre> <p><code>ARG &lt;name&gt;[=&lt;default value&gt;]</code>: Defines build-time variables that users can pass to the builder with the <code>docker build --build-arg &lt;name&gt;=&lt;value&gt;</code> command. (User's homework in source 67)</p>"},{"location":"devops/docker/#building-an-image-from-dockerfile","title":"Building an Image from Dockerfile","text":"<p>To build an image from a Dockerfile, navigate to the directory containing the Dockerfile and run the <code>docker build</code> command.</p> <p>Command: <pre><code>docker build -t &lt;image_name&gt; .\n</code></pre> <code>-t &lt;image_name&gt;</code>: Tags the image with a name (e.g., <code>my_image</code>).    <code>.</code> (dot): Refers to the current directory, indicating that the Dockerfile is in the current working directory.</p> <p>Example: <pre><code># Assuming Dockerfile is in current directory\ndocker build -t new_image .\n</code></pre></p>"},{"location":"devops/docker/#docker-volumes","title":"Docker Volumes","text":"<p>Docker Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. They are normal directories that are declared to act as volumes.</p> <p>Purpose:     Decouple Container from Storage: Data in volumes persists even if the container is deleted.     Share Data: A single volume can be shared and accessed by multiple containers simultaneously.     Host-Container Data Sharing: Volumes can map a directory on the Docker host machine to a directory inside the container, allowing seamless data synchronization between them.</p> <p>Declaration: A directory must be declared as a volume only while creating a container or in a Dockerfile (for image creation). You cannot create a volume from an already existing container.</p> <p>Sharing Mechanism: Data put into a shared volume by one container will be visible to all other containers sharing that same volume.</p> <p>Image Creation and Volumes: If you create an image from a running container that has a volume, the volume's content will be included in the image. However, when a new container is created from this image, the volume will act as a simple directory; it will not be shared as a volume with other containers by default. Sharing needs to be explicitly re-established for new containers.</p>"},{"location":"devops/docker/#methods-to-create-and-share-volumes","title":"Methods to Create and Share Volumes","text":"<ol> <li> <p>Using Dockerfile (for image creation):        Declare the volume within your <code>Dockerfile</code> using the <code>VOLUME</code> instruction.</p> <p>Dockerfile Example:     <pre><code>FROM ubuntu\nVOLUME /my_volume  # Creates a directory named 'my_volume' in the container's root, declared as a volume\n</code></pre></p> <p>Build the image from this Dockerfile: <code>docker build -t my_image .</code></p> <p>Create a container from this image: <code>docker run -it --name container_one my_image /bin/bash</code></p> <p>Now, <code>/my_volume</code> inside <code>container_one</code> is a volume. You can navigate into it (<code>cd /my_volume</code>) and create files (<code>touch file_A file_B</code>).</p> </li> <li> <p>Using <code>docker run</code> command (direct volume declaration):        This method allows direct volume creation and mapping when launching a container.</p> <p>Command Example (Creating a new volume):     <pre><code>docker run -it --name container_three -v /volume_two ubuntu /bin/bash\n</code></pre> <code>-v /volume_two</code>: Creates a directory named <code>volume_two</code> inside <code>container_three</code> and declares it as a volume.</p> <p>Command Example (Host-to-Container Mapping):     <pre><code>docker run -it --name host_container -v /home/ec2-user:/rajput ubuntu /bin/bash\n</code></pre> <code>-v &lt;host_path&gt;:&lt;container_path&gt;</code>: Maps a directory on the host machine (<code>/home/ec2-user</code>) to a directory inside the container (<code>/rajput</code>). </p> <pre><code>Changes in either location will reflect in the other.\n</code></pre> </li> </ol>"},{"location":"devops/docker/#sharing-volumes-between-containers","title":"Sharing Volumes Between Containers","text":"<p>Volumes can be shared between containers using the <code>--volumes-from</code> option during container creation.</p> <p>Scenario: Share <code>my_volume</code> from <code>container_one</code> with <code>container_two</code>.</p> <ol> <li>Create <code>container_one</code> with a volume:     <pre><code># Assuming 'my_image' was built with VOLUME /my_volume in Dockerfile\ndocker run -it --name container_one my_image /bin/bash\n# Inside container_one, create some files in /my_volume\ncd /my_volume\ntouch file_1 file_2 file_3\nexit\n</code></pre></li> <li> <p>Create <code>container_two</code> and share volume from <code>container_one</code>:     <pre><code>docker run -it --name container_two --privileged=true --volumes-from container_one ubuntu /bin/bash\n</code></pre> <code>--privileged=true</code>: Grants full read/write privileges to the shared volume.</p> <p><code>--volumes-from container_one</code>: Tells Docker to share all volumes from <code>container_one</code> with <code>container_two</code>.</p> </li> <li> <p>Verify Sharing:        Inside <code>container_two</code>, navigate to <code>/my_volume</code>. You will see <code>file_1</code>, <code>file_2</code>, <code>file_3</code>.</p> <p>Create a new file in <code>/my_volume</code> from <code>container_two</code>: <code>touch new_file_from_two</code>.</p> <p>Exit <code>container_two</code>.</p> <p>Attach back to <code>container_one</code>: <code>docker attach container_one</code>.</p> <p>Navigate to <code>/my_volume</code> in <code>container_one</code> and list contents: <code>ls</code>. You will see <code>new_file_from_two</code>. This confirms bi-directional sharing.</p> </li> </ol>"},{"location":"devops/docker/#volume-management-commands","title":"Volume Management Commands","text":"<p><code>docker volume ls</code>: Lists all Docker volumes on the local machine.</p> <p><code>docker volume create &lt;volume_name&gt;</code>: Creates a new Docker volume.</p> <p><code>docker volume rm &lt;volume_name&gt;</code>: Deletes a specific Docker volume.</p> <p><code>docker volume prune</code>: Deletes all unused (not attached to any container) Docker volumes.</p> <p><code>docker volume inspect &lt;volume_name&gt;</code>: Displays detailed information about a specific Docker volume.</p>"},{"location":"devops/docker/#essential-docker-commands-with-examples","title":"Essential Docker Commands (with Examples)","text":"<p>Here are basic and important Docker commands:</p> <p>Note</p> <p>Check Docker Service Status:     <pre><code>service docker status\n</code></pre>     Output: Shows if Docker is <code>running</code> or <code>stopped</code>.</p> <p>Note</p> <p>Start Docker Service:     <pre><code>service docker start\n</code></pre>     Purpose: Starts the Docker Engine.</p> <p>Note</p> <p>Check Docker Info (Detailed Information):     <pre><code>docker info\n</code></pre>     Purpose: Provides comprehensive details about Docker installation, including OS, memory usage, root directory, etc..</p> <p>Note</p> <p>List Docker Images:     <pre><code>docker images\n</code></pre>     Purpose: Lists all Docker images available on your local Docker Engine.</p> <p>Note</p> <p>Search Docker Hub for Images:     <pre><code>docker search &lt;image_name&gt;\n</code></pre>     Purpose: Searches Docker Hub for images related to the specified name (e.g., <code>ubuntu</code>, <code>jenkins</code>, <code>centos</code>).</p> <p>Note</p> <p>Pull an Image from Docker Hub:     <pre><code>docker pull &lt;image_name&gt;\n</code></pre>     Purpose: Downloads a specified image from Docker Hub to your local Docker Engine.     Example: <code>docker pull jenkins</code></p> <p>Note</p> <p>Run a Docker Container:     <pre><code>docker run -it --name &lt;container_name&gt; &lt;image_name&gt; /bin/bash\n</code></pre>     Purpose:</p> <pre><code>    Creates a new container from an image and runs it.\n    `-it`:\n        `-i`: Interactive mode (keeps STDIN open even if not attached).\n        `-t`: Allocate a pseudo-TTY (provides a terminal inside the container).\n    `--name &lt;container_name&gt;`: Assigns a custom name to your container. If not specified, Docker generates a random name.\n    `&lt;image_name&gt;`: The name of the image to create the container from (e.g., `ubuntu`, `centos`, `jenkins`).\n    `/bin/bash`: Command to execute inside the container to get a bash shell.\n    Example: `docker run -it --name my_ubuntu_container ubuntu /bin/bash`\n</code></pre> <p>Note</p> <p>Run a Container with Port Mapping:     <pre><code>docker run -d --name &lt;container_name&gt; -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\n</code></pre>         Purpose: </p> <pre><code>Creates and runs a container, mapping a port on the host to a port in the container, allowing external access.\n\n`-d`: Detached mode (runs the container in the background).\n\n`-p &lt;host_port&gt;:&lt;container_port&gt;`: Publishes (exposes) a container's port to the host.\n    `host_port`: The port on the Docker host (e.g., EC2 instance).\n    `container_port`: The port inside the container where the application is listening (e.g., 80 for HTTP, 8080 for Jenkins).\n\nExample (HTTP server): `docker run -d --name web_server -p 80:80 ubuntu`\n    (Note: For the web server example, you would then need to `docker exec` into the container, install Apache, and create a webpage as shown in the source to serve content.)\n\nExample (Jenkins server): `docker run -d --name jenkins_server -p 8080:8080 jenkins/jenkins`\n    (Remember to enable the `8080` port in your EC2 instance's security group for external access.)\n</code></pre> <p>Note</p> <p>List Running Containers:     <pre><code>docker ps\n</code></pre>     Purpose: Shows only the currently running containers.</p> <p>Note</p> <p>List All Containers (Running and Stopped):     <pre><code>docker ps -a\n</code></pre>     Purpose: Shows all containers, including those that have exited (stopped).</p> <p>Note</p> <p>Start a Stopped Container:     <pre><code>docker start &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Starts one or more stopped containers.</p> <p>Note</p> <p>Stop a Running Container:     <pre><code>docker stop &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Stops one or more running containers.</p> <p>Note</p> <p>Remove a Container:     <pre><code>docker rm &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Removes one or more stopped containers.     Important: You cannot remove a running container directly. You must stop it first.</p> <p>Note</p> <p>Attach to a Running Container:     <pre><code>docker attach &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Connects to the main process of a running container. If the main process exits, the container will stop.</p> <p>Note</p> <p>Execute a Command in a Running Container:     <pre><code>docker exec -it &lt;container_name_or_id&gt; &lt;command&gt;\n</code></pre>     Purpose: Runs a new command in a running container, typically to get a shell. It creates a new process inside the container without stopping the main process.     Example: <code>docker exec -it my_ubuntu_container /bin/bash</code> (to get a bash shell inside the container)</p> <p>Note</p> <p>Create an Image from a Container:     <pre><code>docker commit &lt;container_name_or_id&gt; &lt;new_image_name&gt;\n</code></pre>     Purpose: Creates a new image from the current state of a running or stopped container. This is useful after making changes inside a container that you want to persist in a new image.     Example: <code>docker commit my_container updated_image</code></p> <p>Note</p> <p>Check Port Mappings of a Container:     <pre><code>docker port &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Lists the port bindings for a container, showing which container ports are mapped to which host ports.</p>"},{"location":"devops/git/","title":"Git","text":"<p>Git is a Version Control System (VCS), specifically a distributed Version Control System, that was created by Linus Torvalds in 2005. It serves as a tool to manage and track changes in code over time, allowing developers to create \"snapshots\" or versions of their project. This enables capabilities like rewinding to previous states, fast-forwarding, and critically, facilitates teams working collaboratively on the same codebase without overwriting each other's work. Today, Git has become the de facto standard for developers, with 93% of them reportedly using it. It's important to distinguish Git itself from hosting services like GitHub, GitLab, or Bitbucket; these platforms utilize Git but are not Git inherently.</p> <p></p> <p>To effectively manage code, Git operates across three primary zones or stages on your local machine: the working directory, the staging area, and the local repository.</p> <ol> <li>The working directory is where you actively write code, fix bugs, and make changes to your files. When you're using an Integrated Development Environment (IDE) like VS Code, you're working within this directory, which is simply your local project folder. At this stage, Git is passively tracking what's happening but has not yet \"remembered\" any of your changes.</li> <li>The staging area, also referred to as the index or a \"waiting room\" or \"shopping cart\" for changes, is the intermediate step. When you execute the <code>git add</code> command on a file, you are taking changes from your working directory and placing them into this area, explicitly telling Git that you intend to include these specific changes in the next snapshot. This allows you to handpick exactly what changes go into each commit, giving you fine-grained control and helping to keep your commit history clean. It also functions as a safety net, enabling you to review what you're about to commit using <code>git status</code> or <code>git diff</code> before making it permanent.</li> <li>The local repository is where Git permanently saves the snapshots of your project's history on your computer. This is housed within a hidden <code>.git</code> folder in your project directory. When you run <code>git commit</code> with a message, Git takes all the changes currently in the staging area and records them as a commit in your local repository. Each commit acts like a checkpoint in time, allowing you to always revert to it. Git stores an entire snapshot of files at a per-commit level, but it does so efficiently by only storing new or changed file content and pointing to existing unchanged file content, which contributes to its efficiency. Git objects like Blobs (for files) and Trees (for directories) are used internally to manage this data. The <code>git log</code> command helps you view this commit history.</li> </ol> <p>The full local Git workflow typically involves you editing files in your working directory, then staging them with <code>git add</code>, and finally committing them with <code>git commit</code> to your local repository. If you need to temporarily set aside changes without committing, <code>git stash</code> can move them out of the working directory onto a stack.</p> <p>Beyond your local machine, there's the remote repository, which can be thought of as the cloud version of your codebase, serving as a central hub for team collaboration. Services like GitHub, GitLab, or Bitbucket host these remote repositories. After making commits in your local repository, you use <code>git push</code> to upload those local commits to a designated remote branch, making them visible to your team. Conversely, to incorporate updates from your teammates, you use <code>git pull</code>, which effectively fetches new changes from the remote and merges them into your local copy. The <code>git fetch</code> command specifically downloads commits and references without merging, updating remote-tracking branches (e.g., <code>origin/main</code>) in your local repository. The <code>origin</code> remote is conventionally set up automatically when you clone a repository, pointing back to the source you cloned from.</p>"},{"location":"devops/git/#branching-and-merging-strategies","title":"Branching and Merging Strategies","text":"<p>A branch in Git is simply a pointer to a commit, making branches lightweight and inexpensive to create. This feature is fundamental to Git workflows, allowing developers to work on new features or bug fixes in isolation. The <code>git switch</code> command (or the older <code>git checkout</code>) allows you to move between different branches, effectively updating your working directory to reflect the state of that branch.</p> <p>When independent changes are made on different branches that originated from a common point, those branches are said to be diverging. To bring these changes together, Git provides two primary mechanisms: merging and rebasing.</p> <p>Merging combines the history from one branch into another. Git first identifies the merge base, which is the nearest common ancestor commit of the two branches. It then integrates the changes from both branches, creating a new commit called a merge commit. A merge commit is unique because it has two parents, representing the tips of the two merged branches. While merging preserves the true history of the project, showing where branches diverged and merged, it can lead to many merge commits, which might make the history harder to read. A fast-forward merge occurs when there is no diverging history on the target branch; Git simply moves the pointer of the target branch forward to the tip of the merged branch without creating a new merge commit.</p> <p>Rebasing, in contrast to merging, rewrites history. When you rebase one branch onto another (e.g., <code>feature</code> onto <code>main</code>), Git takes the commits from the <code>feature</code> branch, moves them back to their common ancestor, and then \"replays\" them one by one on top of the latest commit of the <code>main</code> branch. This creates new commit objects for the replayed changes, resulting in a linear history that can be easier to read and manage, especially for preparing pull requests. However, you should never rebase a public branch like <code>main</code>, as this rewrites history that others may have already pulled, leading to significant synchronization issues and conflicts for collaborators. Rebasing your own private branches onto a public one is generally acceptable.</p>"},{"location":"devops/git/#handling-conflicts","title":"Handling Conflicts","text":"<p>Conflicts arise when Git attempts to combine changes (during a merge or rebase) where the same lines of code have been modified differently in the diverging branches, and Git cannot automatically decide which change to keep.</p> <p>Git will insert conflict markers (<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>=======</code>, <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>) into the affected files. To resolve these, you must manually edit the file to remove the markers and choose the desired code. After resolving, you must <code>git add</code> the file to stage the resolution, and then either <code>git commit</code> (for a merge) or <code>git rebase --continue</code> (for a rebase) to finalize the process.</p> <p>The <code>git checkout --ours</code> and <code>git checkout --theirs</code> commands can assist in conflict resolution by picking changes from either the current branch (<code>ours</code> in a merge) or the branch being merged/rebased (<code>theirs</code> in a merge, but <code>ours</code> during rebase refers to the target branch and <code>theirs</code> to the branch being replayed). For recurring conflicts, the <code>git rerere</code> (reuse recorded resolution) feature, if enabled, can remember how you resolved a specific conflict hunk and automatically resolve it the next time it appears.</p>"},{"location":"devops/git/#undoing-changes","title":"Undoing Changes","text":"<p>Git offers several powerful commands for undoing changes:</p> <ul> <li><code>git reset</code>: This command is used to undo recent commits or changes in the index (staging area) or working tree.<ul> <li><code>git reset --soft &lt;commit&gt;</code>: Moves the branch pointer back to the specified commit but keeps all changes from the undone commits as staged changes in your staging area, leaving your working directory untouched. This is useful for redoing a commit or fixing its message before pushing.</li> <li><code>git reset --hard &lt;commit&gt;</code>: Moves the branch pointer back to the specified commit and discards all changes in both the staging area and the working directory that were made after that commit. All uncommitted or staged changes are permanently lost from your working files, though the commit objects themselves may still be recoverable using <code>git reflog</code>. Untracked files are not removed by <code>git reset --hard</code>.</li> </ul> </li> <li><code>git revert &lt;commit&gt;</code>: Instead of erasing history, <code>git revert</code> undoes the changes introduced by a specific commit by creating a new commit that is the inverse of the commit being reverted. This preserves the project's history, showing both the original commit and the subsequent commit that undid its effects.</li> <li><code>git reflog</code>: This is an extremely useful command that keeps a record of where your <code>HEAD</code> (your current position in history) has been. It allows you to recover commits or states that might seem lost after actions like hard resets, rebases, or accidental branch deletions, because the underlying commit objects are still referenced in the reflog.</li> <li><code>git commit --amend</code>: This command allows you to change the message of your last commit. You can also stage additional changes before running <code>git commit --amend</code> to incorporate them into the previous commit. Be cautious: this command rewrites history by changing the SHA hash of the last commit, so do not amend commits that have already been pushed to a public branch.</li> </ul>"},{"location":"devops/git/#remote-collaboration-features-hosting-services","title":"Remote Collaboration Features (Hosting Services)","text":"<p>While Git is the underlying VCS, many collaborative features are provided by hosting services like GitHub:</p> <ul> <li>Forking: This is a feature offered by hosting services, not a core Git command. A fork creates a personal copy of an original repository under your own account, allowing you to modify the project without directly affecting the original. This is the standard method for contributing to open-source projects. When working with forks, it's common to add a second remote, typically named <code>upstream</code>, pointing to the original repository to easily pull in its latest changes.</li> <li>Pull Request (PR): Also a feature of hosting services, a pull request is a mechanism to propose changes from one branch (often a feature branch on your fork) to another (e.g., <code>main</code> on the original repository). PRs facilitate discussion and code review before changes are merged into the main codebase.</li> </ul>"},{"location":"devops/git/#ignoring-files","title":"Ignoring Files","text":"<p>The <code>.gitignore</code> file is crucial for telling Git which files or patterns of files it should ignore and not track. This is commonly used for generated files (like build outputs), log files, or dependency directories (<code>node_modules/</code>). You can place <code>.gitignore</code> files at the root of your repository or in subdirectories, and their rules apply to that directory and its subdirectories. The file supports standard shell wildcards, anchored patterns, and negation rules.</p>"},{"location":"devops/git/#advanced-git-commands","title":"Advanced Git Commands","text":"<ul> <li><code>git stash</code>: Temporarily saves changes in your working directory and staging area without committing them, allowing you to switch to a clean working state. <code>git stash list</code> shows stashes, <code>git stash pop</code> applies and removes the most recent, and <code>git stash drop</code> removes a stash.</li> <li><code>git squash</code>: Not a standalone command, but an action typically performed using <code>git rebase -i</code> (interactive rebase). It allows you to combine multiple commits into a single, cleaner commit, often used before merging a feature branch to streamline history.</li> <li><code>git cherry-pick</code>: Applies the changes introduced by a specific commit from one branch onto another branch by creating a new commit on the target branch with those changes. It requires a clean working tree.</li> <li><code>git bisect</code>: A debugging tool that uses a binary search algorithm to efficiently find the specific commit that introduced a bug. You mark commits as \"good\" (bug not present) or \"bad\" (bug present), and Git iteratively checks out commits in the middle of the range until the culprit is found.</li> <li><code>git worktree</code>: Allows you to have multiple working directories linked to the same repository, with each worktree potentially having a different branch checked out simultaneously. You cannot work on a branch that is already checked out by another worktree.</li> <li><code>git tag</code>: Used to mark specific points in history as important, such as release versions (e.g., v1.0.0). A tag is an immutable pointer to a commit. When you <code>git checkout</code> a tag, you enter a \"detached HEAD\" state, meaning you cannot commit directly to it.</li> </ul>"},{"location":"devops/git/#popular-branching-strategies","title":"Popular Branching Strategies","text":"<p>Understanding different branching strategies is key for team collaboration:</p> <ul> <li>Feature Branching: This is a straightforward approach where for each new feature or bug fix, a dedicated branch is created off the <code>main</code> branch. All work for that feature happens on this isolated branch, and once complete and tested, it's merged back into <code>main</code>, often via a pull request. The core idea is that the <code>main</code> branch always contains production-ready code, keeping incomplete work separated. This is extremely common and forms the basis for many workflows, including those on GitHub. While it scales well and allows multiple developers to work without interfering, it requires eventual integration, which can lead to merge conflicts if <code>main</code> has diverged significantly. Regularly updating feature branches from <code>main</code> can mitigate this.</li> <li>Gitflow: A more structured and complex model suited for projects with regular release cycles and multiple versions to maintain. Gitflow introduces specific long-lived branches for different purposes: a <code>master</code> or <code>main</code> branch for production code (where each commit is a release), and a <code>develop</code> branch where integration of the latest development changes occurs. Feature branches are created from <code>develop</code> and merged back into it. Release branches are used to prepare new versions, branching off <code>develop</code> for final polish and bug fixes, and then merged into both <code>master</code> (for release) and <code>develop</code> (to incorporate fixes into ongoing development). Hotfix branches are for quickly addressing critical issues found in production, branching off <code>master</code> and merging back into both <code>master</code> and <code>develop</code>. While robust for large, structured projects, its complexity can be overkill for smaller teams or continuous development environments.</li> <li>GitHub Flow: A lightweight and simpler workflow popularized by GitHub, commonly seen in open-source projects. It centers around one main branch (often called <code>main</code>) that is always deployable. For any new work, a short-lived feature branch is created directly off <code>main</code>, work is done on it, and then a pull request (PR) is opened to merge it back into <code>main</code>. Code review and automated tests happen during the PR process. There are no dedicated <code>develop</code> or <code>release</code> branches, simplifying the process. If a critical bug arises, a quick fix branch is made from <code>main</code>. This flow is ideal for smaller teams and emphasizes simplicity and speed.</li> <li>GitLab Flow: This is a hybrid model that combines the simplicity of GitHub Flow with the idea of environment-specific or release branches. Like GitHub Flow, feature branches are often developed off <code>main</code> and merged back into <code>main</code> when ready, with the code in <code>main</code> considered deployable. However, GitLab Flow suggests using environment branches (e.g., <code>staging</code>, <code>production</code>) or tags to manage deployments to different environments. After merging features into <code>main</code>, code might be deployed to staging by merging <code>main</code> into <code>staging</code>, and then to production by merging <code>main</code> into <code>production</code> or deploying a tagged release. This provides more structure for teams with multiple deployment targets, clearly defining which commit is on which environment. While more complex than GitHub Flow, it's less heavy than Gitflow and is often adopted by teams using GitLab CI/CD pipelines.</li> <li>Trunk-Based Development (TBD): This strategy prioritizes simplicity and continuous integration, relying on a single, long-lived branch, usually called <code>trunk</code> or <code>main</code>. Developers commit to this main branch directly or via very short-lived branches, ideally merging back daily or multiple times a day. The \"trunk\" signifies it as the single source of truth where all work quickly lands. This approach minimizes large merge conflicts, as changes are incremental and integration delays are reduced. New features are often hidden behind feature flags, meaning incomplete code can be merged into <code>main</code> but disabled in production, ensuring the codebase is always deployable. TBD is the foundation for modern DevOps, enabling rapid integration and deployment (e.g., 100 deployments a day) and is enforced by large tech companies like Google, Meta, and Amazon to keep vast codebases clean and moving. This strategy is preferred by large tech companies aiming for rapid integration and deployment.</li> </ul>"},{"location":"devops/kube2/","title":"Labels, Selectors, and Replication Controllers","text":"<p>Kubernetes provides powerful mechanisms for organizing and managing containerized applications. At the core of this organization are Labels, which serve as a foundational concept, enabling the grouping and identification of objects within a cluster. Building upon labels, Selectors allow users to filter and retrieve these labeled objects efficiently. These two concepts are then leveraged by higher-level controllers like ReplicationController and ReplicaSet to manage the desired state and scaling of application Pods.</p>"},{"location":"devops/kube2/#labels-organizing-kubernetes-objects","title":"Labels: Organizing Kubernetes Objects","text":"<p>Labels are fundamental to organizing Kubernetes objects. They are essentially key-value pairs that can be attached to any Kubernetes object, such as Pods, Nodes, or other resources, to provide identifying attributes. The video likens labels to stickers on kitchen containers, where a label like \"salt\" on a box helps you identify its contents without opening it. Similarly, in a Kubernetes environment, labels help identify and categorize objects, especially when dealing with hundreds of Pods.</p> <p>The primary purpose of labels is to help in organizing and managing objects. For instance, if you have multiple Pods running a specific application (e.g., \"Technical Guftgu application\"), you can attach a label like <code>app: technical-guftgu</code> to those Pods. This allows you to quickly sort and search for all Pods associated with that application. Labels are flexible and user-defined, meaning there's no predefined set of labels; you can create any key-value pair that is meaningful to your specific needs. For example, <code>class: pods</code> or <code>environment: development</code> could be custom labels. An object can have multiple labels attached to it, similar to how a person might have multiple labels like \"Bhupinder Rajput,\" \"Teacher,\" or \"Technical Guftgu\".</p> <p>Here\u2019s an example of how labels are defined within a Pod's YAML manifest, specifically under the <code>metadata.labels</code> section:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-daily-pod\n  labels:\n    environment: development\n    class: pods\n    my-name: bhupinder # This label was added later using an imperative command\nspec:\n  containers:\n  - name: c0\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"echo Hello Bhupinder &amp;&amp; sleep 3600\"]\n</code></pre> <p>The video demonstrates adding initial labels (<code>environment: development</code> and <code>class: pods</code>) via a YAML file, which is a declarative method. It also shows how to add an additional label (<code>my-name: bhupinder</code>) to an already existing Pod using an imperative command, demonstrating the flexibility of managing labels directly:</p> <pre><code>kubectl label pods my-daily-pod my-name=bhupinder\n</code></pre> <p>To view the labels attached to Pods, you can use the command:</p> <pre><code>kubectl get pods --show-labels\n</code></pre>"},{"location":"devops/kube2/#selectors-filtering-objects-by-labels","title":"Selectors: Filtering Objects by Labels","text":"<p>Selectors are the mechanism used to filter and retrieve Kubernetes objects based on their attached labels. While labels help in defining and organizing, selectors help in finding those objects. The Kubernetes API currently supports two main types of label selectors: Equality-Based Selectors and Set-Based Selectors.</p> <ol> <li> <p>Equality-Based Selectors: These selectors match objects based on whether a label's value is strictly equal to or not equal to a specified value.</p> <ul> <li>Equal (<code>=</code> or <code>==</code>): Selects objects where the label's value matches exactly.</li> <li>Not Equal (<code>!=</code>): Selects objects where the label's value does not match exactly.</li> </ul> <p>For example, to find all Pods with the label <code>environment</code> set to <code>development</code>, you would use:</p> <pre><code>kubectl get pods -l environment=development\n</code></pre> <p>To find Pods where the <code>environment</code> label is not <code>development</code>, you would use:</p> <pre><code>kubectl get pods -l environment!=development\n</code></pre> </li> <li> <p>Set-Based Selectors: These selectors allow for more complex matching conditions based on sets of values or the existence/absence of a label.</p> <ul> <li><code>in</code>: Selects objects where the label's value is within a specified set of values.</li> <li><code>notin</code>: Selects objects where the label's value is not within a specified set of values.</li> <li><code>exists</code> (just the key): Selects objects that have a label with a specific key, regardless of its value. The video doesn't provide a direct command for <code>exists</code> but mentions it as a type of set-based selector.</li> </ul> <p>Example using <code>in</code> for Pods where <code>environment</code> is either <code>development</code> or <code>testing</code>:</p> <pre><code>kubectl get pods -l 'environment in (development,testing)'\n</code></pre> <p>Example using <code>notin</code> for Pods where <code>environment</code> is not <code>development</code> and not <code>testing</code>:</p> <pre><code>kubectl get pods -l 'environment notin (development,testing)'\n</code></pre> <p>Selectors can also be combined using commas to match multiple labels. For instance, to find Pods that have both <code>class: pods</code> AND <code>my-name: bhupinder</code> labels:</p> <pre><code>kubectl get pods -l 'class=pods,my-name=bhupinder'\n</code></pre> <p>Selectors are also crucial for deleting objects based on labels. For example, to delete Pods where the <code>environment</code> label is <code>development</code>:</p> <pre><code>kubectl delete pods -l environment=development\n</code></pre> <p>Another application of selectors is in Node selection. A Pod can be explicitly scheduled onto a specific Node by first applying a label to the Node, and then configuring the Pod's manifest with a <code>nodeSelector</code> that targets that Node's label. For example, if a Node is labeled <code>hardware: medium</code>, a Pod can be configured to run only on that Node. This is particularly useful in heterogeneous clusters where certain Pods require specific hardware capabilities.</p> <p>First, label the desired Node (e.g., <code>minikube</code>):</p> <pre><code>kubectl label nodes minikube hardware=medium\n</code></pre> <p>Then, define the <code>nodeSelector</code> in the Pod's YAML:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: node-labels\n  labels:\n    environment: development\nspec:\n  nodeSelector:\n    hardware: medium # This ensures the pod runs on a node with this label\n  containers:\n  - name: c0\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"echo Hello Bhupinder &amp;&amp; sleep 3600\"]\n</code></pre> </li> </ol>"},{"location":"devops/kube2/#replicationcontroller-ensuring-pod-availability-and-scaling","title":"ReplicationController: Ensuring Pod Availability and Scaling","text":"<p>While individual Pods are the smallest deployable units in Kubernetes, they are not self-healing by default. If a Pod crashes or is deleted, it will not automatically restart or be re-created by the cluster. This is where ReplicationController (RC) comes into play. The RC is a Kubernetes object designed to ensure that a specified number of identical Pod replicas are always running at any given time. It acts as a controller that continuously monitors the desired state (number of replicas) and the current state of Pods matching its selector.</p> <p>The key benefits of using a ReplicationController include: *   Reliability/High Availability: If a Pod fails, crashes, or is terminated, the RC automatically detects the discrepancy between the desired and current state and creates a new Pod to maintain the specified replica count. This ensures continuous service availability for users. The video clarifies that a new Pod is created, not the old one restarted, and it will have a new IP address. *   Load Balancing: By ensuring multiple replicas, the RC inherently supports distributing incoming traffic across these identical Pods, preventing a single instance from being overloaded. *   Scaling: The RC allows for both scaling up (increasing the number of Pods to handle more load) and scaling down (decreasing the number of Pods when demand is low). This is crucial for real-time applications like streaming services (e.g., Netflix, Disney+ Hotstar) that experience fluctuating user loads.</p> <p>A ReplicationController manifest defines the desired number of replicas, a selector to identify the Pods it manages, and a Pod template that describes the Pods to be created.</p> <p>Here\u2019s a sample ReplicationController manifest:</p> <pre><code>apiVersion: v1\nkind: ReplicationController # Specifies the object type\nmetadata:\n  name: my-replica # Name of the ReplicationController\nspec:\n  replicas: 5 # Desired number of Pod replicas\n  selector:\n    my-name: bhupinder # RC will manage Pods with this label\n  template: # Defines the Pods to be created\n    metadata:\n      name: test-pod # Name for the Pods created by this RC\n      labels:\n        my-name: bhupinder # Pods must have this label to be managed by the selector\n    spec:\n      containers:\n      - name: c0\n        image: ubuntu\n        command: [\"/bin/bash\", \"-c\", \"echo Hello Bhupinder &amp;&amp; sleep 3600\"]\n</code></pre> <p>To apply this manifest and create the ReplicationController:</p> <pre><code>kubectl apply -f my-rc.yaml\n</code></pre> <p>To check the status of the ReplicationController and the Pods it manages:</p> <pre><code>kubectl get rc\nkubectl get pods\n</code></pre> <p>The video demonstrates the auto-healing capability by manually deleting a Pod created by the RC. Upon deletion, the RC immediately creates a new Pod to maintain the desired count of five replicas.</p> <p>To scale the number of replicas, you can use the <code>kubectl scale</code> command. For instance, to change the number of Pods from 5 to 8, or from 8 to 1:</p> <pre><code># Scale up from 5 to 8 replicas\nkubectl scale --replicas=8 rc my-replica\n\n# Scale down from 8 to 1 replica\nkubectl scale --replicas=1 rc my-replica\n</code></pre>"},{"location":"devops/kube2/#replicaset-the-advanced-replication-controller","title":"ReplicaSet: The Advanced Replication Controller","text":"<p>ReplicaSet (RS) is considered an advanced version of the ReplicationController. While both serve the same core purpose of maintaining a stable set of identical Pod replicas, the key distinction lies in their selector capabilities.</p> <p>The primary difference is that ReplicationController only supports equality-based selectors, meaning it can only match Pods whose labels are exactly equal to or not equal to a specified value. In contrast, ReplicaSet supports both equality-based selectors and set-based selectors. This expanded selector capability allows ReplicaSet to manage Pods with more complex label matching requirements, using operators like <code>in</code>, <code>notin</code>, or simply checking for the existence of a label.</p> <p>Another notable difference is their API version: ReplicationController typically uses <code>v1</code>, whereas ReplicaSet is available in <code>apps/v1</code>.</p> <p>Here\u2019s a sample ReplicaSet manifest:</p> <pre><code>apiVersion: apps/v1 # API version for ReplicaSet\nkind: ReplicaSet # Specifies the object type\nmetadata:\n  name: my-rs # Name of the ReplicaSet\nspec:\n  replicas: 2 # Desired number of Pod replicas\n  selector:\n    matchLabels: # Equality-based selector\n      my-name: bhupinder\n    matchExpressions: # Set-based selector (can be used in addition or instead)\n      - {key: env, operator: In, values: [dev, prod]} # Example of set-based selector\n  template: # Defines the Pods to be created\n    metadata:\n      name: test-seven # Name for the Pods created by this RS\n      labels:\n        my-name: bhupinder # Pods must have this label\n        env: dev # Example label for set-based selector\n    spec:\n      containers:\n      - name: c0\n        image: ubuntu\n        command: [\"/bin/bash\", \"-c\", \"echo Technical Guftgu &amp;&amp; sleep 3600\"]\n</code></pre> <p>To apply this manifest and create the ReplicaSet:</p> <pre><code>kubectl apply -f my-rs.yaml\n</code></pre> <p>To check the status of the ReplicaSet and the Pods it manages:</p> <pre><code>kubectl get rs\nkubectl get pods\n</code></pre> <p>Similar to ReplicationController, ReplicaSet also provides auto-healing and scaling capabilities. If a Pod managed by the ReplicaSet is deleted, a new one will be created instantly to maintain the desired replica count. Scaling up or down is also achieved using the <code>kubectl scale</code> command, similar to RC:</p> <pre><code># Scale down from 2 to 1 replica\nkubectl scale --replicas=1 rs my-rs\n\n# Scale up from 1 to 3 replicas (example not in source, but derived from scale command)\nkubectl scale --replicas=3 rs my-rs\n</code></pre>"},{"location":"devops/kube2/#relationships-between-concepts","title":"Relationships Between Concepts","text":"<p>The concepts of Labels, Selectors, ReplicationController, and ReplicaSet are deeply interconnected in Kubernetes:</p> <ol> <li>Labels serve as the fundamental identifying metadata attached to Kubernetes objects. They provide context and categorisation for various resources within the cluster.</li> <li>Selectors depend entirely on labels. They are the querying mechanism used to find and group objects that possess specific labels. Without labels, selectors would have no information to filter upon.</li> <li>ReplicationController and ReplicaSet are higher-level Kubernetes objects that utilize both labels and selectors to manage the lifecycle and scaling of Pods. Both controllers maintain a desired number of Pods by continuously checking the Pods that match their defined <code>selector</code>. The <code>template</code> within these controllers specifies the characteristics of the Pods they are responsible for creating, including the labels that must match the controller's <code>selector</code>. The ReplicaSet offers more flexibility in its selector due to supporting set-based matching.</li> <li>In essence, labels organize, selectors find, and ReplicationControllers/ReplicaSets use these two mechanisms to reliably and scalably manage Pods.</li> </ol> <p>```</p>"},{"location":"devops/kube3/","title":"Jobs, init container and pod lifecycle","text":""},{"location":"devops/kube3/#kubernetes-jobs","title":"Kubernetes Jobs","text":"<p>Kubernetes Jobs are a distinct type of object in Kubernetes, designed for a specific purpose that differs from standard Pods like those managed by Deployments or ReplicaSets. While standard Pods are engineered to ensure an application or service remains continuously running\u2014meaning if a Pod fails or is deleted, a new one is automatically created to replace it\u2014Jobs serve a different function. A Job is meant to perform a task once and then terminate. Once the task is complete, the Job's Pod does not get recreated automatically.</p> <p>The fundamental difference lies in their recreation policy. Pods (via ReplicaSets, Deployments, or StatefulSets) ensure high availability by recreating Pods if they fail, thereby maintaining service uptime. In contrast, a Job executes a task to completion, and then the associated Pod automatically terminates. It will not recreate itself after finishing its work.</p> <p>Jobs are highly useful for various one-time or scheduled tasks. Examples include: *   Log rotation: Deleting old logs and creating new ones at specific intervals, for instance, every two hours. *   Database backups: Creating a Pod to take a database backup, which then terminates after the backup is complete. *   Helm chart installations: Using Jobs to perform specific installation tasks within a Helm chart. *   Background processes: Running a specific, time-bound task in the background that needs to finish and then stop, without continuous recreation. *   Scheduled tasks: Performing a specific job after a certain time period, like an hour or thirty minutes, and then ensuring it stops automatically.</p> <p>When you define a Job in a YAML manifest, the <code>apiVersion</code> will be <code>batch/v1</code>, as Jobs operate within the batch category. The <code>kind</code> field will be <code>Job</code>. Inside the <code>spec.template.spec</code> section, you define the container(s) that will perform the task, similar to a regular Pod. A crucial aspect of a Job's Pod definition is the <code>restartPolicy</code>, which should be set to <code>Never</code>. This policy ensures that once the container completes its task, it terminates and is not restarted.</p> <p>Here is an example of a simple Job YAML:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-container\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo 'Technical Guru'; sleep 5\"] # This command prints and then waits 5 seconds\n      restartPolicy: Never # Crucial for Jobs\n</code></pre> <p>In this example, the container will print \"Technical Guru\" and then sleep for 5 seconds. After 5 seconds, the container will stop, and the Job will mark it as complete.</p> <p>An important distinction from other Kubernetes objects is that the Job object itself is not deleted automatically once its task is complete. While the Pod created by the Job will terminate, the Job resource will persist. You must manually delete the Job using <code>kubectl delete -f &lt;your-job-manifest.yaml&gt;</code>.</p>"},{"location":"devops/kube3/#parallelism-in-kubernetes-jobs","title":"Parallelism in Kubernetes Jobs","text":"<p>Kubernetes Jobs also support parallelism, allowing you to run multiple Pods concurrently for a specific task. This is useful when you have a job that can be broken down into several independent sub-tasks that can be processed simultaneously.</p> <p>To configure parallelism, you add the <code>parallelism</code> field within the <code>spec</code> section of your Job manifest. For instance, setting <code>parallelism: 5</code> would instruct Kubernetes to create and run five Pods concurrently for the Job.</p> <p>Additionally, you can define an <code>activeDeadlineSeconds</code> field, which specifies the maximum duration in seconds that a Job can be active. If the Job exceeds this time limit, all its running Pods will be terminated, and the Job will be marked as failed. For example, <code>activeDeadlineSeconds: 40</code> would mean the Job will run for a maximum of 40 seconds before its Pods are terminated and deleted, even if the task isn't fully completed.</p> <p>Here is an example of a Job YAML demonstrating parallelism and an active deadline:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: parallel-job\nspec:\n  parallelism: 5 # Creates 5 pods to run in parallel\n  activeDeadlineSeconds: 40 # Max 40 seconds for the job to be active\n  template:\n    spec:\n      containers:\n      - name: my-container\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo 'Working in parallel!'; sleep 20\"] # Work for 20 seconds\n      restartPolicy: Never\n</code></pre> <p>In this scenario, five Pods will be created, each executing the specified command. The <code>activeDeadlineSeconds</code> ensures that after 40 seconds, all these Pods will be terminated, regardless of their completion status.</p>"},{"location":"devops/kube3/#cronjobs","title":"CronJobs","text":"<p>CronJobs extend the functionality of Jobs by allowing them to be scheduled to run periodically. They are similar to the traditional <code>cron</code> utility found in Unix-like operating systems. CronJobs are ideal for tasks that need to run at regular intervals, such as daily reports, hourly data cleanups, or backups.</p> <p>For a CronJob, the <code>kind</code> field in the YAML manifest is <code>CronJob</code>. The crucial field for scheduling is <code>schedule</code>, which takes a cron format string (e.g., <code>* * * * *</code> for every minute, <code>0 0 * * *</code> for once a day at midnight). The actual Job definition is nested under <code>jobTemplate.spec</code>, which has the same structure as a standard Job's <code>spec</code>.</p> <p>Here is an example of a CronJob YAML:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cronjob\nspec:\n  schedule: \"* * * * *\" # Runs every minute\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cron-container\n            image: busybox\n            command: [\"sh\", \"-c\", \"echo 'Technical Guru printing every minute for 5 seconds'; sleep 5\"]\n          restartPolicy: OnFailure # Or Never, depending on task\n</code></pre> <p>This CronJob will create a new Pod every minute. Each Pod will run for 5 seconds, printing \"Technical Guru printing every minute for 5 seconds,\" and then terminate. After its completion, the Pod is marked as <code>Completed</code>.</p>"},{"location":"devops/kube3/#init-containers","title":"Init Containers","text":"<p>Init Containers (short for \"Initialization Containers\") are specialized containers that run before the main application containers in a Pod. Their primary purpose is to perform setup or initialization tasks that are required for the main application to function correctly.</p> <p>A key characteristic of Init Containers is their sequential execution and dependency: *   Order: Init Containers run one by one, in the order they are defined. *   Completion: Each Init Container must complete successfully before the next one starts. *   Success Requirement: If an Init Container fails, Kubernetes will repeatedly restart the Pod (including all Init Containers) until that specific Init Container succeeds. The main application container will not start until all Init Containers have successfully completed.</p> <p>Init Containers are valuable for managing dependencies or preparing the environment for the main application. Some common use cases include: *   Database Seeding: Populating a database with initial data or creating necessary tables before the application starts consuming it. *   Waiting for Dependencies: Delaying the main application's start until external services (like a database or another microservice) are fully ready and reachable. *   Configuration Provisioning: Fetching configuration files or secrets from an external source and writing them to a shared volume that the main application container can then read. *   Pre-installation/Setup: Installing required packages, binaries, or performing complex setup scripts that are prerequisites for the main application. *   Volume Population: Downloading or preparing data into a shared volume, which the main container then uses.</p> <p>Init Containers share volumes with the main application container, allowing them to pass data or prepared files. For example, an Init Container can download a file to a volume, and the main container can then access that file from the same mounted volume.</p> <p>Here is an example of a Pod YAML with an Init Container:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: init-demo\nspec:\n  initContainers: # Init Containers defined here\n  - name: init-myservice\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Like and Subscribe Technical Guftgu' &gt; /tmp/exchange/test-file; sleep 30\"] # Creates a file with content\n    volumeMounts:\n    - name: workdir\n      mountPath: /tmp/exchange\n  containers: # Main application container\n  - name: main-app\n    image: busybox\n    command: [\"sh\", \"-c\", \"while true; do cat /tmp/data/test-file; sleep 5; done\"] # Accesses the file\n    volumeMounts:\n    - name: workdir\n      mountPath: /tmp/data # Note: different mountPath for demonstration of access\n  volumes: # Shared volume\n  - name: workdir\n    emptyDir: {}\n</code></pre> <p>In this example, the <code>init-myservice</code> Init Container will first run. It creates a file named <code>test-file</code> in the <code>/tmp/exchange</code> directory (which is mounted from the <code>workdir</code> volume) and writes \"Like and Subscribe Technical Guftgu\" into it, then sleeps for 30 seconds. Only after this Init Container successfully completes will the <code>main-app</code> container start. The <code>main-app</code> container then continuously reads the content of <code>test-file</code> from its own mount path <code>/tmp/data</code> (which maps to the same shared <code>workdir</code> volume), demonstrating that it can access the data prepared by the Init Container.</p>"},{"location":"devops/kube3/#pod-lifecycle-and-conditions","title":"Pod Lifecycle and Conditions","text":"<p>Understanding the Pod lifecycle and its various phases and conditions is fundamental to troubleshooting and monitoring applications in Kubernetes. These represent the different states a Pod can be in from creation to termination.</p>"},{"location":"devops/kube3/#pod-phases","title":"Pod Phases","text":"<p>The Pod lifecycle can go through several distinct phases:</p> <ul> <li> <p>Pending: In this phase, the Pod has been accepted by the Kubernetes system, but one or more of its containers are not yet running. This could mean the Pod is waiting for a node to be assigned, or the container images are still being downloaded, or the required resources (like CPU or memory) are not yet available on any node.</p> </li> <li> <p>Running: The Pod has been bound to a node, and at least one container within it is actively running. This indicates that the Pod has been successfully scheduled and its application is operational. Even if multiple containers are defined in a Pod, as long as one is running, the Pod's phase will be 'Running'.</p> </li> <li> <p>Succeeded: This phase means that all containers in the Pod have successfully terminated, and they will not be restarted. This is the expected phase for Jobs after they complete their tasks.</p> </li> <li> <p>Failed: If all containers in the Pod have terminated, and at least one container terminated in failure (i.e., exited with a non-zero exit code), or was terminated by the system, the Pod enters the 'Failed' phase. This often indicates an issue with the application or its environment.</p> </li> <li> <p>Unknown: This phase indicates that the state of the Pod could not be obtained, typically due to a communication error between the Kubernetes Master (API Server) and the node where the Pod is supposed to be running. This can happen because of network issues or problems with the Kubelet on the node.</p> </li> </ul>"},{"location":"devops/kube3/#pod-conditions","title":"Pod Conditions","text":"<p>When you use the command <code>kubectl describe pod &lt;pod-name&gt;</code>, you can inspect the conditions of a Pod, which provide more detailed insights into its current state and any events that have occurred. These conditions are Boolean values (True, False, Unknown) that indicate whether a particular state is met. Some common Pod conditions include:</p> <ul> <li> <p>PodScheduled: This condition is <code>True</code> if the Pod has been successfully scheduled to a specific node. It indicates that the scheduler has found a suitable node for the Pod to run on.</p> </li> <li> <p>Initialized: This condition becomes <code>True</code> once all Init Containers in the Pod have successfully completed their tasks. As seen in the Init Container example, this would be <code>False</code> while the Init Container is still running or failing.</p> </li> <li> <p>ContainersReady: This condition is <code>True</code> when all containers within the Pod are ready. This means they are running and passing their readiness probes (if configured).</p> </li> <li> <p>Ready: This condition signifies that the Pod is ready to serve traffic. It is <code>True</code> only if both <code>PodScheduled</code>, <code>Initialized</code>, and <code>ContainersReady</code> are <code>True</code>, and all readiness probes (if defined for the main containers) are successful.</p> </li> <li> <p>Unschedulable: If the Pod cannot be scheduled to any available node, perhaps due to insufficient resources (like CPU or memory) or other scheduling constraints, this condition might become <code>True</code>.</p> </li> </ul> <p>Observing these phases and conditions through commands like <code>kubectl get pods</code> and <code>kubectl describe pod</code> is a crucial skill for monitoring and debugging your Kubernetes deployments.</p>"},{"location":"devops/kube4/","title":"Deploying object","text":"<p>Kubernetes Deployments are a crucial object that provides declarative updates for Pods and ReplicaSets. They act as a top-level object or a manager (like a general manager or supervisor) that controls ReplicaSets, which in turn control Pods. This hierarchy offers significant advantages, especially when dealing with application updates and rollbacks.</p> <p>Historically, ReplicaSets and Replication Controllers were not able to handle application updates or rollbacks effectively in real-time environments. For instance, if an update to an application like WhatsApp or Facebook caused issues, there was no straightforward way to revert to a previous, stable version. Deployments were created to address this limitation.</p>"},{"location":"devops/kube4/#how-deployment-works-and-its-core-functionality","title":"How Deployment Works and its Core Functionality","text":"<p>When you use a Deployment, you define the desired state of your application. The Deployment object then monitors the cluster to ensure that this desired state is maintained.</p> <ol> <li>Controlling ReplicaSets: A Deployment controls ReplicaSets. When you apply changes to your application's code or configuration through a Deployment, it creates a new ReplicaSet for each new version. For example, if you have an application running as Version 1 (V1) under <code>ReplicaSet-V1</code>, and you update your code, the Deployment will create <code>ReplicaSet-V2</code> to manage the new version, while maintaining a record of V1. This process allows Deployment to maintain multiple versions of your application.</li> <li>Pod Management: ReplicaSets are responsible for creating and maintaining the desired number of Pods. If you specify two replicas, the ReplicaSet will ensure two Pods are running at all times. If a Pod fails or is deleted, the ReplicaSet will automatically create a new one to maintain the desired count, a mechanism known as self-healing.</li> <li>Updates and Rollbacks: Deployments provide fine-grained control over how new Pods are rolled out, updated, or rolled back to a previous state. When you update an application through a Deployment, it will gracefully move Pods from the old ReplicaSet to the new one. The old ReplicaSet is typically stopped but kept as a version for potential future rollbacks.</li> </ol> <p>An important concept to remember during rollbacks is that while the code or application version reverts to an older state, the number of Pods remains the same as in the current state. For example, if your current deployment is running on four Pods and you roll back to a version that previously ran on only one Pod, the application on all four current Pods will revert to the old code, but all four Pods will continue to run.</p>"},{"location":"devops/kube4/#key-features-and-use-cases-of-deployments","title":"Key Features and Use Cases of Deployments","text":"<p>Deployments offer several powerful features and use cases:</p> <ul> <li>Rolling Out New Versions: You can declaratively update your Pods and ReplicaSets to roll out a new version of your application.</li> <li>Declaring New States: You can define a new desired state for your Deployment by updating its Pod template specification. This will create a new ReplicaSet and manage the transition of Pods from the old ReplicaSet to the new one.</li> <li>Monitoring Rollout Status: Deployments allow you to check the status of a rollout to see if it succeeded or not.</li> <li>Rolling Back to Previous Revisions: If a new update causes issues, you can easily roll back to any previous version of your application that the Deployment has maintained.<ul> <li>To undo the last rollout, you can use the <code>kubectl rollout undo</code> command.</li> <li>To roll back to a specific revision, you can use <code>kubectl rollout undo deploy &lt;deployment-name&gt; --to-revision=&lt;number&gt;</code>.</li> </ul> </li> <li>Scaling: Deployments facilitate scaling up (increasing the number of Pods) and scaling down (decreasing the number of Pods) to manage workload. For example, you can scale from two Pods to four, or from four Pods down to two. When scaling down, the Pods that were most recently created are typically deleted first.</li> <li>Pausing and Resuming: Deployments can be paused to apply multiple updates to a Pod template specification or to make configuration changes without triggering rollouts. Once changes are complete, the Deployment can be resumed.</li> <li>Cleaning Up Old ReplicaSets: Deployments help in managing and cleaning up old, unneeded ReplicaSets.</li> </ul>"},{"location":"devops/kube4/#practical-examples-and-commands","title":"Practical Examples and Commands","text":"<p>To demonstrate Deployment functionality, let's look at some conceptual YAML structures and command-line instructions.</p>"},{"location":"devops/kube4/#1-deployment-yaml-structure","title":"1. Deployment YAML Structure","text":"<p>A Deployment is defined in a YAML file. The <code>kind</code> field specifies <code>Deployment</code>, and other fields define its behavior, such as the number of replicas, the container image, and commands to run.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment # Specifies the object type as Deployment\nmetadata:\n  name: my-deployment # Name of the Deployment object\nspec:\n  replicas: 2 # Desired number of Pod replicas\n  selector:\n    matchLabels:\n      app: my-deployment # Labels to select Pods managed by this Deployment\n  template:\n    metadata:\n      labels:\n        app: my-deployment # Labels applied to Pods created by this Deployment\n    spec:\n      containers:\n      - name: my-container # Name of the container\n        image: ubuntu # Container image to use, e.g., 'ubuntu' or 'centos'\n        command: [\"/bin/bash\", \"-c\", \"while true; do echo 'Technical Guftgu'; sleep 5; done\"] # Command to run inside the container\n</code></pre>"},{"location":"devops/kube4/#2-deploying-your-application","title":"2. Deploying Your Application","text":"<p>After creating the YAML file (e.g., <code>my-deploy.yaml</code>), you apply it to your Kubernetes cluster:</p> <ul> <li>Create/Edit YAML file: <code>vi my-deploy.yaml</code></li> <li>Apply the Deployment: <code>kubectl apply -f my-deploy.yaml</code><ul> <li>This command will create the Deployment, which in turn creates a ReplicaSet, and then the specified number of Pods.</li> </ul> </li> </ul>"},{"location":"devops/kube4/#3-checking-deployment-status-and-details","title":"3. Checking Deployment Status and Details","text":"<p>You can verify the status and details of your Deployment, ReplicaSets, and Pods using <code>kubectl</code> commands:</p> <ul> <li>Check Deployment status: <code>kubectl get deploy</code><ul> <li>This command shows the Deployment name, how many Pods are <code>READY</code> (e.g., <code>2/2</code> means 2 are ready out of 2 desired), how many are <code>UP-TO-DATE</code>, how many are <code>AVAILABLE</code>, and the <code>AGE</code> of the Deployment.</li> </ul> </li> <li>Describe a Deployment for detailed information: <code>kubectl describe deploy my-deployment</code><ul> <li>This provides extensive details about the Deployment, including its configuration, events, and related ReplicaSets and Pods.</li> </ul> </li> <li>Check ReplicaSet status: <code>kubectl get rs</code><ul> <li>This shows details for ReplicaSets, including <code>DESIRED</code>, <code>CURRENT</code>, and <code>READY</code> Pod counts.</li> </ul> </li> <li>Check Pod status: <code>kubectl get pods</code><ul> <li>This lists the Pods, their <code>STATUS</code>, <code>RESTARTS</code>, and <code>AGE</code>. Pod names created by a Deployment will typically start with the Deployment name followed by a random string.</li> </ul> </li> <li>View logs inside a container: <code>kubectl logs -f &lt;pod-name&gt;</code><ul> <li>This allows you to see the output of the command running inside a specific Pod's container.</li> </ul> </li> <li>Execute a command inside a container (e.g., to check OS release): <code>kubectl exec &lt;pod-name&gt; -- cat /etc/os-release</code></li> </ul>"},{"location":"devops/kube4/#4-scaling-deployments","title":"4. Scaling Deployments","text":"<p>You can easily scale your Deployment up or down:</p> <ul> <li>Scale up: <code>kubectl scale --replicas=4 deploy my-deployment</code> (changes the desired number of Pods to 4)</li> <li>Scale down: <code>kubectl scale --replicas=1 deploy my-deployment</code> (changes the desired number of Pods to 1)</li> </ul>"},{"location":"devops/kube4/#5-updating-and-rolling-back-deployments","title":"5. Updating and Rolling Back Deployments","text":"<p>To update your application, modify the <code>image</code> or <code>command</code> in your <code>my-deploy.yaml</code> file, then reapply it:</p> <ul> <li>Modify <code>my-deploy.yaml</code> (e.g., change <code>image: ubuntu</code> to <code>image: centos</code> or update the <code>command</code> output).</li> <li>Reapply the file: <code>kubectl apply -f my-deploy.yaml</code><ul> <li>This will create a new ReplicaSet for the updated version, and Pods will be transitioned.</li> </ul> </li> <li>Check rollout history: <code>kubectl rollout history deploy my-deployment</code><ul> <li>This command shows the revision history of your Deployment, indicating how many times you've made and applied changes.</li> </ul> </li> <li>Rollback to the previous version: <code>kubectl rollout undo deploy my-deployment</code><ul> <li>This command reverts the Deployment to the immediately preceding version.</li> </ul> </li> <li>Rollback to a specific version: <code>kubectl rollout undo deploy my-deployment --to-revision=2</code> (reverts to revision number 2).</li> </ul>"},{"location":"devops/kube4/#potential-deployment-failure-scenarios-interview-focus","title":"Potential Deployment Failure Scenarios (Interview Focus)","text":"<p>During an interview, you might be asked about reasons for Deployment failures. Here are some common scenarios:</p> <ul> <li>Insufficient Quota: Not enough resources (CPU, memory) are available in the cluster or on the nodes.</li> <li>Unhealthy Nodes: The Kubernetes nodes where Pods are scheduled are not in a healthy state or are down.</li> <li>Image Pull Errors: The specified container image cannot be pulled from the registry. This could be due to incorrect image name/tag, private registry authentication issues, or the image not existing.</li> <li>Insufficient Permissions: The service account or user deploying the application lacks the necessary permissions to create or manage Kubernetes resources.</li> <li>Limit Ranges: Resource limits defined (e.g., CPU or memory) are exceeded, preventing Pods from being scheduled or starting.</li> <li>Application Runtime Misconfiguration: Issues within the application code or its configuration prevent it from starting or running correctly within the container.</li> <li><code>kubectl</code> connection issues: Problems with the <code>kubectl</code> setup (e.g., issues accessing the cluster using <code>putty</code> due to incorrect key format) can prevent commands from executing.</li> </ul> <p>These are the main concepts and practical aspects of Kubernetes Deployments as discussed in the provided sources.</p>"},{"location":"devops/kube5/","title":"Networking","text":"<p>Kubernetes networking is a fundamental and often advanced topic, critical for understanding how applications communicate within a cluster and with the outside world. It addresses several key challenges, including communication between containers within a Pod, communication between different Pods, exposing applications to external users, and enabling services to communicate internally within the cluster. All the practical examples discussed here can be performed using Minikube on an AWS EC2 instance.</p>"},{"location":"devops/kube5/#container-to-container-communication-within-a-pod","title":"Container-to-Container Communication Within a Pod","text":"<p>Within a single Kubernetes Pod, multiple containers can exist. While a container itself does not have its own IP address, the Pod does. If there are two or more containers inside the same Pod, they can communicate with each other using <code>localhost</code>. This is akin to members of the same household not needing a telephone to speak to each other; they are within the same \"house\" (the Pod). For example, if you have two containers, <code>c0</code> and <code>c01</code>, within a single Pod, and <code>c01</code> is running an Apache server, <code>c0</code> can access <code>c01</code> by making a request to <code>localhost</code> on the Apache server's port (e.g., <code>localhost:80</code>).</p> <p>To illustrate this, consider a Pod definition that creates two containers: one running Ubuntu (<code>c0</code>) and another running Apache HTTPD (<code>c01</code>). <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: c0\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n  - name: c01\n    image: httpd # Apache HTTP Server\n    ports:\n    - containerPort: 80 # Exposing port 80 for Apache\n</code></pre> After applying this YAML with <code>kubectl apply -f &lt;filename&gt;.yaml</code>, you can check the Pod status using <code>kubectl get pods</code>. Once the Pod is running, you can execute a command inside the <code>c0</code> container to test communication with <code>c01</code>. You would typically install <code>curl</code> in <code>c0</code> first, then run <code>curl localhost:80</code>. A successful response like \"It works\" (the default Apache message) indicates that <code>c0</code> can communicate with <code>c01</code>.</p>"},{"location":"devops/kube5/#pod-to-pod-communication-within-the-same-node","title":"Pod-to-Pod Communication Within the Same Node","text":"<p>When different Pods need to communicate, even if they are on the same Node, they cannot use <code>localhost</code> because they are in different \"houses\" (different Pods). Instead, each Pod is assigned its own unique IP address. These Pod IPs enable communication between different Pods on the same Node. For instance, if you have <code>Pod1</code> running Nginx and <code>Pod2</code> running Apache on the same Node, <code>Pod1</code> can communicate with <code>Pod2</code> by using <code>Pod2</code>'s IP address and the port it exposes.</p> <p>The YAML for two separate Pods would look something like this: For an Nginx Pod (e.g., <code>pod-nginx.yaml</code>): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre> For an Apache Pod (e.g., <code>pod-apache.yaml</code>): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: apache-pod\nspec:\n  containers:\n  - name: apache-container\n    image: httpd\n    ports:\n    - containerPort: 80\n</code></pre> After applying both YAMLs, you can retrieve their IP addresses using <code>kubectl get pods -o wide</code>. Then, from within one Pod (e.g., <code>nginx-pod</code>), you can execute <code>curl &lt;apache-pod-ip&gt;:80</code> to confirm communication.</p> <p>It is important to remember that by default, these Pod IPs are not accessible from outside the Node. They are meant for internal cluster communication.</p>"},{"location":"devops/kube5/#the-challenge-of-ephemeral-pod-ips-and-the-role-of-services","title":"The Challenge of Ephemeral Pod IPs and the Role of Services","text":"<p>A significant challenge in Kubernetes is the ephemeral nature of Pod IPs. Pods are designed to be short-lived and can be terminated and recreated for various reasons, such as scaling operations, updates, or failures. When a Pod is recreated, it gets a new IP address. This poses a major problem for applications or users trying to access these Pods, as they cannot reliably keep track of constantly changing IP addresses. For example, if a frontend application needs to connect to a backend database Pod, and that database Pod's IP keeps changing, the frontend would lose its connection.</p> <p>To overcome this, Kubernetes introduces the Service object. A Service acts as a logical bridge or a stable abstraction layer in front of a set of Pods. It provides a static Virtual IP (VIP) and a DNS name that remains constant, regardless of whether the underlying Pods are terminated, recreated, or moved to different Nodes. When a client connects to the Service's VIP, the Service then redirects the traffic to the appropriate Pod(s). This redirection is dynamically maintained by a component called Kube-proxy, which continuously monitors the cluster and updates the mapping between the Service's VIP and the current Pod IPs. Kube-proxy queries the Kubernetes API server to learn about new Services and Pods in the cluster.</p> <p>Services use labels and selectors to determine which Pods they should target. You define specific labels on your Pods (e.g., <code>app: my-app</code>), and then the Service's selector is configured to match those labels. This ensures the Service only routes traffic to the intended Pods.</p> <p>There are four main types of Services: 1.  ClusterIP: Exposes the Service on a cluster-internal IP. This type is the default and only accessible from within the cluster. 2.  NodePort: Exposes the Service on a static port (the NodePort) on each Node in the cluster. This makes the Service accessible from outside the cluster using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. 3.  LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. This type is specific to cloud environments. 4.  Headless: Used when you don't need a stable IP but want direct access to Pods. It doesn't allocate a ClusterIP and provides DNS records for each Pod directly.</p> <p>The source indicates a hierarchy where NodePort builds on ClusterIP, and LoadBalancer builds on NodePort.</p>"},{"location":"devops/kube5/#clusterip-service-example","title":"ClusterIP Service Example","text":"<p>A ClusterIP Service provides a virtual IP that is only visible and accessible from within the cluster. It is commonly used for internal communication between different components of a microservice architecture.</p> <p>Here is a conceptual YAML for a Deployment and a ClusterIP Service: First, a Deployment (e.g., <code>deploy-httpd.yaml</code>) to manage Apache Pods: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-apache-app # Label for Pods\n  template:\n    metadata:\n      labels:\n        app: my-apache-app # Labels applied to Pods\n    spec:\n      containers:\n      - name: apache-container\n        image: httpd\n        ports:\n        - containerPort: 80\n</code></pre> Then, a ClusterIP Service (e.g., <code>service-clusterip.yaml</code>) to expose the Deployment: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: demo-service\nspec:\n  selector:\n    app: my-apache-app # Matches the Pods' label\n  ports:\n    - protocol: TCP\n      port: 80 # Service port\n      targetPort: 80 # Container port\n  type: ClusterIP # Explicitly defined as ClusterIP\n</code></pre> After applying these files, <code>kubectl get svc</code> will show the Service and its assigned ClusterIP. You can then use <code>curl &lt;ClusterIP&gt;:80</code> from another Pod within the cluster to access the Apache application. If you then manually delete the Apache Pod (<code>kubectl delete pod &lt;pod-name&gt;</code>), the Deployment will automatically recreate it with a new IP, but the ClusterIP of the Service will remain the same, and you will still be able to access the new Pod using the original ClusterIP. This demonstrates the static nature of the Service's Virtual IP.</p>"},{"location":"devops/kube5/#nodeport-service-example","title":"NodePort Service Example","text":"<p>A NodePort Service enables external access to applications running in Pods. It does this by exposing the Service on a static port (the NodePort, typically in the range 30000-32767) on each Node in the cluster. External users can then access the application using the Node's IP address (or Public DNS) and this specific NodePort. The NodePort will forward the traffic to the Service's ClusterIP, which then routes it to the correct Pod.</p> <p>Using the same Deployment as before, here is a conceptual YAML for a NodePort Service (e.g., <code>service-nodeport.yaml</code>): <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: demo-service\nspec:\n  selector:\n    app: my-apache-app # Matches the Pods' label\n  ports:\n    - protocol: TCP\n      port: 80 # Service port\n      targetPort: 80 # Container port\n      nodePort: 31341 # Optional: specific NodePort, or Kubernetes assigns one\n  type: NodePort # Explicitly defined as NodePort\n</code></pre> After applying this, <code>kubectl get svc</code> will show the NodePort assigned to your Service (e.g., <code>31341:80/TCP</code>). To access the application from outside the cluster (e.g., from your web browser), you would use the Public DNS of your EC2 instance (Node) followed by the NodePort. For instance, <code>http://&lt;EC2-Public-DNS&gt;:31341</code>. It's crucial to ensure that the assigned NodePort is allowed in your cloud provider's security group (e.g., AWS Security Group) for inbound traffic.</p>"},{"location":"devops/kube5/#kubernetes-volumes-for-data-persistence","title":"Kubernetes Volumes for Data Persistence","text":"<p>Containers are designed to be stateless and ephemeral; any data stored directly within a container's filesystem is lost if the container crashes or is terminated. This presents a problem for applications that need to store persistent data. Kubernetes solves this using Volumes. A Volume in Kubernetes is essentially a directory that is accessible to the containers within a Pod. Unlike container-specific storage, a Volume's lifecycle is tied to the Pod, not individual containers.</p> <p>Important Volume behaviors: *   If a container within a Pod crashes or is restarted, the data in the Volume persists and the new or restarted container can access it. *   If the Pod itself is deleted or fails, the Volume associated with it is also deleted, and any data it contained is lost.</p> <p>Volumes can be shared between multiple containers within the same Pod. Kubernetes offers various types of Volumes, including local storage (like <code>emptyDir</code> and <code>hostPath</code>), network file systems (like NFS), and cloud-provider-specific storage solutions (like AWS EBS or Azure Disk). The video focuses on <code>emptyDir</code> and <code>hostPath</code> volumes.</p>"},{"location":"devops/kube5/#emptydir-volume-example","title":"EmptyDir Volume Example","text":"<p>An EmptyDir volume is created when a Pod is assigned to a Node and exists as long as that Pod is running on that Node. It starts empty, hence its name. Its primary purpose is to provide temporary, shared storage for multiple containers within the same Pod. Data in an EmptyDir volume is lost if the Pod is deleted.</p> <p>Consider a Pod definition with two containers (<code>c1</code> and <code>c2</code>) that share an EmptyDir volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-volume-emptydir\nspec:\n  containers:\n  - name: c1\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n    volumeMounts:\n    - name: exchange # Volume name\n      mountPath: /tmp/exchange # Path inside container c1\n  - name: c2\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n    volumeMounts:\n    - name: exchange # Same volume name\n      mountPath: /tmp/data # Path inside container c2\n  volumes:\n  - name: exchange # Defines the EmptyDir volume\n    emptyDir: {}\n</code></pre> After applying this YAML, you can verify data sharing. Execute a command in <code>c1</code> to create a file within its mounted path (<code>/tmp/exchange/</code>). For example, <code>kubectl exec -it my-volume-emptydir -c c1 -- /bin/bash</code> to enter the container, then <code>echo \"Technical Guftgu Zindabad\" &gt; /tmp/exchange/my-file.txt</code>. Then, exit <code>c1</code> and enter <code>c2</code> using <code>kubectl exec -it my-volume-emptydir -c c2 -- /bin/bash</code>. Navigate to <code>c2</code>'s mounted path (<code>/tmp/data/</code>) and list the contents. You should see <code>my-file.txt</code> created by <code>c1</code>. This confirms that both containers are accessing and sharing the same underlying volume.</p>"},{"location":"devops/kube5/#hostpath-volume-example","title":"HostPath Volume Example","text":"<p>A HostPath volume maps a file or directory from the host Node's filesystem into a Pod. This type of volume is useful when you want the data to persist even if the Pod is deleted, as the data resides on the underlying Node's disk. It also allows Pods to access specific files or directories on the host. However, it ties the Pod to a specific Node and is not suitable for multi-node clusters where Pods might move, or for highly available applications unless carefully managed [Outside source - I'm noting this is outside the source, as the source doesn't explicitly mention the limitations of HostPath in multi-node environments, but focuses on its persistence].</p> <p>Here is a conceptual YAML for a Pod using a HostPath volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-volume-hostpath\nspec:\n  containers:\n  - name: hostpath-container\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n    volumeMounts:\n    - name: test-volume\n      mountPath: /tmp/hostpath-mount # Path inside the container\n  volumes:\n  - name: test-volume\n    hostPath:\n      path: /tmp/data # Path on the host Node's filesystem\n      type: DirectoryOrCreate # Ensures the directory exists or creates it\n</code></pre> After applying this YAML, you can execute a command inside the <code>hostpath-container</code> to create a file in its mounted path (<code>/tmp/hostpath-mount/</code>). For instance, <code>kubectl exec -it my-volume-hostpath -c hostpath-container -- /bin/bash</code>, then <code>echo \"I love Technical Guftgu\" &gt; /tmp/hostpath-mount/my-file.txt</code>. After creating the file, exit the container. Now, access the host Node's filesystem directly (e.g., via SSH into the EC2 instance) and navigate to <code>/tmp/data/</code>. You should find <code>my-file.txt</code> there, demonstrating that the data written by the container was persisted to the host machine. Similarly, if you create a file directly on the host in <code>/tmp/data/</code>, the container can also see and access that file.</p>"},{"location":"devops/kube5/#common-kubernetes-commands","title":"Common Kubernetes Commands","text":"<p>Throughout these operations, various <code>kubectl</code> commands are essential for managing and inspecting Kubernetes resources: *   <code>kubectl apply -f &lt;filename.yaml&gt;</code>: Applies a configuration defined in a YAML file. *   <code>kubectl get &lt;resource-type&gt;</code>: Lists resources (e.g., <code>kubectl get pods</code>, <code>kubectl get deploy</code>, <code>kubectl get svc</code>, <code>kubectl get rs</code>). *   <code>kubectl get &lt;resource-type&gt; -o wide</code>: Provides more detailed output, often including IP addresses. *   <code>kubectl describe &lt;resource-type&gt; &lt;resource-name&gt;</code>: Shows extensive details about a specific resource, including its configuration, events, and related objects. *   <code>kubectl logs -f &lt;pod-name&gt;</code>: Streams logs from a container in a Pod. *   <code>kubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- &lt;command&gt;</code>: Executes a command inside a specific container, often used to get an interactive shell. *   <code>kubectl scale --replicas=&lt;count&gt; deploy &lt;deployment-name&gt;</code>: Scales a Deployment up or down by changing the desired number of Pod replicas [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl rollout history deploy &lt;deployment-name&gt;</code>: Shows the revision history of a Deployment [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl rollout undo deploy &lt;deployment-name&gt;</code>: Reverts a Deployment to its previous version [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl rollout undo deploy &lt;deployment-name&gt; --to-revision=&lt;number&gt;</code>: Reverts a Deployment to a specific historical revision [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl delete -f &lt;filename.yaml&gt;</code> or <code>kubectl delete &lt;resource-type&gt; &lt;resource-name&gt;</code>: Deletes a resource.</p>"},{"location":"devops/kube5/#potential-deployment-failure-scenarios-interview-focus","title":"Potential Deployment Failure Scenarios (Interview Focus)","text":"<p>During an interview, understanding why Kubernetes Deployments might fail is crucial. Common reasons include [Previous conversation, not explicit in source, but good for exam/interview]: *   Insufficient Quota: Lack of available CPU or memory resources in the cluster or on the Nodes. *   Unhealthy Nodes: The Kubernetes Nodes are in an unhealthy state or are down, preventing Pod scheduling. *   Image Pull Errors: The specified container image cannot be pulled from the registry due to an incorrect name/tag, authentication issues for private registries, or the image not existing. *   Insufficient Permissions: The user or service account deploying the application lacks the necessary permissions to create or manage Kubernetes resources. *   Limit Ranges: Resource limits defined (e.g., CPU or memory requests/limits) are exceeded, preventing Pods from being scheduled or starting correctly. *   Application Runtime Misconfiguration: Issues within the application code or its configuration prevent it from starting or running correctly inside the container. *   <code>kubectl</code> Connection Issues: Problems with the <code>kubectl</code> client's configuration or network connectivity to the Kubernetes API server.</p>"},{"location":"devops/kube6/","title":"Persistent Volumes & Liveness Probes","text":""},{"location":"devops/kube6/#persistent-volume-and-persistent-volume-claim","title":"Persistent Volume and Persistent Volume Claim","text":"<p>The video introduces the concept of Persistent Volumes (PV) and Persistent Volume Claims (PVC) in Kubernetes as a solution for managing storage persistently, especially in a distributed environment.</p> <p>Understanding the Need for Persistent Storage: Traditionally, in IT setups, organizations use centralized storage systems where multiple workstations can access and save data. This ensures that data is consistently available regardless of which machine is being used. In Kubernetes, a similar need arises because pods, which contain your applications, are ephemeral.</p> <p>Limitations of Previous Storage Concepts: *   EmptyDir: The video revisits the <code>EmptyDir</code> concept, explaining that it creates a temporary storage volume that is shared between containers within the same pod. However, if the pod itself is deleted, the <code>EmptyDir</code> and all its data are also deleted. When a new pod is created, new storage is also created, meaning data from the previous pod is lost. *   HostPath: Another concept discussed is <code>HostPath</code>, where a pod uses a directory directly on the worker node's local file system for storage. The significant problem with <code>HostPath</code> is that if a pod is deleted and then recreated on a different worker node within the cluster (which is a common occurrence in Kubernetes for load balancing and fault tolerance), the data stored on the original worker node's local machine will not be accessible to the new pod. This means data persistence is not guaranteed across different worker nodes.</p> <p>Introducing Persistent Volume (PV): To address the issues of ephemeral storage and data loss across pod recreations or migrations between worker nodes, Kubernetes provides Persistent Volumes (PV). A Persistent Volume is essentially an abstraction of a physical piece of storage that exists independently of a pod or even a specific worker node. It's a common, shared storage that remains available even if pods are deleted or moved. The video illustrates this with an example of an Elastic Block Store (EBS) volume from AWS, which can be connected to multiple worker nodes. If a pod is deleted from one worker node and recreated on another, it can still access the same shared EBS volume, ensuring data consistency. EBS is a block-level storage where data is stored in blocks, and it supports mounting to a single EC2 instance (worker node) at a time.</p> <p>The analogy used in the video explains that an administrator might acquire a large cloud storage (e.g., 100GB from AWS EBS). From this large pool, smaller, independent \"Persistent Volume objects\" are created (e.g., 30GB, 20GB, 50GB). These PVs are then made available for consumption by applications.</p> <p>Persistent Volume Claim (PVC): Pods do not directly access Persistent Volumes. Instead, they make a Persistent Volume Claim (PVC). A PVC is a request for a specific amount of storage with certain access modes. For example, a pod might request 1GB of storage. When a PVC is made, Kubernetes looks for an available Persistent Volume that matches the requested specifications (size, access mode) and then \"binds\" that PV to the PVC. Once bound, the pod can use the storage via its PVC. The video also explains that once the storage is used by a pod and its work is done, the data in the Persistent Volume can be \"reclaimed\" or \"recycled\" for future use by other pods if the original pod is deleted.</p> <p>Example YAML for a Persistent Volume (PV): The video demonstrates creating a PV named <code>my-pv-for-ebs</code> from an AWS EBS volume. This YAML specifies the capacity, access modes, reclaim policy, and the ID of the actual EBS volume. <pre><code># my-persistent-volume.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv-for-ebs # Name of the Persistent Volume\nspec:\n  capacity:\n    storage: 1Gi # Defines the capacity of this PV (1 Gigabyte)\n  accessModes:\n    - ReadWriteOnce # Indicates that this volume can be mounted as read-write by a single node\n  persistentVolumeReclaimPolicy: Recycle # Specifies that the volume can be recycled after its claim is released\n  awsElasticBlockStore:\n    volumeID: &lt;YOUR_EBS_VOLUME_ID&gt; # The actual ID of your AWS EBS volume\n    fsType: ext4 # The file system type on the EBS volume\n</code></pre> This YAML defines a Persistent Volume named <code>my-pv-for-ebs</code> which is a 1GB slice of an existing AWS EBS volume. It's set to be <code>ReadWriteOnce</code>, meaning it can be mounted by only one node at a time in read-write mode, and its reclaim policy is <code>Recycle</code>, allowing the data to be reused.</p> <p>Example YAML for a Persistent Volume Claim (PVC): Next, a PVC is created to request storage from an available PV. <pre><code># my-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-ebs-vol-claim # Name of the Persistent Volume Claim\nspec:\n  accessModes:\n    - ReadWriteOnce # Must match the access mode of the PV it intends to bind to\n  resources:\n    requests:\n      storage: 1Gi # Requests 1 Gigabyte of storage\n</code></pre> This <code>my-ebs-vol-claim</code> PVC requests 1GB of storage with <code>ReadWriteOnce</code> access. Kubernetes will then match this claim to a suitable PV, such as <code>my-pv-for-ebs</code>, and bind them.</p> <p>Example YAML for a Deployment using PVC: Finally, a Deployment that utilizes this PVC is created, showing how a pod accesses the persistent storage. <pre><code># my-nginx-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx-container\n          image: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - name: my-ebs-volume # Name of the volume to mount\n              mountPath: /usr/share/nginx/html # Path inside the container where the volume will be mounted\n      volumes:\n        - name: my-ebs-volume # Definition of the volume\n          persistentVolumeClaim:\n            claimName: my-ebs-vol-claim # Reference to the PVC that provides the storage\n</code></pre> In this Deployment, an Nginx container is defined, and a <code>volumeMount</code> is created to attach the <code>my-ebs-volume</code> to <code>/usr/share/nginx/html</code> inside the container. The <code>my-ebs-volume</code> itself is defined to use the <code>my-ebs-vol-claim</code> PVC. This setup ensures that any data written to <code>/usr/share/nginx/html</code> within the Nginx container is stored on the persistent EBS volume, maintaining data integrity even if the Nginx pod is deleted and recreated on a different worker node.</p>"},{"location":"devops/kube6/#livenessprobe","title":"LivenessProbe","text":"<p>The video then transitions to the concept of LivenessProbe, explaining its crucial role in ensuring the continuous availability and health of applications running within Kubernetes.</p> <p>The Shortcoming of Basic Kubernetes Monitoring: Kubernetes inherently monitors if a pod is running and if the containers within that pod are running. It performs basic checks, such as sending a ping or running a simple test command, and if a response is received, it assumes the pod and container are healthy. However, this basic check does not guarantee that the application inside the container is actually functioning correctly. For instance, a web server might be running, but the specific application it hosts could be frozen, stuck in a loop, or unable to serve requests. In such a scenario, Kubernetes would still consider the container \"running,\" even though the application is effectively dead.</p> <p>Purpose of LivenessProbe: LivenessProbe addresses this limitation by actively checking the health of the application itself, inside the container. It ensures that the application is not just running, but is also responsive and performing its intended function.</p> <p>How LivenessProbe Works: A LivenessProbe is configured to execute a specific command, make an HTTP GET request, or check a TCP socket at predefined intervals. *   Success: If the probe's check (e.g., a command) executes successfully and returns an exit code of 0, the application is considered healthy. *   Failure: If the check returns a non-zero exit code, times out, or fails to connect, it indicates that the application is unhealthy or stuck. *   Action on Failure: If the LivenessProbe fails a specified number of times consecutively (defined by <code>failureThreshold</code>), Kubernetes will terminate the unhealthy container. Once terminated, Kubernetes will then create a new container to replace it, aiming to bring the application back to a healthy state. This mechanism helps maintain the reliability and availability of your services, especially when combined with load balancers, as it ensures that only healthy application instances are serving traffic.</p> <p>Example YAML for LivenessProbe: The video provides a YAML example for a Deployment that includes a LivenessProbe. <pre><code># my-liveness-probe-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-liveness-probe-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n        - name: my-container\n          image: busybox\n          command: [\"/bin/sh\", \"-c\", \"echo 'Hello from LivenessProbe' &gt; /tmp/healthy; sleep 300\"] # Command to run on container startup\n          livenessProbe:\n            exec:\n              command: [\"cat\", \"/tmp/healthy\"] # The command to execute for the probe\n            initialDelaySeconds: 5 # Initial delay before the first probe is performed (in seconds)\n            periodSeconds: 5 # How often the probe is performed (in seconds)\n            timeoutSeconds: 1 # Timeout for the probe command (in seconds)\n            failureThreshold: 3 # Number of consecutive failures before Kubernetes restarts the container\n</code></pre> Explanation of LivenessProbe Parameters: *   The container uses <code>busybox</code> and is configured to create a file <code>/tmp/healthy</code> immediately upon startup, then sleeps for 300 seconds. This setup simulates an application that is initially healthy. *   The <code>livenessProbe</code> is defined using an <code>exec</code> action, meaning it will run a command to check the application's health. The command chosen is <code>cat /tmp/healthy</code>, which attempts to read the content of the <code>healthy</code> file. If the file exists and is readable, <code>cat</code> will return a 0 exit code (success); otherwise, it will return a non-zero exit code (failure). *   <code>initialDelaySeconds: 5</code>: This parameter tells Kubernetes to wait for 5 seconds after the container starts before executing the first LivenessProbe check. This is important to give the application sufficient time to initialize and become ready to serve requests before being probed. *   <code>periodSeconds: 5</code>: After the initial delay, the probe will run the <code>cat /tmp/healthy</code> command every 5 seconds. *   <code>timeoutSeconds: 1</code>: If the <code>cat /tmp/healthy</code> command does not complete and return an output within 1 second, the probe will consider it a failure. *   <code>failureThreshold: 3</code>: This specifies that if the probe fails 3 consecutive times, Kubernetes will consider the container unhealthy, terminate it, and then recreate a new one. This prevents transient issues from immediately causing a container restart.</p>"},{"location":"devops/kube7/","title":"Namespaces & Resource Quotas","text":"<p>This response provides detailed notes from the video, focusing on Kubernetes Namespaces and Resource Quotas, with examples and explanations to aid your study for exams and interviews.</p>"},{"location":"devops/kube7/#kubernetes-namespaces","title":"Kubernetes Namespaces","text":"<p>The video introduces Namespaces as a fundamental concept in Kubernetes for organizing and managing resources within a cluster. It addresses the challenge of managing multiple projects or applications from different companies within a single Kubernetes cluster, which can become complicated if all resources exist in one flat space. The speaker draws an analogy to Git, where initially all work might be done on a <code>master</code> branch, but then branches are created to separate development efforts; Namespaces serve a similar purpose in Kubernetes by providing boundaries or logical separation within a cluster. You can think of a Namespace as a dedicated area or a \"folder\" for a specific project or team, where all related Kubernetes objects like pods, services, and deployments reside.</p> <p>The primary purpose of Namespaces is to make managing multiple projects easier by creating these isolated environments. For example, if a cluster hosts applications for companies A, B, C, and D, putting all their pods and resources in one place would be difficult to manage. By creating a separate Namespace for each project or company, such as a \"Project A\" Namespace or a \"Project B\" Namespace, you establish a clear boundary for their resources. This helps avoid confusion, especially if similar applications are running for different entities. A project manager, for instance, can quickly understand what is running by just looking at the Namespace name, which can be named after the project, team, area, or location.</p> <p>Namespaces also allow for the attachment of authorization and policy rules to specific sections of the cluster. This means you can define which users or teams have access to which Namespace, and what actions they can perform within it. Furthermore, Namespaces are critical for allocating resources effectively, as they allow you to define Resource Quotas (discussed next) for each isolated section. This enables you to give different amounts of resources to different projects based on their needs or the budget allocated to them.</p> <p>By default, when you start a Kubernetes cluster and haven't created any custom Namespaces, all your work and resources are placed in the <code>default</code> Namespace. When you run a command like <code>kubectl get pods</code> without specifying a Namespace, Kubernetes will automatically search for pods within this <code>default</code> Namespace. The video demonstrates this by showing \"No resources found in default namespace\" when no pods are present and no custom Namespace is specified.</p> <p>To view all existing Namespaces in your cluster, you can use the command <code>kubectl get namespaces</code>. Typically, you'll see <code>default</code>, <code>kube-system</code> (used by Kubernetes itself), <code>kube-public</code>, and <code>kube-node-lease</code>.</p> <p>Creating and Managing Namespaces:</p> <p>The video shows how to create your own Namespace using a simple YAML file. For instance, to create a Namespace named <code>dev</code>: <pre><code># namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev # The name of your Namespace\n</code></pre> Once the YAML file is created, you apply it using <code>kubectl apply -f namespace.yaml</code>. After application, running <code>kubectl get namespaces</code> will show your newly created <code>dev</code> Namespace as active.</p> <p>When creating resources like pods within a specific Namespace, you must explicitly mention the Namespace. For example, to create a pod from <code>pod.yaml</code> in the <code>dev</code> Namespace: <code>kubectl apply -f pod.yaml -n dev</code>. If you then try to list pods using <code>kubectl get pods</code> (which defaults to the <code>default</code> Namespace), you will see \"No resources found in default namespace\" because your pod was created in <code>dev</code>. To see pods in your custom Namespace, you must specify it: <code>kubectl get pods -n dev</code>.</p> <p>Similarly, when deleting resources from a specific Namespace, you must include the <code>-n</code> flag: <code>kubectl delete -f pod.yaml -n dev</code>. If you attempt to delete without specifying the Namespace, Kubernetes will look in <code>default</code> and report that the resource is not found there.</p> <p>For convenience, you can change your current Kubernetes context to default to a specific Namespace, so you don't have to use <code>-n</code> every time. This is done with the command: <code>kubectl config set-context --current --namespace=dev</code>. After running this, subsequent <code>kubectl get pods</code> commands will automatically check the <code>dev</code> Namespace. To verify which Namespace your current context is set to, you can use <code>kubectl config view --minify | grep namespace</code>.</p> <p>It's important to remember that most Kubernetes resources like pods, services, and deployments are Namespace-scoped. However, some lower-level or cluster-wide resources, such as Nodes and Persistent Volumes, are not Namespace-scoped. This means Persistent Volumes and Nodes operate at the cluster level and are not tied to any specific Namespace, which is a crucial point for understanding Kubernetes architecture and potentially for interview questions.</p> <p>Namespaces are particularly beneficial for environments with many users or multiple teams sharing a large Kubernetes cluster. For smaller setups with only a few applications and users, Namespaces might not be strictly necessary.</p>"},{"location":"devops/kube7/#resource-quota","title":"Resource Quota","text":"<p>Following the discussion of Namespaces, the video delves into Resource Quota, a mechanism for managing resource consumption within a Namespace. Without Resource Quotas, pods can potentially consume all available CPU and memory on a cluster, leading to resource starvation for other applications.</p> <p>The Need for Resource Quota:</p> <p>By default, containers running on Kubernetes operate without explicit CPU or memory limits. This means an application can scale its resource usage indefinitely if not restrained. While this might seem beneficial for a single application, in a shared cluster, it can lead to one application monopolizing resources, thereby impacting the performance or even availability of other applications. The analogy used here is that of a buffet versus a fixed-menu restaurant: without limits, a container might consume as much as it wants from an available pool, but with limits, it can only take what's allocated.</p> <p>Purpose of Resource Quota:</p> <p>Resource Quotas allow you to define and enforce limits on the total amount of compute resources (like CPU and memory) or object count (like number of pods or services) that a Namespace can consume. This ensures fair resource allocation among different teams or projects sharing a cluster, preventing any single Namespace from overwhelming the system.</p> <p>Key Concepts: Request and Limit</p> <p>Within the context of Resource Quotas, two critical parameters for defining resource allocation for individual containers are:</p> <ol> <li> <p>Request: This is the minimum guaranteed amount of resources (CPU and memory) that a container needs to function. Kubernetes guarantees that this amount will always be available to the container. The Kubernetes scheduler uses the <code>request</code> value to decide which worker node to place a pod on; it will only place a pod on a node that has enough available resources to satisfy its <code>request</code>.</p> <ul> <li>CPU <code>request</code>: Specified in units of cores or millicores (m). One CPU core is equivalent to 1000 millicores. So, 0.5 CPU is 500m.</li> <li>Memory <code>request</code>: Specified in units of bytes, such as megabytes (Mi) or gigabytes (Gi).</li> </ul> </li> <li> <p>Limit: This is the maximum amount of resources (CPU and memory) that a container is allowed to consume. If a container attempts to exceed its CPU limit, it will be throttled. If it exceeds its memory limit, it will be terminated by Kubernetes.</p> <ul> <li>The <code>limit</code> for CPU and memory must always be greater than or equal to its corresponding <code>request</code>. You cannot request more than your limit.</li> </ul> </li> </ol> <p>Example YAML for defining Requests and Limits within a Pod:</p> <p>Here's an example from the video of a pod definition that includes <code>requests</code> and <code>limits</code> for its container: <pre><code># pod-resources.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-resource-pod\nspec:\n  containers:\n  - name: my-container\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Hello'; sleep 3600\"]\n    resources:\n      requests:\n        memory: \"64Mi\" # Request 64 MiB of memory\n        cpu: \"100m\"   # Request 100 millicores of CPU\n      limits:\n        memory: \"128Mi\" # Limit to 128 MiB of memory\n        cpu: \"200m\"   # Limit to 200 millicores of CPU\n</code></pre> This YAML specifies that the <code>my-container</code> needs at least <code>64Mi</code> memory and <code>100m</code> CPU, but will not be allowed to use more than <code>128Mi</code> memory or <code>200m</code> CPU. After creating this pod using <code>kubectl apply -f pod-resources.yaml -n dev</code> (assuming you are in the <code>dev</code> Namespace), you can inspect its resource allocation with <code>kubectl describe pod my-resource-pod</code> to see the <code>Requests</code> and <code>Limits</code> section.</p> <p>Relationship Between ResourceQuota and Container Resources:</p> <p>A ResourceQuota object is typically applied to a Namespace to set aggregate limits on CPU, memory, and storage that all pods within that Namespace can consume collectively.</p> <p>Example YAML for a Resource Quota: <pre><code># my-quota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: my-quota\nspec:\n  hard:\n    cpu: \"400m\"  # Total CPU limit for the Namespace\n    memory: \"40Mi\" # Total memory limit for the Namespace\n</code></pre> This Resource Quota limits the <code>dev</code> Namespace to a total of 400 millicores of CPU and 40 megabytes of memory.</p> <p>A crucial point demonstrated in the video is that the total sum of <code>requests</code> and <code>limits</code> of all containers within a Namespace must not exceed the <code>hard</code> limits defined in the Resource Quota for that Namespace. The video illustrates a scenario where a Resource Quota was set for <code>400m</code> CPU, but a deployment attempted to create three replicas, each requesting <code>200m</code> CPU. This would total <code>600m</code> CPU, which exceeds the Namespace's <code>400m</code> quota. In such a case, the deployment will fail, and inspecting it with <code>kubectl describe deployment &lt;deployment-name&gt;</code> will show <code>FailedQuota</code> errors, indicating that the requested resources exceed the available quota.</p> <p>Defaulting Behavior of Request and Limit:</p> <p>The video highlights important defaulting behaviors when <code>request</code> and <code>limit</code> are not explicitly defined:</p> <ol> <li> <p>If <code>request</code> is NOT defined, but <code>limit</code> IS defined: In this scenario, the <code>request</code> for the container will automatically be set equal to its <code>limit</code>. This means if you only specify <code>limits: {cpu: \"100m\"}</code>, the container will implicitly have <code>requests: {cpu: \"100m\"}</code> as well. The video demonstrates this by creating a <code>LimitRange</code> with a 1 CPU limit and no request, then a pod gets 1 CPU for both request and limit.</p> </li> <li> <p>If <code>request</code> IS defined, but <code>limit</code> is NOT defined: In this case, the <code>limit</code> for the container will NOT be set equal to its <code>request</code>. Instead, it will be set to the default limit defined for the cluster or Namespace, if one exists. If no such default exists (e.g., in a <code>LimitRange</code> object), the limit might effectively be considered unlimited (0), meaning it can use as much as available. The video demonstrates this by defining a pod with a <code>750m</code> CPU request but no limit. When described, the pod's limit appears as <code>1 CPU</code> (which is <code>1000m</code>), indicating it picked up the default limit, not the request value.</p> </li> <li> <p>If NEITHER <code>request</code> nor <code>limit</code> are defined: By default, containers will have no CPU or memory limits, and can consume resources as needed. The video also explains that in this case, <code>request</code> effectively becomes equal to <code>limit</code>, both being unlimited (or constrained only by the cluster's overall capacity).</p> </li> </ol> <p>These interactions are crucial for understanding how resource allocation behaves in Kubernetes, especially for interview scenarios.</p> <p>The video concludes by reiterating the importance of Namespaces and Resource Quotas for managing complex Kubernetes environments, ensuring proper resource allocation, and preventing resource contention among different projects or teams. It encourages hands-on practice through the provided lab steps to solidify understanding.</p>"},{"location":"devops/kube8/","title":"Horizontal Pod Autoscaling","text":"<p>Kubernetes offers powerful capabilities for managing resources and automatically scaling applications to meet demand. This detailed overview will cover Resource Quota for CPU and Memory, and Horizontal Pod Autoscaling (HPA), complete with practical considerations for exams and interviews.</p>"},{"location":"devops/kube8/#kubernetes-resource-quota-cpu-and-memory-management","title":"Kubernetes Resource Quota: CPU and Memory Management","text":"<p>Resource Quotas in Kubernetes allow you to define the minimum and maximum resource allocations for containers within a pod. This is crucial for efficient resource utilization and preventing a single container from consuming excessive resources.</p> <p>Default Resource Ranges for a Container: When defining resource limits for a container, Kubernetes has default ranges that must be adhered to. These ranges specify the minimum and maximum values for CPU and memory that a container can request or be limited to. *   CPU: A container's minimum CPU request cannot be less than 0.5 CPU, and its maximum CPU limit cannot exceed 1 CPU. *   Memory: A container's minimum memory request cannot be less than 500MB, and its maximum memory limit cannot exceed 1GB.</p> <p>Request and Limit Behavior: Understanding the interaction between <code>requests</code> and <code>limits</code> is vital. *   When <code>limit</code> is mentioned but <code>request</code> is not: If you specify a <code>limit</code> for CPU, for instance, 500 milliCPU, but do not mention the <code>request</code>, then the <code>request</code> will automatically be set equal to the <code>limit</code>. This means the container will be allocated 500 milliCPU as its request.     *   Example Scenario: <pre><code># Example part of a Pod YAML definition\nresources:\n  limits:\n    cpu: \"500m\" # Limit is 500 milliCPU\n  # requests: # Request is not mentioned\n</code></pre>         In this case, the effective request will also be 500 milliCPU. *   When <code>request</code> is mentioned but <code>limit</code> is not: If you specify a <code>request</code> for CPU, for example, 600 milliCPU, but do not mention the <code>limit</code>, then the <code>limit</code> will not automatically equal the <code>request</code>. Instead, the <code>limit</code> will default to its maximum allowed value, which for CPU is 1 CPU.     *   Example Scenario: <pre><code># Example part of a Pod YAML definition\nresources:\n  requests:\n    cpu: \"600m\" # Request is 600 milliCPU\n  # limits: # Limit is not mentioned\n</code></pre>         In this case, the effective limit will be 1 CPU. *   When only one resource (e.g., Memory) is mentioned: If you define <code>request</code> and <code>limit</code> only for memory, for instance, and do not mention CPU resources, then CPU will automatically pick up its default values. This means the CPU request will be 0.5 CPU and the CPU limit will be 1 CPU.</p> <p>Memory Resource Quota Examples and Error Scenarios: When defining memory resources, it's critical to stay within the default ranges. *   Setting a Limit Range (e.g., using a <code>LimitRange</code> object):     You can define a <code>LimitRange</code> object to set default minimum and maximum memory for containers within a namespace.     *   Example YAML structure for <code>LimitRange</code>: <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-default-limit-range\nspec:\n  limits:\n  - default:\n      memory: 1Gi # Default limit for memory\n    defaultRequest:\n      memory: 500Mi # Default request for memory\n    type: Container\n</code></pre>         This example sets a maximum memory of 1GB and a minimum request of 500MB for any container within the specified namespace. *   Valid Memory Allocation: If you set a <code>request</code> of 600MB and a <code>limit</code> of 800MB for a container, this is valid because 600MB is above the 500MB minimum request and 800MB is below the 1GB maximum limit.     *   Example YAML part: <pre><code>resources:\n  requests:\n    memory: \"600Mi\"\n  limits:\n    memory: \"800Mi\"\n</code></pre>         When applied, this pod will be created successfully, and <code>kubectl describe pod &lt;pod-name&gt;</code> will show both the requested 600MB and the limited 800MB. *   Error Scenario: Limit Exceeds Maximum: If you try to set a memory limit greater than 1GB (e.g., 1200MB or 1800MB), Kubernetes will throw an error. The error message will explicitly state that the maximum memory for your container is 1GB.     *   Example YAML part leading to error: <pre><code>resources:\n  limits:\n    memory: \"1200Mi\" # This will cause an error\n</code></pre>         The command <code>kubectl apply -f mem-too-high-limit.yaml</code> would result in an error similar to: \"The maximum memory for your container is 1GB and you have increased the limit to 1200MB, which is not possible\". *   Error Scenario: Request Below Minimum: If you attempt to set a memory request less than 500MB (e.g., 300MB), it will also result in an error. The error message will indicate that the minimum memory usage per container is 500MB.     *   Example YAML part leading to error: <pre><code>resources:\n  requests:\n    memory: \"300Mi\" # This will cause an error\n</code></pre>         The command <code>kubectl apply -f mem-too-low-request.yaml</code> would result in an error similar to: \"The minimum memory usage per container is 500MB, but you have requested 300MB which is not possible\".</p> <p>These resource quota configurations ensure that containers operate within defined boundaries, preventing resource hogging and promoting cluster stability. It is important to note that these default limits apply to containers within a specific namespace when you explicitly define it, not necessarily to the default namespace.</p>"},{"location":"devops/kube8/#horizontal-pod-autoscaling-hpa","title":"Horizontal Pod Autoscaling (HPA)","text":"<p>Horizontal Pod Autoscaling (HPA) is a Kubernetes feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. It's a critical component for achieving automation in modern DevOps environments, as manual scaling is inefficient and impractical for fluctuating loads.</p> <p>Concept and Need for HPA: Imagine an application like Hotstar during a major cricket match, experiencing over 1.2 crore (12 million) viewers. Without auto-scaling, handling such sudden and massive increases in user traffic would require manual intervention, leading to potential service disruptions. HPA addresses this by allowing your application to automatically create new pods (horizontal scaling) when the load increases, and reduce pods when the load decreases.</p> <p>Core Components of HPA: HPA relies on several key components to function effectively: 1.  Scalable Object: HPA scales objects like Deployments, ReplicaSets, or Replication Controllers. It cannot scale every Kubernetes object, such as Services. 2.  Horizontal Pod Autoscaler (HPA) Object: This is a dedicated Kubernetes API resource and a controller that defines the auto-scaling behavior. It monitors the metrics and makes decisions to scale up or down. 3.  Metric Server: This is a crucial add-on that needs to be installed in your cluster. Its primary role is to collect resource metrics (like CPU utilization, memory utilization, storage utilization) from all running pods. The HPA controller constantly queries the Metric Server for this data.</p> <p>How HPA Makes Scaling Decisions: *   Metric Collection: The Metric Server continuously collects data on CPU utilization (and other configured metrics) for all running pods. *   Average Utilization: HPA evaluates the average CPU utilization across all pods of a given scalable object (e.g., a Deployment). For example, if you have two containers, one with 40% load and another with 20% load, the average would be (40+20)/2 = 30%. *   Target Comparison: The HPA configuration includes a target CPU utilization percentage (e.g., 20% or 30%). If the calculated average utilization exceeds this target, HPA will trigger a scale-up event. *   Pod Creation/Deletion: Based on the average utilization relative to the target, HPA calculates how many new pods are needed to bring the average utilization down to the target. It then instructs the Deployment or ReplicaSet to create (or delete) pods accordingly. For instance, if one pod has 80% CPU utilization and the target is 30%, HPA might create two more pods, so the 80% load can be distributed, aiming for an average closer to the 30% target.</p> <p>Scaling Parameters: When configuring HPA, you define key parameters: *   Target CPU Percentage: The desired average CPU utilization for your pods (e.g., <code>20%</code> or <code>30%</code>). *   Minimum Replicas: The minimum number of pods that should always be running, even if the load is very low (e.g., <code>1</code> pod). HPA will not scale down below this number. *   Maximum Replicas: The maximum number of pods HPA can create (e.g., <code>10</code> pods). This prevents uncontrolled scaling and resource exhaustion.</p> <p>Downscaling Behavior and Cooling Period: While HPA scales up quickly when load increases, it has a more cautious approach to scaling down. *   Cooling Period: To prevent rapid, unnecessary scaling fluctuations (known as \"thrashing\"), HPA has a default cooling period of 5 minutes before it will delete pods. This means that even if the load drops significantly, HPA will wait for 5 minutes to confirm that the low load persists before downscaling. If the load increases again within this 5-minute window, it avoids deleting pods unnecessarily. *   Check Frequency: The HPA controller checks the resource metrics from the Metric Server and re-evaluates scaling decisions every 30 seconds.</p> <p>Horizontal vs. Vertical Autoscaling: It is important to distinguish between these two types of scaling: *   Horizontal Scaling: Involves creating new pods to distribute the load across more instances. This is like adding more rooms to a house when more family members arrive. *   Vertical Scaling: Involves increasing the capacity of existing pods (e.g., allocating more CPU or memory to an already running pod). This is like expanding an existing room to accommodate more people. The video uses an analogy of increasing the capacity of an existing 50GB storage to 100GB (vertical) versus adding a completely new 50GB storage (horizontal). HPA focuses on horizontal scaling.</p>"},{"location":"devops/kube8/#practical-implementation-steps-for-hpa","title":"Practical Implementation Steps for HPA","text":"<p>To demonstrate HPA, you need to set up the Metric Server, create a sample application Deployment, and then configure the HPA object.</p> <ol> <li> <p>Setting up the Metric Server:     The Metric Server is not installed by default in Kubernetes and is essential for HPA to work.</p> <ul> <li>Download the Metric Server YAML: You typically download it from its GitHub repository, which is an open-source project.     Command: <code>wget https://raw.githubusercontent.com/kubernetes-sigs/metrics-server/master/deploy/1.8%2B/metrics-server-deployment.yaml</code> (Note: The exact URL might vary slightly depending on the Kubernetes version and Metric Server release. The source indicates a GitHub link for download).</li> <li>Modify the Metric Server YAML: After downloading, you need to edit the <code>metrics-server-deployment.yaml</code> file. Inside the <code>Deployment</code> section, find the <code>args</code> (arguments) for the <code>metrics-server</code> container. You must add the argument <code>--kubelet-insecure-tls</code> to bypass certificate validation, as you typically won't have self-signed certificates set up in a basic lab environment.<ul> <li>YAML modification example (inside <code>containers[].args</code> for <code>metrics-server</code>):     <pre><code># ... (other parts of metrics-server-deployment.yaml)\ncontainers:\n- name: metrics-server\n  image: k8s.gcr.io/metrics-server/metrics-server:v0.5.0 # Example image\n  args:\n    - --cert-dir=/tmp\n    - --secure-port=4443\n    - --kubelet-insecure-tls # Add this line\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n    - --kubelet-use-node-status-port\n# ...\n</code></pre></li> </ul> </li> <li>Apply the Metric Server YAML: After modification, apply the YAML file to deploy the Metric Server.     Command: <code>kubectl apply -f metrics-server-deployment.yaml</code>.</li> <li>Verify Metric Server: You can check the pods in the <code>kube-system</code> namespace to ensure the Metric Server pod is running. You can also view its logs to confirm it's collecting metrics.     Commands:<ul> <li><code>kubectl get pods -n kube-system</code></li> <li><code>kubectl logs -f &lt;metrics-server-pod-name&gt; -n kube-system</code> (Look for \"Generated self-signed certificate\" to confirm proper setup).</li> </ul> </li> </ul> </li> <li> <p>Creating a Sample Deployment:     Next, create a sample application deployment that HPA will manage. This deployment will typically include resource requests and limits.</p> <ul> <li>Deployment YAML structure example:     <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deploy # Name of your deployment\nspec:\n  replicas: 1 # Start with 1 replica for demonstration\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-container # Name of your container\n        image: httpd:latest # Example image (Apache server)\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: \"200m\" # Request 200 milliCPU\n          limits:\n            cpu: \"500m\" # Limit 500 milliCPU\n</code></pre>     This YAML defines a deployment named <code>my-deploy</code> with an Apache server running in a container, requesting 200m CPU and limited to 500m CPU. It starts with one replica.</li> <li>Apply the Deployment YAML:     Command: <code>kubectl apply -f my-deployment.yaml</code>.</li> <li>Verify Deployment and Pod:     Command: <code>kubectl get all</code> (You should see your deployment, replica set, and one running pod).</li> </ul> </li> <li> <p>Configuring the HPA Object:     Now, create the HPA object that will monitor and scale your deployment.</p> <ul> <li>HPA creation command:     Command: <code>kubectl autoscale deployment my-deploy --cpu-percent=20 --min=1 --max=10</code>.<ul> <li><code>deployment my-deploy</code>: Specifies that HPA should scale the <code>my-deploy</code> deployment.</li> <li><code>--cpu-percent=20</code>: Sets the target average CPU utilization to 20%.</li> <li><code>--min=1</code>: Sets the minimum number of pods to 1.</li> <li><code>--max=10</code>: Sets the maximum number of pods to 10.</li> </ul> </li> <li>Verify HPA: After running the command, you can check the HPA status.     Command: <code>kubectl get hpa</code> (You will see the HPA named <code>my-deploy</code>, target CPU, min/max pods, and current replicas). Initially, the target CPU might show <code>0%</code> or a low value if no load is present.</li> </ul> </li> <li> <p>Demonstrating Autoscaling:     To see HPA in action, you need to simulate a load increase on your application.</p> <ul> <li>Open Two Terminals: Keep one terminal for monitoring and another for generating load.</li> <li>Monitoring Terminal: In the first terminal, continuously monitor your pods and HPA.     Command: <code>watch kubectl get all</code> (This command will refresh every 2 seconds, showing changes in pod counts and HPA metrics). You will observe the <code>TARGET</code> column for your HPA.</li> <li>Load Generation Terminal: In the second terminal, get a shell into your running application pod.     Command: <code>kubectl exec -it &lt;my-deploy-pod-name&gt; -- /bin/bash</code>.</li> <li>Generate Load: Inside the pod's shell, run a command that consumes CPU, such as an <code>apt update</code> or a simple infinite loop.     Command (example for CPU load): <code>while true; do true; done</code> (This command will make the CPU usage of the container spike). Alternatively, if your image has <code>apt</code>, you can run <code>apt update</code> repeatedly.</li> <li>Observe Scaling Up: As you run the load generation command, watch the monitoring terminal. You will see the <code>TARGET</code> CPU percentage of your HPA increase from <code>0%</code> (or its initial low value). Once it consistently exceeds <code>20%</code>, you will observe new pods being created automatically. The <code>REPLICAS</code> count in <code>kubectl get all</code> will increase, and new pod entries will appear as <code>Running</code>.</li> <li>Observe Scaling Down: Once you stop the load generation (e.g., by pressing <code>Ctrl+C</code> in the load generation terminal), the CPU utilization will drop. Due to the 5-minute cooling period, the extra pods will not be deleted immediately. After approximately 5 minutes, if the load remains low, HPA will automatically delete the excess pods, returning the replica count closer to the minimum defined value (e.g., 1).</li> </ul> </li> </ol> <p>This practical demonstration highlights how Kubernetes, through HPA and Metric Server, can automatically manage application scaling based on real-time resource utilization, making your applications resilient and efficient in handling varying loads.</p>"},{"location":"devops/kubernetes_install/","title":"Kubernetes Cluster Installation Guide","text":""},{"location":"devops/kubernetes_install/#sanity-setup-and-pre-requisites","title":"Sanity Setup and Pre-requisites","text":"<p>Perform Sanity Checks on All Hosts</p> <pre><code># 1. Disable SELinux on all hosts.\n# 2. Docker user should have root access.\n# 3. Add host entries.\n# 4. Disable swap on all hosts.\n# 5. Enable passwordless SSH from docker user and root.\n</code></pre>"},{"location":"devops/kubernetes_install/#master-node-setup","title":"Master Node Setup","text":"<p>SSH into the Master Node</p> <pre><code>ssh user@master-node\n</code></pre> <p>Disable Swap</p> <pre><code>swapoff -a\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n</code></pre> <p>Configure Networking for Kubernetes</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\nsudo sysctl --system\nlsmod | grep br_netfilter\nlsmod | grep overlay\nsysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward\nsysctl -p\n</code></pre> <p>Install Container Runtime (containerd)</p> RPM-based InstallationTar-based Installation <pre><code>yum install -y containerd-*.rpm\n</code></pre> <pre><code>curl -LO https://github.com/containerd/containerd/releases/download/v1.7.14/containerd-1.7.14-linux-amd64.tar.gz\nsudo tar Cxzvf /usr/local containerd-1.7.14-linux-amd64.tar.gz\ncurl -LO https://raw.githubusercontent.com/containerd/containerd/main/containerd.service\nsudo mkdir -p /usr/local/lib/systemd/system/\nsudo mv containerd.service /usr/local/lib/systemd/system/\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n</code></pre> <p>Enable and Start containerd</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now containerd\nsystemctl status containerd\n</code></pre> <p>Install Runc</p> RPM-based InstallationTar-based Installation <pre><code>yum install -y runc\n</code></pre> <pre><code>curl -LO https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\nsudo install -m 755 runc.amd64 /usr/local/sbin/runc\n</code></pre> <p>Install CNI Plugin</p> <pre><code>curl -LO https://github.com/containernetworking/plugins/releases/download/v1.5.0/cni-plugins-linux-amd64-v1.5.0.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.0.tgz\n</code></pre> <p>Install Kubernetes Components</p> <pre><code># Download rpm from official kubernetes documentation\nyum install -y kube*.rpm\nkubeadm version\nkubelet --version\nkubectl version --client\n</code></pre> <p>Configure crictl for Containerd</p> <pre><code>sudo crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock\n</code></pre> <p>Initialize Kubernetes Control Plane</p> <pre><code># Load necessary Kubernetes images before initializing\nkubeadm config images list\n# Example images:\n# registry.k8s.io/kube-apiserver:v1.30.1\n# registry.k8s.io/kube-controller-manager:v1.30.1\n# registry.k8s.io/kube-scheduler:v1.30.1\n# registry.k8s.io/kube-proxy:v1.30.1\n# registry.k8s.io/coredns/coredns:v1.11.1\n# registry.k8s.io/pause:3.9\n# registry.k8s.io/etcd:3.5.12-0\n\nkubeadm init --kubernetes-version=v1.30.1 \\\n  --control-plane-endpoint \"hostIP:6443\" \\\n  --upload-certs \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --apiserver-advertise-address=hostIP\n</code></pre> <p>Set Up kubeconfig for kubectl</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Install Calico for Networking</p> <pre><code>wget https://github.com/manish-chet/DataEngineering/blob/main/kubernetes/calico_edited.yaml\nkubectl apply -f calico_edited.yaml\n</code></pre>"},{"location":"devops/kubernetes_install/#worker-node-setup","title":"Worker Node Setup","text":"<p>Repeat Master node Setup Steps and Join Cluster</p> <p><pre><code># Repeat the above steps on all worker nodes, then join them to the cluster:\nsudo kubeadm join hostIP:6443 --token xxxxx --discovery-token-ca-cert-hash sha256:xxx\n</code></pre> If you need the join command again, run the following on the master node: <pre><code>kubeadm token create --print-join-command\n</code></pre></p>"},{"location":"devops/kubernetes_install/#validation","title":"Validation","text":"<p>Validate Cluster Status</p> <pre><code>kubectl get nodes\nkubectl get pods -A\n</code></pre>"},{"location":"devops/kubernetes_install/#references","title":"References","text":"<ul> <li>Official Kubernetes Documentation</li> <li>Mirantis Kubernetes Guide</li> <li>Containerd Setup Guide</li> <li>Kubernetes Networking</li> <li>Kubernetes Dashboard</li> </ul>"},{"location":"devops/kubernetesarchitecture/","title":"Kubernetes Architecture Explained","text":"<p>The architecture diagram depicted in the image shows two basic components: Master and Worker (also referred to as Node). These can be considered nodes. In the example discussed, the diagram shows a master and two nodes, indicating the use of two EC2 instances for the lab. A node signifies a server that manages containers or pods. The master is the component that controls these nodes. This structure follows a client-server model, similar to systems like Chef. For a lab setup, three EC2 instances would be created, and one would be designated as the master.</p> <p>The diagram also illustrates how users interact with the cluster. Users communicate with the cluster through the API Server. Users can create manifests, which are configuration files written in YAML or JSON format, similar to recipes in Chef. These manifests describe the desired state, such as creating a pod with a specific number of containers. The API Server reads the manifest, understands the desired state, and then communicates with the Controller Manager to initiate the process.</p>"},{"location":"devops/kubernetesarchitecture/#components-of-control-plane","title":"Components of Control Plane","text":""},{"location":"devops/kubernetesarchitecture/#1-kube-api-server","title":"1. Kube API Server","text":"<p>This is described as the most important component, acting as the point of contact for all communication. All other components communicate through the API Server. User requests and requests from nodes always come to the API Server. It acts like a receptionist in a bank, receiving requests and forwarding them to the appropriate components; it doesn't solve the requests itself. Nodes communicate with the API Server, not directly with other master components. The Kubelet component on the nodes communicates with the API Server. It is meant to scale automatically as per load. It is the front end of the control plane.</p>"},{"location":"devops/kubernetesarchitecture/#2-etcd","title":"2. ETCD","text":"<p>This is described as a database or storage, similar to Chef's data bag, which maintains the current state. It stores information about the cluster's state, including the number of containers in a pod, pod IP addresses, and other details. ETCD is an external component, not a fundamental part of Kubernetes itself, but Kubernetes cannot work without it. It acts as a ledger of all activities. Importantly, only the API Server can access ETCD directly; no other component like the Controller Manager or Scheduler can. It stores data as key-value pairs.</p> <p>ETCD has the following features:</p> <ul> <li>Fully replicated \u2013 entire state is available on every node in the cluster</li> <li>Secure \u2013 implements automatic TLS with optional client certificate authentication</li> <li>Fast \u2013 benchmarked at 10,000 writes per second</li> </ul>"},{"location":"devops/kubernetesarchitecture/#3-kube-scheduler","title":"3. Kube Scheduler","text":"<p>This component performs the actions to make the actual state equal to the desired state. If a pod should have four containers but only has three, the Controller Manager notes the mismatch and tells the Scheduler to create the additional container. The Scheduler is the one that actually performs the work, like creating pods. It decides on which node to create a pod based on factors like available resources.</p>"},{"location":"devops/kubernetesarchitecture/#4-controller-manager","title":"4. Controller Manager","text":"<p>This component is responsible for maintaining the balance between the actual state and the desired state of the cluster. It ensures that the requested number of containers for a pod are available. Using a bank analogy, it's like the person who verifies a withdrawal request against the account balance before allowing the cashiers to dispense money. It guarantees that what was requested (desired state) matches what is actually running (actual state).</p> <p>Components on master that run controller: - Node Controller: For checking the cloud provider to determine if a node has been detected in the cloud after it stops responding - Route Controller: Responsible for setting up network routes on your cloud - Service Controller: Responsible for load balancer on your cloud against service of type <code>LoadBalancer</code> - Volume Controller: For creating, attaching, and mounting volumes and interacting with cloud provider to orchestrate volumes</p>"},{"location":"devops/kubernetesarchitecture/#components-of-worker-plane","title":"Components of Worker Plane","text":"<p>The Worker Nodes are depicted separately from the Master. They are also referred to as workers or minions. A cluster can have one master and one node, one master and multiple nodes, or even multiple masters and multiple nodes.</p> <p>Each Worker Node contains three basic components:</p>"},{"location":"devops/kubernetesarchitecture/#1-kubelet","title":"1. Kubelet","text":"<p>This is an agent that runs on each node. It communicates with the API Server and reports the state of the node. It receives requests from the API Server and is responsible for managing pods on the node. Its general task is to create and manage multiple pods. It reports the success or failure status back to the master. Uses port 10255.</p>"},{"location":"devops/kubernetesarchitecture/#2-kube-proxy","title":"2. Kube-Proxy","text":"<p>This component handles the networking for the pods on the node. Its basic job is to assign IP addresses to pods. It runs on each node and ensures that each pod gets its own unique IP address.</p>"},{"location":"devops/kubernetesarchitecture/#3-container-engine","title":"3. Container Engine","text":"<p>This is the software that runs the containers. It is recommended to refer to it as a \"Container Engine\" rather than specifically \"Docker,\" as Kubernetes can work with various container engines like Docker, Rkt, or Containerd. The Container Engine is not a part of Kubernetes itself; it needs to be installed on each node. It works with the Kubelet to pull images, create and run containers, and expose containers on specified ports.</p>"},{"location":"devops/kubernetesarchitecture/#pod","title":"Pod","text":"<p>The diagram also features Pods as the basic unit within the nodes. A Pod is the smallest or atomic unit in Kubernetes. While direct containers can be created with a container engine like Docker, Kubernetes introduces the concept of Pods as a logical unit. Kubernetes talks to Pods, not directly to containers. A Pod is a logical envelope or wrapper for containers. It typically contains one or more containers. In Kubernetes, the control unit is called a Pod, not containers.</p>"},{"location":"flink/iq/","title":"FLINK_IQ","text":"<p>1. See you using Flink, briefly introduce Flink?</p> <p>Apache Flink is an open-source stream processing and batch processing framework designed for big data processing and analytics. It provides fault tolerance, high throughput, and low-latency processing of large-scale data streams</p> <p>2. What are the differences between Flink and Spark Streaming?</p> <ol> <li>The design ideas are different. Flink considers batch to be a kind of streaming, and spark considers a streaming batch.</li> <li>The architecture model is different. Spark has Driver, Master, Worker, and Executor. Flink has the concepts of TaskManager, JobManager, Task, SubTask, and Slot</li> <li>Flink's streaming data processing is much stronger than spark, for example, time supports three kinds of time There are more windows than spark</li> <li>In the case of out-of-order data, Flink is stronger than spark, because flink has watermark. In fact, the calculation method when running is the time of the last data-if watermaker is greater than the end of the window, execute</li> <li>For fault tolerance, flink is also better than spark. For example, flink supports two-stage transactions to ensure that data after program crashes will not be re-consumed. Spark also has checkpoints, but it only ensures that data is not lost, and it cannot be repeated. consumption.</li> </ol> <p>3. What are the roles of Flink cluster? What are the functions?</p> <p>Flink programs mainly have three roles: TaskManager, JobManager, and Client when they are running.</p> <p>JobManager In the role of a manager in the cluster, it is the coordinator of the entire cluster. It is responsible for receiving the execution of Flink Job, coordinating checkpoints, recovering from failures, and managing Task Manager.</p> <p>TaskManager It is responsible for the resource information on the node where the manager is located, such as memory, disk, and network. It will report the resource to the JobManager when it is started.</p> <p>Client It is the client submitted by the Flink program. When a user submits a Flink program, a Client is first created. Then the program submitted by the user will be preprocessed and submitted to the cluster for processing.</p> <p>4. What is TaskSlot?</p> <p>In Flink's architecture, TaskManager is the working node that is actually used to execute our program. TaskManager is a JVM process. In fact, in order to achieve the concept of resource isolation and parallel execution, the concept of TaskSlot was proposed at this time, which is actually In order to control how many Tasks the TaskManager can receive, the TaskManager is controlled by taskslot, that is, if we have a source that specifies three parallelism, then he will use three slots, and the other one needs to be mainly parallel as an operator When the degree is the same, and there is no change in the degree of parallelism, or there is no shuffle, they will be together at this time. This is an optimized concept.</p> <p>5. What are the commonly used operators in Flink?</p> <p>Map operator</p> <p>Filter operator</p> <p>KeyBy operator</p> <p>Window window</p> <p>6. What is the parallelism of Flink and What is the parallelism setting of Flink?</p> <p>The parallelism of Flink is well understood. For example, kafkaSource, its parallelism is the number of partitions by default. The degree of parallelism is this operator, and how many taskslot are needed, we should know that is the advantage of parallel computing. Generally, the degree of parallelism is set according to the amount of data. It is best to keep the source and map operators without shuffle, because the pressure on the source and map operators is not very large, but when our data table is widened, It is better to set it larger.</p> <p>7. What is the relationship between Flink's Slot and parallelism?</p> <p>slot is a concept in TaskManager. Parallelism is a concept in the program, that is, the concept of execution level. In fact, slot specifies how many slots this TaskManager has and how much parallelism can be supported, but the parallelism developed by the program uses slots That is slot, that is, TaskManager is the provider and the program is the user</p> <p>8. What if Flink encounters an abnormal restart of the program?</p> <p>Flink has some restart strategies, and as long as the checkpoint is done, it can be done at least once. Of course, it may not be accurate once, but some components can be done. The restart strategy generally set is a fixed delay restart strategy. The restart does not delete the checkpoint. Generally, the number of restarts set by our company is 4 times. If it stops, we will send a nail warning and start from the checkpoint when it starts.</p> <p>9. Flink's distributed cache</p> <p>The distributed cache implemented by Flink is similar to Hadoop. The purpose is to read the file locally and put it in the taskmanager node to prevent the task from repeatedly pulling data and reduce performance.</p> <p>10. Broadcast variables in Flink</p> <p>We know that Flink is parallel, and the calculation process may not be performed in a Slot. Then there is a situation: when we need to access the same data. Then the broadcast variable in Flink is to solve this situation. We can understand the broadcast variable as a public shared variable. We can broadcast a dataset, and then different tasks can be obtained on the node. There will only be one copy of this data on each node.</p> <p>11. Do you know what windows in Flink are?</p> <p>Flink supports two ways to divide windows, according to time and count. session is also a kind of time</p> <p>Tumbing Count Window\uff1a Perform calculation when reaching a certain number, no folding</p> <p>Sliding Time Window\uff1a When a certain period of time is reached, roll over, there can be overlap, generally used to calculate the recent demand, such as nearly 5 minutes.</p> <p>Tubing time Window\uff1a When a certain period of time is reached, the slide is carried out, which can be thought of as the Nokia slide phone used before. This is actually a micro batch</p> <p>Sliding Count Window\uff1a Slide when it reaches a certain number</p> <p>Session Window: The window data has no fixed size, it is divided according to the parameters passed in by the user, and the window data does not overlap. It is similar to calculating the user's previous actions when the user logs out.</p> <p>12. Flink's state storage?</p> <p>Flink often needs to store intermediate states during calculations to avoid data loss and recover from abnormal states. Choosing a different state storage strategy will affect the state interaction between JobManager and Subtask, that is, JobManager will interact with State to store state.</p> <p>Flink provides three state storage: MemoryStateBackend</p> <p>FsSateBackend</p> <p>RocksDBStateBackend</p> <p>13. What kind of time are there in Flink?</p> <p>Event time: the time when the event actually occurred</p> <p>Intake time: time to enter flink</p> <p>Processing time: the time to enter the flink operator</p> <p>14. What is Watermark in Flink?</p> <p>Watermark is an operation used by Flink to deal with out-of-order time. In fact, in Flink, if we use event time and take kafka's source, then the window execution time at this time is the smallest among the partitions Partitions are used for triggering, and each partition must be triggered to perform calculations. Why is this? In fact, it is because the partitions of Kafka are disordered. Orderly in the zone. The execution time is the maximum time minus the watermark&gt;window end time, and the calculation will be executed at this time. Watermarks are used in Apache Flink to track the progress of event time. They represent a threshold for event times and indicate that no events with timestamps earlier than the watermark should arrive any longer. They help define when window computations should be considered complete.</p> <p>15. What is Unbounded streams in Apache Flink?</p> <p>Any type of data is produced as a stream of events. Data can be processed as unbounded or bounded streams. Unbounded streams have a beginning but no end. They do not end and continue to provide data as it is produced. Unbounded streams should be processed continuously, i.e., events should be handled as soon as they are consumed. Since the input is unbounded and will not be complete at any point in time, it is not possible to wait for all of the data to arrive. Processing unbounded data sometimes requires that events are consuming in a specific order, such as the order in which events arrives, to be able to reason about result completeness.</p> <p>16. What is Bounded streams in Apache Flink?</p> <p>Bounded streams have a beginning and an end point. Bounded streams could be processed by consuming all data before doing any computations. Ordered ingestion is not needed to process bounded streams since a bounded data set could always be sorted. Processing of bounded streams is also called as batch processing.</p> <p>17. What is Dataset API in Apache Flink?</p> <p>The Apache Flink Dataset API is used to do batch operations on data over time. This API is available in Java, Scala, and Python. It may perform various transformations on datasets such as filtering, mapping, aggregating, joining, and grouping.</p> <p>DataSet API helps us in enabling the client to actualize activities like a guide, channel, gathering and so on.It is utilized for appropriated preparing, it is an uncommon instance of stream preparing where we have a limited information source.They are regular programs that implement transformation on data sets like filtering, mapping, etc.</p> <p>Data sets are created from sources like reading files, local collections, etc.All the results are returned through sinks, the execution can happen in a local JVM or on clusters of many machines.</p> <p>DataSet&gt; wordCounts = text .flatMap(new LineSplitter()) .groupBy(0) .sum(1); <p>18. What is DataStream API in Apache Flink?</p> <p>The Apache Flink DataStream API is used to handle data in a continuous stream. On the stream data, you can perform operations such as filtering, routing, windowing, and aggregation. On this data stream, there are different sources such as message queues, files, and socket streams, and the resulting data can be written to different sinks such as command line terminals. This API is supported by the Java and Scala programming languages.</p> <p>DataStream&gt; dataStream = env .socketTextStream(\"localhost\", 9091) .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(7)) .sum(1); <p>19. What is Apache Flink Table API?</p> <p>Table API is a relational API with an expression language similar to SQL. This API is capable of batch and stream processing. It is compatible with the Java and Scala Dataset and Datastream APIs. Tables can be generated from internal Datasets and Datastreams as well as from external data sources. You can use this relational API to perform operations such as join, select, aggregate, and filter. The semantics of the query are the same if the input is batch or stream. val tableEnvironment = TableEnvironment.getTableEnvironment(env) // register a Table tableEnvironment.registerTable(\"TestTable1\", ...); // create a new Table from a Table API query val newTable2 = tableEnvironment.scan(TestTable1).select(...);</p> <p>20. What is Apache Flink FlinkML?</p> <p>FlinkML is the Flink Machine Learning (ML) library. It is a new initiative in the Flink community, with an expanding list of algorithms and contributors. FlinkML aims to include scalable ML algorithms, an easy-to-use API, and tools to help reduce glue code in end-to-end ML systems. Note: Flink Community has planned to delete/deprecate the legacy flink-libraries/flink-ml package in Flink1.9, and replace it with the new flink-ml interface proposed in FLIP39 and FLINK-12470.</p> <p>21. Explain the Apache Flink Job Execution Architecture?</p> <p>Program: It is a piece of code that is executed on the Flink Cluster.</p> <p>Client: It is in charge of taking code from the given programm and creating a job dataflow graph, which is then passed to JobManager. It also retrieves the Job data.</p> <p>JobManager: It is responsible for generating the execution graph after obtaining the Job Dataflow Graph from the Client. It assigns the job to TaskManagers in the cluster and monitors its execution.</p> <p>TaskManager:It is in charge of executing all of the tasks assigned to it by JobManager. Both TaskManagers execute the tasks in their respective slots in the specified parallelism. It is in charge of informing JobManager about the status of the tasks.</p> <p>22. What are the features of Apache Flink?</p> <p>One of the key features of Apache Flink is its ability to process data in real-time with low-latency and high throughput. It supports event time processing, which means it can handle out-of-order events and provide correct results based on event timestamps. Flink also provides extensive windowing operations for aggregating data within time intervals, such as tumbling windows, sliding windows, and session windows.</p> <p>Another important feature of Flink is its support for fault-tolerance. It achieves fault-tolerance through a mechanism called \"exactly-once\" processing, which guarantees that each event is processed exactly once, even in the presence of failures. This is crucial for applications where data correctness is paramount.</p> <p>Apache Flink can process all data in the form of streams at any point in time. Apache Flink does not give a burden on users' shoulders for tunning or managing the physical execution concepts. The memory management is done by the Apache Flink itself and not by the user. Apache Flink optimizer chooses the best plan to execute the user's program hence very little intervention is required in terms of tunning. Apache Flink can be deployed on various cluster frameworks. It is capable to support various types of the file system. Apache Flink can be integrated with Hadoop YARN in a very good way.</p> <p>23. What are the Apache Flink domain-specific libraries?</p> <p>The following is the list of Apache Flink domain-specific libraries. FlinkML: It is used for machine learning. Table: It is used to perform the relational operation. Gelly: It is used to perform the Graph operation. CEP: It is used for complex event processing.</p> <p>24. What is the programing model of Apache Flink?</p> <p>The Apache Flink Datastream and the Dataset work as a programming model of flink and its other layers of architecture. The Datastream programming model is useful in real-time stream processing whereas the Dataset programming model is useful in batch processing.</p> <p>25. What are the use cases of Apache Flink?</p> <p>Many real-world industries are using Apache Flink and their use cases are as mentioned below.</p> <p>Financial Services Financial industries are using Flink to perform fraud detection in real-time and send mobile notifications.</p> <p>Healthcare The hospitals are using Apache Flink to collect data from devices such as MRI, IVs for real-time issue detection and analysis.</p> <p>Ad Tech Ads companies are using Flink to find out the real-time customer preference.</p> <p>Oil Gas The Oil and Gas industries are using Flink for real-time monitoring of pumping and issue detection.</p> <p>Telecommunications The telecom companies are using Apache Flink to provide the best services to the users such as real-time view of billing and payment, the optimization of antenna per-user location, mobile offers, and so on.</p> <p>26. What is bounded and unbounded data in Apache Flink?</p> <p>Apache Flink processes data in the form of bounded and unbounded streams. The bounded data will have a start point and an endpoint and the computation starts once all data has arrived. It is also called batch processing. The unbounded data will have a start point but no endpoint because it is streaming of data. The processing of unbounded data is continous and doesn't wait for complete data. As soon the data is generated the processing will start.</p> <p>27. How Apache Flink handles the fault-tolerance?</p> <p>Apache Flink manages the fault-tolerance of stream applications by capturing the snapshot of the streaming dataflow state so in case of failure those snapshots will be used for recovery. For batch processing, Flink uses the program's sequence of transformations for recovery.</p> <p>To achieve fault tolerance, Apache Flink employs a combination of techniques such as data replication, checkpointing, and exactly-once processing semantics. The framework allows users to define fault-tolerant data streams, which are resilient to failures and can effectively recover from possible errors.</p> <ol> <li> <p>Checkpointing: Apache Flink periodically captures the state of executing jobs by taking checkpoints. Checkpoints consist of the in-memory state of all operators and the metadata necessary for restoring the state, such as the offset of each stream source. Users can configure the frequency of checkpoints to strike a balance between reliability and performance</p> </li> <li> <p>State Backends: Apache Flink supports different state backends (e.g., in-memory, RocksDB) to persist checkpointed state. The chosen backend determines how and where the state is stored, allowing for fault tolerance and efficient recovery.</p> </li> <li> <p>Exactly-once Processing: Flink's checkpointing, along with its transactional processing capabilities, enables exactly-once processing semantics. It ensures that each record is processed exactly once, even in the presence of failures or system restarts. This guarantees consistency and correctness in data processing.</p> </li> <li> <p>Failure Handling: In case of failures, Flink automatically reverts the system to the latest successful checkpoint. It replays the data from that point onwards, resuming processing from a consistent state.</p> </li> </ol> <p>28. What is the responsibility of JobManager in Apache Flink Cluster?</p> <p>The Job Manager is responsible for managing and coordinating with distributed processing of a program. It assigns the task to node managers, handles the failures for recovery, and performs the checkpointing. It has three components namely ResourceManager, Dispatcher, and JobMaster.</p> <p>29. What is the responsibility of TaskManager in Apache Flink Cluster?</p> <p>The Task Manager is responsible for executing the dataflow task and return the result to JobManager. It executes the task in the form of a slot hence the number of slots shows the number of process execution.</p> <p>30. What is the difference between stream processing and batch processing?</p> <p>In Batch processing, the data is a bounded set of the stream that has a start point and the endpoint, so once the entire data is ingested then only processing starts in batch processing mode. In-stream processing the nature of data is unbounded which means the processing will continue as the data will be received.</p> <p>Flink How to ensure accurate one-time consumption Flink There are two ways to ensure accurate one-time consumption Flink Mechanism 1\u3001Checkpoint Mechanism 2\u3001 Two stage submission mechanism</p> <p>Checkpoint Mechanism:Mainly when Flink Turn on Checkpoint When , Will turn out for the Source Insert a barrir, And then this barrir As the data flows all the time , When it comes to an operator , This operator starts to make checkpoint, It's made from barrir The state of the current operator when it comes to the previous time , Write the state to the state backend . And then barrir Flow down , When it flows to keyby perhaps shuffle Operator time , For example, when the data of an operator , Depending on multiple streams , There will be barrir alignment , That is, when all barrir All come to this operator to make checkpoint, Flow in turn , When it flows to sink Operator time , also sink The operator is also finished checkpoint Will send to jobmanager The report checkpoint n Production complete .</p> <p>Two stage submission mechanism: Flink Provides CheckpointedFunction And CheckpointListener These two interfaces ,CheckpointedFunction There is snapshotState Method , Every time checkpoint Trigger execution method , The cache data is usually put into the State , You can think of it as one hook, This method can be used to achieve pre submission ,CheckpointListyener There is notifyCheckpointComplete Method ,checkpoint Notification method after completion , There are some extra operations that can be done here . for example FLinkKafkaConumerBase Use this to do Kafka offset Submission of , In this method, you can implement the submit operation . stay 2PC If the corresponding process, such as a checkpoint Failure words , that checkpoint It will roll back , No impact on data consistency , So if you're informing checkpoint Success followed by failure , Then it will be in initalizeSate Method to complete the transaction commit , This ensures data consistency . It's mainly based on checkpoint The state file to judge .</p> <p>flink and spark difference: flink It's a similar spark Of \" Open source technology stack \", Because it also provides batch processing , Flow computation , Figure calculation , Interactive query , Machine learning, etc .flink It's also memory computing , similar spark, But the difference is ,spark The calculation model of is based on RDD, Consider streaming as a special batch process , His DStream In fact, or RDD. and flink Consider batch processing as a special stream computing , But there are two engines in the layer of batch processing and streaming computing , Abstract the DataSet and DataStream.flink It's also very good in performance , Streaming delay ratio spark Less , Can do real flow computing , and spark It can only be a quasi flow calculation . And in batch processing , When the number of iterations gets more ,flink Faster than spark faster , So if flink Come out earlier , Maybe more than what we have now Spark More fire .</p> <p>31. Flink watermark Transmission mechanism.</p> <p>Flink Medium watermark Mechanism is used to deal with disorder ,flink It has to be event time , A simple example is , If the window is 5 second ,watermark yes 2 second , that All in all 7 second , When will calculation be triggered at this time , Suppose the initial time of the data is 1000, Then wait until 6999 It will trigger 5999 The calculation of windows , So the next one is 13999 Is triggered when 10999 The window of In fact, this is watermark The mechanism of , In multi parallelism , For example, in kafka The window will not be triggered until all partitions are reached</p> <p>32. Flink window join:</p> <p>1\u3001window join, That is, according to the specified fields and scrolling sliding window and session window inner join</p> <p>2\u3001 yes coGoup In fact, that is left join and right join,</p> <p>3\u3001interval join That is to say In the window join There are some problems , Because some of the data really came after the meeting , It's still a long time , Then there will be interval join But it has to be the time of the event , And also specify watermark And water level and getting event timestamps . And set it up Offset interval , because join I can't wait all the time .</p> <p>33. keyedProcessFunction How it works</p> <p>keyedProcessFunction There is one ontime Operation of the , If so event In time that The time to call is to look at ,event Of watermark Is it greater than trigger time Time for , If it is greater than, calculate it , No, just wait , If it is kafka Words , Then the default is to trigger the partition key in the shortest time .</p> <p>34. How to deal with offline data such as the association with offline data?</p> <p>1\u3001async io 2\u3001broadcast 3\u3001async io + cache 4\u3001open Method , Then the thread is refreshed at a fixed time , Cache updates are deleted first , Then write another one, and then write to the cache</p> <p>35. What if there is a data skew?</p> <p>Flink How to view data skew \uff1a stay flink Of web ui You can see the data skew in , It's every one subtask The amount of data processed varies greatly , For example, some have only one M yes , we have 100M This is a serious data skew . KafkaSource Data skew at the end For example, upstream kafka It was specified when it was sent key There are data hotspots , So just after the access , Do a load balancing \uff08 The premise is not keyby\uff09. Aggregation class operator data skew Pre aggregation plus global aggregation</p> <p>36. FlinkTopN And offline TopN The difference between?</p> <p>topn It is a common function in both offline and real-time computing , It's different from... In offline computing topn, Real time data is continuous , This will give topn It's very difficult to calculate , Because it's going to keep a... In memory topn Data structure of , When new data comes , Update this data structure</p> <p>37. Sparkstreaming and flink in checkpoint?</p> <p>sparkstreaming Of checkpoint It will lead to repeated consumption of data however flink Of checkpoint Sure Make sure it's accurate one time , At the same time, it can be incremental , fast checkpoint Of , There are three states ,memery\u3001rocksdb\u3001hdfs</p> <p>38. A brief introduction cep State programming:</p> <p>Complex Event Processing\uff08CEP\uff09\uff1a FLink Cep Is in FLink Complex time processing library implemented in ,CEP Allows event patterns to be detected in an endless stream of time , Give us a chance to grasp the important parts of the data , One or more time streams composed of simple events are matched by certain rules , Then output the data the user wants , That is, complex events that satisfy the rules .</p> <p>Flink Data aggregation in , How to aggregate without windows valueState Used to save a single value ListState Used to hold list Elements MapState Used to save a set of key value pairs ReducingState Provided with ListState Same method , Return to one ReducingFunction The aggregated value . AggregatingState and ReducingState similar , Return to one AggregatingState The value after internal aggregation</p> <p>39. How to deal with abnormal data in Flink.</p> <p>Abnormal data in our scenario , It is generally divided into missing fields and outlier data . outliers \uff1a For example, data on the age of the baby , For example, for the maternal and infant industry , The age of a baby is a crucial data , It's the most important , Because the baby is bigger than 3 At the age of 20, you hardly buy things from mothers and babies . There are days like ours \u3001 Unknown \u3001 And for a long time . This is an exception field , We will show the data to store managers and regional managers , Let them know how many ages are not allowed . If we have to deal with it , It can be corrected in real time according to the time of purchase , For example, maternity clothing \u3001 The rank of milk powder \u3001 The size of a diaper , As well as pacifiers, some can distinguish the age group to carry on the processing . We don't process the data in real time , We're going to have a low-level strategy task, night dimension, to run , Run once a week . Missing field \uff1a For example, some fields are really missing , If you can fix it, you can fix it . Give up if you can't fix it , It's like the news recommendation filter in the last company .</p> <p>40. Is there any possibility of data loss in Flink?</p> <p>Flink There are three kinds of data consumption semantics \uff1a At Most Once One consumption at most In case of failure, it may be lost At Least Once At least once If there is a fault, it may be repeated Exactly-Once Exactly once If something goes wrong , It can also ensure that the data will not be lost or repeated . flink The new version is no longer available At-Most-Once semantics .</p> <p>Flink interval join Can you write it simply DataStream keyed1 = ds1.keyBy(o -&gt; o.getString(\"key\")) DataStream keyed2 = ds2.keyBy(o -&gt; o.getString(\"key\")) // Time stamp on the right -5s&lt;= Stream timestamp on the left &lt;= Time stamp on the right -1s keyed1.intervalJoin(keyed2).between(Time.milliseconds(-5), Time.milliseconds(5)) <p>41. How to maintain Checkpoint?</p> <p>By default , If set Checkpoint Options ,Flink Only the most recently generated 1 individual Checkpoint. When Flink When the program fails , From the nearest one Checkpoint To recover . however , If we want to keep more than one Checkpoint, And can choose one of them to recover according to the actual needs , It's more flexible .Flink Support to keep multiple Checkpoint, Need to be in Flink Configuration file for conf/flink-conf.yaml in , Add the following configuration to specify that at most Checkpoint The number of . For small files, please refer to The death of Daedalus - Solutions to the problem of small files in the field of big data .</p> <p>42. What's the difference at Spark and Flink Serialization?</p> <p>Spark The default is Java Serialization mechanism , At the same time, there is an optimization mechanism , That is to say kryo Flink It's a self implemented serialization mechanism , That is to say TypeInformation</p> <p>43. How to deal with late data?</p> <p>In Flink, late data refers to events that arrive after the watermark has progressed past their event time. This typically happens due to network delays, out-of-order arrival, or slow event sources.</p> <p>To handle late data, Flink provides the following mechanisms:</p> <p>Allowed Lateness You can configure how long Flink should wait for late events using the allowedLateness() method.</p> <p>java Copy Edit .window(TumblingEventTimeWindows.of(Time.minutes(5))) .allowedLateness(Time.minutes(2)) This keeps the window open for 2 extra minutes to accept late events.</p> <p>Late elements within this period will update the window and trigger re-evaluation.</p> <p>Side Output for Late Data Events that arrive after the allowed lateness can be redirected to a side output for separate handling.</p> <p>java Copy Edit OutputTag lateOutputTag = new OutputTag(\"late-data\"){}; windowedStream     .sideOutputLateData(lateOutputTag); You can process or store this late data elsewhere for analysis or alerting. <p>Watermarks Configuration</p> <p>Watermarks indicate the progress of event time. You can use bounded out-of-orderness watermarks to tolerate out-of-order events.</p> <p>java Copy Edit env.assignTimestampsAndWatermarks(     WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofMinutes(2)) ); <p>44. When to use aggregate perhaps process:</p> <p>aggregate\uff1a Incremental aggregation process\uff1a Total polymerization When calculating the accumulation operation, you can use aggregate operation . When calculating the full amount of data in the window, use process, For example, sorting and other operations.</p> <p>45. How does Flink handle stateful computations efficiently:</p> <p>Flink is designed to handle stateful computations efficiently by using a distributed and fault-tolerant mechanism called StateBackend. It provides several options for managing state, including in-memory state and state that can be stored on disk or in an external system like Apache Hadoop or Amazon S3.</p> <p>Flink's StateBackend allows users to choose between three options: MemoryStateBackend, FsStateBackend, and RocksDBStateBackend. These options differ in terms of their trade-offs between performance and fault tolerance. For example, the MemoryStateBackend provides high performance and low latency but does not recover state after a failure, while the FsStateBackend provides fault tolerance by storing state on a distributed file system.</p> <p>46. What are the important factors to consider when tuning the performance of Apache Flink applications:</p> <p>When tuning the performance of Apache Flink applications, several important factors need to be considered. These factors range from resource allocation to algorithm design and configuration settings. Here are some key aspects to focus on:</p> <ol> <li> <p>Resource Allocation: Efficiently allocating resources is crucial for optimal performance. This includes tuning the number of Task Managers and slots, Memory, and CPU resources. Understanding the data and workload patterns can help determine the right resource allocation strategy.</p> </li> <li> <p>Data Serialization and Deserialization: Choosing the appropriate serialization format can greatly impact performance. Flink provides multiple serialization formats, such as Avro, JSON, and Protobuf. Assessing the size and complexity of data structures can help decide the most suitable serialization method.</p> </li> <li> <p>Memory Management: Flink employs managed memory which consists of both heap and off-heap memory. Configuring the sizes of managed memory components, such as network buffers and managed memory fractions, can significantly impact performance</p> </li> <li> <p>Parallelism: Parallelism influences the throughput and resource utilization of Flink applications. Setting an appropriate degree of parallelism, considering the available resources and input characteristics, is important.</p> </li> <li> <p>Operators' Chaining and State Size: Operator chaining can optimize performance by reducing serialization and deserialization costs. Additionally, Flink provides various state backends, such as MemoryStateBackend and RocksDBStateBackend, that allow selecting different state storage options based on the state size and access patterns.</p> </li> </ol> <p>47. How does Flink handle exactly-once semantics and end-to-end consistency in data processing?:</p> <p>Apache Flink provides built-in mechanisms to handle exactly-once semantics and ensure end-to-end consistency in data processing pipelines. This is essential in scenarios where duplicate or lost data cannot be tolerated, such as financial transactions, data pipelines, or event-driven applications.</p> <p>Flink achieves exactly-once semantics through a combination of checkpointing, state management, and transactional sinks. Checkpointing is a mechanism that periodically takes a snapshot of the application's state, including the operator's internal state and the position in the input streams. By storing these checkpoints persistently, Flink can recover the state and precisely revert to a previous consistent state when failures occur. The state managed by operators includes both user-defined operator state and Flink's internal bookkeeping state.</p> <p>To enable exactly-once semantics, it is important to ensure that the output of the pipeline is also processed atomically and deterministically. Flink achieves this through transactional sinks, which are responsible for writing the output of a stream into an external system (e.g., a database). When a failure occurs, these sinks coordinate with Flink's checkpointing to guarantee that the data is only committed if the checkpoint is successful. This ensures that the output of the pipeline is consistent and non-duplicative</p> <p>48. How does Flink handle windowing in stream processing?:</p> <p>Flink supports various windowing operations, such as tumbling windows, sliding windows, and session windows. Windowing in Flink allows you to group and process events based on time or count constraints. It provides flexibility in defining window sizes and slide intervals.</p> <p>49. If everything is a stream, why are there a DataStream and a DataSet API in Flink?:</p> <p>Bounded streams are often more efficient to process than unbounded streams. Processing unbounded streams of events in (near) real-time requires the system to be able to immediately act on events and to produce intermediate results (often with low latency). Processing bounded streams usually does not require producing low latency results, because the data is a while old anyway (in relative terms). That allows Flink to process the data in a simple and more efficient way.</p> <p>The DataStream API captures the continuous processing of unbounded and bounded streams, with a model that supports low latency results and flexible reaction to events and time (including event time). The DataSet API has techniques that often speed up the processing of bounded data streams. In the future, the community plans to combine these optimizations with the techniques in the DataStream API.</p> <p>50. What are windowing strategies in Flink?</p> <p>Windowing assigns unbounded streams into finite-sized windows for aggregation. Common strategies include: Time windows (fixed or sliding), Count windows (fixed number of events), and Session windows (gaps between events define windows).</p> <p>51. What are the different types of operators in Flink?:</p> <p>Flink operators include map, flatmap, filter, keyBy, window, reduce, aggregate, join, etc., offering a rich set of transformations for data processing.</p> <p>52. What is the difference between a keyed and non-keyed stream in Flink?:</p> <p>Keyed streams partition data based on a key, enabling operations like windowing and stateful aggregations on specific keys. Non-keyed streams process data without partitioning.</p> <p>53. Explain the role of the process function in Flink.</p> <p>Process functions are low-level operators that provide fine-grained control over event processing and state management. They offer advanced features like timers and custom state management logic.</p> <p>54. What are the different windowing techniques in Apache Flink?:</p> <p>Apache Flink supports a variety of windowing techniques, including:</p> <p>Tumbling windows: These windows are fixed in size and slide across the stream.</p> <p>Sliding windows: These windows are variable in size and slide across the stream.</p> <p>Session windows: These windows are based on the arrival time of events.</p> <p>Count windows: These windows are based on the number of events that arrive within a given time interval.</p> <p>55. What is the difference between bounded and unbounded streams?:</p> <p>Bounded streams are datasets that have a defined start and end. Unbounded streams are datasets that have a defined start but no defined end. Bounded streams can be processed by ingesting the complete data and them preforming any computations. Unbounded streams have to be processed continuously as new data comes in. In most cases, unbounded streams have to be process in the order in which messages are received. Bounded messages can be processed in any order since the messages can be sorted as needed.</p> <p>56. What features does flink framework provide to handle state?</p> <p>Flink framework provides the following features to handle state. Data structure specific state primitives - Flink framework provides specific state primitives for different data structures such as lists and maps. Pluggable state storages - Flink supports multiple pluggable state storage systems that store state in-memory or on disc. Exactly-once state consistency - Flink framework has checkpoint and recovery algorithms, which guarantee the consistency of state in case of failures. Store large state data - Flink has the ability to store very large application state data, of several terabytes, due to its asynchronous and incremental checkpoint algorithm. Scalable Applications - Flink applications are highly scalable since the application state data can be distributed across containers.</p> <p>57. What features does flink framework provide to handle time?:</p> <p>Flink framework provides the following features to handle time. Event-time mode - Flink framework supports applications that process steams based on event-time mode, i.e. applications that process streams based on timestamp of events. Processing-time mode - Flink framework also supports applications that process streams based on processing-time mode, i.e. applications that process streams based on the clock time of the processing machine. Watermark support Flink framework provides watermark support in the processing of streams based on event-time mode. Late data processing Flink framework supports the processing of events that arrive late, after a related computation has already been performed.</p>"},{"location":"hadoop/Bigdata/","title":"Overview","text":""},{"location":"hadoop/Bigdata/#what-is-bigdata","title":"What is Bigdata?","text":"<p>Big Data is a collection of data that is huge in volume, yet growing exponentially with time. It is a data with so large size and complexity that none of traditional data management tools can store it or process it efficiently.</p>"},{"location":"hadoop/Bigdata/#5vs-of-bigdata","title":"5V's of Bigdata","text":"<ul> <li> <p>Volume: This refers to the sheer quantity of data, which is typically enormous. It can range from terabytes to petabytes and even exabytes of data.</p> <p>Eg:- Batch,Near Realtime,Streaming, Streams.</p> </li> <li> <p>Velocity: This refers to the speed at which new data is generated and the speed at which data moves around. With the growth of the Internet and smart devices, data is being generated continuously, in real time, from various sources. </p> <p>Eg:-Structured,Unstructured,Semi Structured.</p> </li> <li> <p>Variety: This refers to the different types of data that are now available. Big Data can include structured data (SQL databases), semi-structured data (XML, JSON), unstructured data (Word documents, JPEG images), and even complex structured data (from ERP systems or websites). </p> <p>Eg:-Terabytes,Records,Transactions,tables.</p> </li> <li> <p>Veracity: This refers to the quality of the data, which can vary greatly. Veracity allows us to deal with uncertainty or imprecision, which is often an issue with many forms of big data.</p> <p>Eg:-Trustworthiness, Authenticity, Origin, Accountability.</p> </li> <li> <p>Value: This is the ability to turn data into value. This is becoming the most important V of Big Data because it's important that businesses make a return on their investment in big data and data analytics.</p> <p>Eg:-Statistical, Events, Corelation, Hypotheticcal.</p> </li> </ul>"},{"location":"hadoop/Bigdata/#examples-of-bigdata","title":"Examples Of BigData","text":"<p>Social Media: Data from posts, likes, shares on platforms like Facebook, Twitter, Instagram.</p> <p>Healthcare: Large volumes of data from electronic health records (EHRs), lab results, and patient histories.</p> <p>Finance: Data from financial transactions, stock exchanges, and trading systems.</p> <p>Telecommunications: Call detail records (CDRs) and network logs.</p> <p>E-commerce: Data from millions of transactions daily on platforms like Amazon, eBay.</p> <p>IoT: Real-time data from devices like smart homes, wearables, and industrial sensors.</p> <p>Transport: Data on travel times, routes, and traffic conditions from services like Uber and Google Maps.</p>"},{"location":"hadoop/Bigdata/#types-of-data","title":"Types of Data","text":"<ul> <li> <p>Structured Data: This is data that adheres to a model and is easily searchable. Examples include data stored in relational databases and spreadsheets. </p> </li> <li> <p>Unstructured Data: This type of data does not have a predefined model or is not organized in a pre-defined manner. Examples include text files, images, videos, emails, web pages, and social media posts.</p> </li> <li> <p>Semi-Structured Data: This is a hybrid of structured and unstructured data. While it does not conform to the formal structure of data models, it contains tags or other markers to enforce hierarchy and order. Examples include JSON, XML, and email messages with both defined fields and free-form text.</p> </li> </ul>"},{"location":"hadoop/Bigdata/#what-is-a-cluster","title":"What is a cluster","text":"<p>A cluster, in the context of computing, refers to a group of computers or servers that work together and can be viewed as a single system. These computers, known as nodes, interact with each other to accomplish a common goal. This setup is used to improve performance and availability over that provided by a single computer, while typically being much more cost-effective and scalable than a single computer of comparable speed or availability. </p>"},{"location":"hadoop/Bigdata/#vertical-scaling","title":"Vertical Scaling","text":"<p>Vertical Scaling, also known as scaling up, involves increasing the capacity of a single server, such as using a more powerful CPU, adding more RAM, or increasing disk space.</p>"},{"location":"hadoop/Bigdata/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Horizontal Scaling, also known as scaling out, involves adding more servers to a system and distributing the load across multiple servers. </p>"},{"location":"hadoop/HDFS/","title":"HDFS","text":""},{"location":"hadoop/HDFS/#hadoop","title":"Hadoop","text":"<p>Hadoop is an open-source software framework for storing and processing big data in a distributed fashion on large clusters of commodity hardware. Essentially, it accomplishes two tasks: massive data storage and faster processing. It was developed by the Apache Software Foundation and is based on two main components:</p> <ul> <li> <p>Hadoop Distributed File System (HDFS): This is the storage component of Hadoop, designed to hold large amounts of data, potentially in the range of petabytes or even exabytes. The data is distributed across multiple nodes in the cluster, providing high availability and fault tolerance.</p> </li> <li> <p>Map-Reduce: This is the processing component of Hadoop, which provides a software framework for writing applications that process large amounts of data in parallel. MapReduce operations are divided into two stages: the Map stage, which sorts and filters the data, and the Reduce stage, which summarizes the data.</p> </li> <li> <p>Yet Another Resource Negotiator (YARN): This is the resource management layer in Hadoop. Introduced in Hadoop 2.0, YARN decouples the programming model from the resource management infrastructure, and it oversees and manages the compute resources in the clusters.</p> </li> </ul>"},{"location":"hadoop/HDFS/#properties-of-hadoop","title":"Properties of Hadoop","text":"<p>Scalability: Can store and distribute large data sets across many servers.</p> <p>Cost-effectiveness: Designed to run on inexpensive, commodity hardware.</p> <p>Flexibility: Can handle any type of data, structured or unstructured.</p> <p>Fault Tolerance: Data is automatically replicated to other nodes in the cluster.</p> <p>Data Locality: Processes data on or near the node where it's stored, reducing network I/O.</p> <p>Simplicity: Provides a simple programming model (MapReduce) for processing data.</p> <p>Open-source: Freely available to use and modify with a large community of contributors.</p>"},{"location":"hadoop/HDFS/#hdfs-architecture-and-core-concepts","title":"HDFS Architecture and Core Concepts","text":"<p>HDFS (Hadoop Distributed File System) is a distributed file system. It is designed using a master/slave architecture.</p> <p>A Hadoop cluster consists of multiple computers networked together.</p> <ul> <li>Racks: A rack is a physical enclosure where multiple computers are fixed.Each rack typically has its individual power supply and a dedicated network switch. The importance of racks lies in the possibility of an entire rack failing if its switch or power supply goes out of network, affecting all computers within it. Multiple racks are connected, with their switches linked to a core switch, forming the Hadoop cluster.</li> </ul>"},{"location":"hadoop/HDFS/#masterslave-architecture-namenode-and-datanode","title":"Master/Slave Architecture: NameNode and DataNode","text":"<p>In HDFS, there is one master and multiple slaves.</p> <ul> <li> <p>NameNode (Master Node): The Hadoop master is called the NameNode. It is called NameNode because it stores and manages the names of directories and files within the HDFS namespace.</p> <p>Responsibilities:</p> <ol> <li>Manages the file system namespace. </li> <li>Regulates access to files by clients (e.g., checking access permissions, user quotas). </li> <li>Maintains an image of the entire HDFS namespace in memory, known as in-memory FS image (File System Image). This allows it to perform checks quickly. </li> <li>Does not store actual file data. </li> <li>Assigns DataNodes for block storage based on free disk space information from DataNodes. </li> <li>Maintains the mapping of blocks to files, their order, and all other metadata.</li> </ol> </li> <li> <p>DataNode (Slave Node): The Hadoop slaves are called DataNodes. They are called DataNodes because they store and manage the actual data of the files.</p> <p>Responsibilities:</p> <ol> <li>Stores file data in the form of blocks. </li> <li>Periodically sends a heartbeat to the NameNode to signal that it is alive. This heartbeat also includes resource capacity information that helps the NameNode in making decisions. </li> <li>Sends a block report to the NameNode, which is health information about all the blocks maintained by that DataNode.</li> </ol> </li> </ul>"},{"location":"hadoop/HDFS/#key-terminologies-and-components-of-hdfs","title":"Key terminologies and Components of HDFS","text":""},{"location":"hadoop/HDFS/#block","title":"Block","text":"<p>Block is nothing but the smallest unit of storage on a computer system. It is the smallest contiguous storage allocated to a file. In Hadoop, we have a default block size of 128MB or 256MB.</p> <p>Note</p> <p>If you have a file of 50 MB and the HDFS block size is set to 128 MB, the file will only use 50 MB of one block. The remaining 78 MB in that block will remain unused, as HDFS blocks are allocated on a per-file basis. It's important to note that this is one of the reasons why HDFS is not well-suited to handling a large number of small files. Since each file is allocated its own blocks, if you have a lot of files that are much smaller than the block size, then a lot of space can be wasted. This is also why block size in HDFS is considerably larger than it is in other file systems (default of 128 MB, as opposed to a few KBs or MBs in other systems).</p> <p>Larger block sizes mean fewer blocks for the same amount of data, leading to less metadata to manage, less communication between the NameNode and DataNodes, and better performance for large, streaming reads of data.</p>"},{"location":"hadoop/HDFS/#replication-management","title":"Replication Management","text":"<p>To provide fault tolerance HDFS uses a replication technique. In that, it makes copies of the blocks and stores in on different DataNodes. Replication factor decides how many copies of the blocks get stored. It is 3 by default but we can configure to any value.</p>"},{"location":"hadoop/HDFS/#rack-awareness","title":"Rack Awareness","text":"<p>A rack contains many DataNode machines and there are several such racks in the production. HDFS follows a rack awareness algorithm to place the replicas of the blocks in a distributed fashion. This rack awareness algorithm provides for low latency and fault tolerance. Suppose the replication factor configured is 3. Now rack awareness algorithm will place the first block on a local rack. It will keep the other two blocks on a different rack. It does not store more than two blocks in the same rack if possible.</p>"},{"location":"hadoop/HDFS/#secondary-namenode","title":"Secondary Namenode","text":"<p>The Secondary NameNode in Hadoop HDFS is a specially dedicated node in the Hadoop cluster that serves as a helper to the primary NameNode, but not as a standby NameNode. Its main roles are to take checkpoints of the filesystem metadata and help in keeping the filesystem metadata size within a reasonable limit.</p> <p>Here is what it does:</p> <ol> <li> <p>Checkpointing: The Secondary NameNode periodically creates checkpoints of the namespace by merging the fsimage file and the edits log file from the NameNode. The new fsimage file is then transferred back to the NameNode. These checkpoints help reduce startup time of the NameNode</p> </li> <li> <p>Size management: The Secondary NameNode helps in reducing the size of the edits log file on the NameNode. By creating regular checkpoints, the edits log file can be purged occasionally, ensuring it does not grow too large.</p> </li> </ol> <p>A common misconception is that the Secondary NameNode is a failover option for the primary NameNode. However, this is not the case; the Secondary NameNode cannot substitute for the primary NameNode in the event of a failure. For that, Hadoop 2 introduces the concept of Standby NameNode.</p>"},{"location":"hadoop/HDFS/#standby-namenode","title":"Standby Namenode","text":"<p>In Hadoop, the Standby NameNode is part of the High Availability (HA) feature of HDFS that was introduced with Hadoop 2.x. This feature addresses one of the main drawbacks of the earlier versions of Hadoop: the single point of failure in the system, which was the NameNode.</p> <ol> <li> <p>The Standby NameNode is essentially a hot backup for the Active NameNode. The Standby NameNode and Active NameNode are in constant synchronization with each other. When the Active NameNode updates its state, it records the changes to the edit log, and the Standby NameNode applies these changes to its own state, keeping both NameNodes in sync.</p> </li> <li> <p>The Standby NameNode maintains a copy of the namespace image in memory, just like the Active NameNode. This means it can quickly take over the duties of the Active NameNode in case of a failure, providing minimal downtime and disruption.</p> </li> <li> <p>Unlike the Secondary NameNode, the Standby NameNode is capable of taking over the role of the Active NameNode immediately without any data loss, thus ensuring the High Availability of the HDFS system.</p> </li> </ol> <p></p> <p>Hadoop incorporates robust features for fault tolerance and high availability to ensure the reliability and continuous operation of the cluster. These two concepts, while related to system resilience, address different aspects of failure within the Hadoop ecosystem.</p>"},{"location":"hadoop/HDFS/#hadoop-fault-tolerance","title":"Hadoop Fault Tolerance","text":"<p>Fault tolerance in Hadoop primarily addresses what happens when a data node fails. If a data file is broken into blocks and stored across various data nodes, the failure of one such node could lead to the loss of a part of the file, making it unreadable. Hadoop's solution to this fundamental problem is replication. It involves creating backup copies of each data block and storing them on different data nodes. This mechanism ensures that if one copy becomes unavailable, the data can still be read from another copy.</p> <p>The number of copies made for each block is determined by the replication factor, which can be configured on a file-by-file basis and even modified after a file has been created in HDFS. For instance, if a file's replication factor is set to two, HDFS automatically creates two copies of each block for that file, ensuring they are placed on different machines. Typically, the replication factor is set to three, which is considered a reasonably good level of protection, though it can be increased for files deemed super critical.</p> <p>Beyond individual node failures, Hadoop also provides protection against entire rack failures through rack awareness. Without rack awareness, if all three copies of a file's block were on nodes within the same rack, an entire rack failure would lead to the loss of all copies. By configuring Hadoop for rack awareness, it ensures that at least one copy of a block is placed in a different rack, thereby protecting against such widespread failures.</p> <p>The Name Node plays a crucial role in maintaining the desired replication factor. Each data node sends periodic heartbeats to the Name Node. If a data node stops sending heartbeats, the Name Node identifies it as failed. In response, the Name Node initiates the re-replication of affected blocks to restore the number of replicas to the configured factor, for example, back to three. The Name Node constantly monitors and tracks the replication factor of every block and triggers replication whenever necessary. Reasons for re-replication can include a data node becoming unavailable, a replica getting corrupted, a hard disk failing on a data node, or even a user increasing the replication factor of a file.</p> <p>While replication offers robust protection against failures, it comes with a cost: increased storage consumption. Making three copies of a file effectively reduces the cluster's usable storage capacity to one-third of its raw capacity, leading to higher costs. To mitigate this, Hadoop 2.x introduced storage policies, and Hadoop 3.x offers Erasure Coding as an alternative to traditional replication. Despite these alternatives, replication remains the conventional method for fault avoidance, and its costs are generally manageable because disks are relatively inexpensive.</p>"},{"location":"hadoop/HDFS/#hadoop-high-availability","title":"Hadoop High Availability","text":"<p>High availability refers to the uptime of a system, representing the percentage of time a service is operational. Enterprises typically aim for extremely high uptime, such as 99.999%, for their critical systems. It's important to distinguish high availability from fault tolerance: while data node, disk, or even rack failures, as discussed in fault tolerance, do not typically bring down the entire Hadoop cluster, high availability specifically addresses faults that would render the entire system unusable. The cluster, as a whole, usually remains available during data-related faults, with replication handling the underlying data protection.</p> <p>The primary single point of failure in a Hadoop cluster is the Name Node. The Name Node is responsible for maintaining the file system namespace, including the list of directories and files, and managing the mapping of files to their blocks. Every client interaction with the Hadoop cluster begins with the Name Node. Consequently, if the Name Node fails, the entire Hadoop cluster becomes unusable, preventing any read or write operations. Therefore, protecting against Name Node failures is essential to achieve high availability for a Hadoop cluster.</p> <p>The fundamental solution to protect against any failure is a backup. For the Name Node, this involves two key aspects: backing up all the HDFS namespace information that the Name Node maintains and having a standby Name Node machine readily available to take over its role quickly.</p> <p>The Name Node maintains the complete file system in its memory as an in-memory FS image. Additionally, it keeps an edit log on its local disk, which records every change made to the file system like a journal. Because the in-memory FS image can be reconstructed from the edit log, backing up the Name Node's edit log is crucial. The recommended solution for backing up the edit log in Hadoop 2.x is the Quorum Journal Manager (QJM). The QJM consists of at least three machines, each running a lightweight Journal Node daemon. Instead of writing edit log entries to its local disk, the Name Node is configured to write them to the QJM. Utilizing three Journal Nodes provides double protection for the critical edit log, and for even higher protection, a QJM can consist of five or seven nodes.</p> <p>A separate machine is added to the cluster and configured as a Standby Name Node. This Standby Name Node is set up to continuously read the edit log from the QJM, ensuring it stays updated with the latest file system changes. This configuration enables the Standby Name Node to assume the active Name Node role within a few seconds. Furthermore, all data nodes are configured to send their block reports (health information for blocks) to both the Active and Standby Name Nodes.</p> <p>The mechanism by which the Standby Name Node determines that the Active Name Node has failed and should take over is managed by Zookeeper and Failover Controllers. A Failover Controller runs on each Name Node. The Failover Controller on the active Name Node maintains a lock in Zookeeper, while the Standby Name Node's Failover Controller continuously attempts to acquire this lock. If the Active Name Node fails or crashes, the lock it held in Zookeeper expires. As soon as the Standby Name Node successfully acquires the lock, it recognizes that the active Name Node has failed and proceeds to transition from its standby state to the active role.</p>"},{"location":"hadoop/HDFS/#secondary-name-node","title":"Secondary Name Node","text":"<p>It is common to confuse the Secondary Name Node with the Standby Name Node, but they serve distinct purposes. As explained, a Standby Name Node acts as a direct backup for the Name Node in case of failure. The Secondary Name Node, however, addresses a different operational concern.</p> <p>When the Name Node restarts, for example, due to maintenance, it loses its in-memory FS image. It then reconstructs this image by reading the edit log. The challenge arises because the edit log grows continuously, and its size directly impacts the Name Node's restart time. A very large edit log could cause the Name Node to take an hour or more to start, which is undesirable.</p> <p>The Secondary Name Node solves this problem by performing a checkpoint activity periodically, typically every hour. During a checkpoint, the Secondary Name Node performs the following steps:</p> <ol> <li> <p>It reads the current edit log.</p> </li> <li> <p>It then creates the latest file system state, which is an exact copy of the in-memory FS image.</p> </li> <li> <p>This state is then saved to disk as an on-disk FS image.</p> </li> <li> <p>Once the new on-disk FS image is created, the Secondary Name Node truncates (clears) the edit log, as all the changes have been applied to the new on-disk image.</p> </li> <li> <p>For subsequent checkpoints, the Secondary Name Node reads the previous on-disk FS image and applies only the new changes accumulated in the edit log during the last hour. It then replaces the old on-disk FS image with the new one and truncates the edit log again.</p> </li> </ol> <p>This checkpointing process is essentially a merging of an on-disk FS image and the edit log. It is a quick process because it only deals with a limited amount of new edit logs (e.g., from the last hour), and the FS image itself is smaller compared to the cumulative edit log. The primary benefit is that when the Name Node eventually restarts, it also performs this quick checkpoint activity, reading a much smaller edit log (only from the last checkpoint) and applying it to the latest on-disk FS image, thus minimizing the restart time.</p> <p>It is important to note that when a Hadoop High Availability configuration is implemented with a Standby Name Node, the Standby Name Node also performs this checkpoint activity. Consequently, in a high availability setup, a separate Secondary Name Node service is no longer required.</p>"},{"location":"hadoop/HDFS/#write-operation-in-hdfs","title":"Write Operation in HDFS","text":"<p>When a client intends to write a file into HDFS, the very first step involves the client interacting with the NameNode. The NameNode is considered the \"centerpiece\" of the HDFS cluster because it stores all the metadata and possesses complete information about the entire cluster's data and its slave nodes (DataNodes). Therefore, any write operation must begin with a create request sent from the client to the File System API, which then forwards it to the NameNode. The NameNode's initial role is to check for access rights to ensure that the specific user or users have permission to write to the requested path.</p> <p>Once access rights are verified, the NameNode's crucial function is to provide the address of the slave (DataNode) where the client should begin writing the data directly. It's a significant point to understand that the client sends only one copy of the data. This is a key design choice because if the client were to send multiple copies (e.g., three copies for a replication factor of three), it would create significant network overhead. For instance, writing 10 TB of data would necessitate sending 30 TB over the network, which is inefficient.</p> <p>After the client starts writing the data directly to the initial DataNode via an FS Data Output Stream, the replication process begins among the DataNodes themselves, not initiated by the client. This process follows a data write pipeline: once the first DataNode has received and started writing a block, it immediately starts copying that block to another DataNode. This second DataNode, upon receiving the block, in turn starts copying it to a third DataNode, and so on, until the required replication level is achieved. For example, if the replication factor is three, the block will be written to DataNode 1, then DataNode 1 will copy to DataNode 3, and DataNode 3 will copy to DataNode 7. All decisions regarding which DataNodes handle the replication are taken care of by the NameNode (the master). DataNodes are in constant communication with the NameNode and report block information.</p> <p>Once the required replicas are created, an acknowledgement process takes place in reverse order. The last DataNode in the pipeline sends an acknowledgement to the second-to-last DataNode, which then sends it to the first DataNode. Finally, the first DataNode sends the ultimate acknowledgement back to the client.</p> <p>An important aspect of HDFS write operations is that they occur in parallel. It's not a serialized process where block one is fully written before block two begins. While write operations are generally costly, HDFS's parallel writing prevents them from being prohibitively slow, ensuring that writing terabytes of data doesn't take days. Furthermore, HDFS is designed to automatically handle failures during writing. If any DataNode goes down while data is being written, the NameNode will immediately provide the address of another DataNode where the data can be copied, ensuring data integrity and availability.</p> <p>A common question is who divides the large file into smaller blocks. The responsibility for dividing the file into smaller blocks falls to the HDFS client itself (also referred to as the Hadoop client). The user or their specific machine does not need to manually divide or specify any logic for splitting the file. The Hadoop setup itself acts as the Hadoop client, containing all the necessary APIs to perform this division automatically. You, as the user, are the client, but the actual HDFS setup on your machine acts as the 'Hadoop client' that performs the block division.</p>"},{"location":"hadoop/HDFS/#read-operation-in-hdfs","title":"Read Operation in HDFS","text":"<p>When a client wishes to read a file from HDFS, the initial and crucial step involves the client interacting with the NameNode. The NameNode is recognized as the \"centerpiece\" of the HDFS cluster because it is responsible for storing all the metadata related to files and their block locations across the various DataNodes. Before providing any information, the NameNode first checks for access rights to ensure that the particular user or client is authorized to read the requested file.</p> <p>Once access rights are verified, the NameNode's primary role shifts to providing the specific locations of the data blocks. It will furnish the client with the addresses of the DataNodes where each block of the file is actually stored. For example, it might instruct the client to \"go on slave two to read block one,\" \"go on slave ten to read block two,\" and so on. A very important distinction in HDFS is that the client then directly interacts with these respective DataNodes to read the file, and the NameNode is not involved in the actual data transfer during the read process.</p> <p>The reason for this direct client-DataNode interaction, bypassing the NameNode for data flow, is critical: the NameNode would become a severe bottleneck if all data had to pass through it. Considering that HDFS deals with data in the range of petabytes and operates across thousands of nodes, routing all read operations through a single NameNode would make it incredibly inefficient and slow. Therefore, the read operation itself is distributed and performed in parallel directly from the DataNodes, which significantly enhances efficiency. For instance, one client might be reading a block from DataNode One, while another client simultaneously reads a different block from DataNode Three, or even the same client reads different blocks from different DataNodes in parallel.</p> <p>To ensure security during this direct interaction, the NameNode doesn't just give out DataNode addresses freely. It also provides a token to the client. This token acts like an \"ID card\"; the client must show this token to the DataNode for authentication before the DataNode grants access to the data. This mechanism prevents unauthorized access to the data stored on the DataNodes.</p> <p>At a deeper, API level, when an end-user initiates a read operation on their client machine, a Java Virtual Machine (JVM) starts. The very first API class to come into play is the <code>HDFS client class</code>. This class sends an \"open request\" to the <code>Distributed File System API</code>. This <code>File System API</code> then interacts directly with the NameNode to request the block locations. After the NameNode performs its authorization and authentication checks, it provides the necessary block locations. Following this, the client uses an <code>FS Data Input Stream</code> \u2013 a standard Java API specifically adapted for Hadoop \u2013 to start directly reading the data from the DataNodes.</p> <p>Finally, HDFS is designed with robust fault tolerance during read operations. If, at any point while reading data, a DataNode goes down (e.g., due to a server failure or power loss), there are no issues. The client will immediately complain to the NameNode about the problem. The NameNode, which constantly monitors its DataNodes through heartbeats, will detect that the particular slave has stopped responding. In such a scenario, the NameNode will then provide the location of another DataNode where the same block of data is available. This ensures that the client can continue reading the file seamlessly without data loss or interruption.</p>"},{"location":"hadoop/hadoopiq/","title":"Hadoop","text":"<p>Q.1- What are the main components of a Hadoop Application?</p> <p>Over time, there are various forms in which a Hadoop application is defined. But in most of the cases there are following four core components of Hadoop application:</p> <p>\u25cf   HDFS: This is the file system in which Hadoop data is stored. It is a distributed file system with very high bandwidth.</p> <p>\u25cf   Hadoop Common_: This is a common set of libraries and utilities used by Hadoop.</p> <p>\u25cf   Hadoop MapReduce: This is based on the MapReduce algorithm for providing large-scale data processing.</p> <p>\u25cf   Hadoop YARN: This is used for resource management in a Hadoop cluster. It can also schedule tasks for users.</p> <p>Q.2- What is the core concept behind Apache Hadoop framework?</p> <p>Apache Hadoop is based on the concept of MapReduce algorithm. In the MapReduce algorithm, Map and Reduce operations are used to process very large data sets. In this concept, the Map method does the filtering and sorting of data. Reduce method performs the summarising of data. This is a concept from functional programming. The key points in this concept are scalability and fault tolerance. In Apache Hadoop these features are achieved by multi-threading and efficient implementation of MapReduce.</p> <p>Q.3- What is Hadoop Streaming?</p> <p>Hadoop distribution provides a Java utility called Hadoop Streaming. It is packaged in a jar file. With Hadoop Streaming, we can create and run Map Reduce jobs with an executable script. We can create executable scripts for Mapper and Reducer functions. These executable scripts are passed to Hadoop Streaming in a command. Hadoop Streaming utility creates Map and Reduce jobs and submits these to a cluster. We can also monitor these jobs with this utility.</p> <p>Q.4- What is the difference between Nodes in HDFS?</p> <p>The differences between NameNode, BackupNode and Checkpoint NameNode are as follows:</p> <p>\u25cf   NameNode: NameNode is at the heart of the HDFS file system that manages the metadata i.e. the data of the files is not stored on the NameNode but rather it has the directory tree of all the files present in the HDFS file system on a Hadoop cluster. NameNode uses two files for the namespace: fsimage file: This file keeps track of the latest checkpoint of the namespace. edits file: This is a log of changes made to the namespace since checkpoint.</p> <p>\u25cf   Checkpoint Node:    Checkpoint Node keeps track of the latest checkpoint in a directory that has the same structure as that of NameNode\u2019s directory. Checkpoint node creates checkpoints for the namespace at regular intervals by downloading the edits and fsimage file from the NameNode and merging it locally. The new image is then again updated back to the active NameNode.</p> <p>\u25cf   BackupNode: This node also provides check pointing functionality like that of the Checkpoint node but it also maintains its up-to-date in-memory copy of the file system namespace that is in sync with the active NameNode.</p> <p>Q.5- What is the optimum hardware configuration to run Apache Hadoop?</p> <p>To run Apache Hadoop jobs, it is recommended to use dual core machines or dual processors. There should be 4GB or 8GB RAM with the processor with Error-correcting code (ECC) memory. Without ECC memory, there is a high chance of getting checksum errors. For storage high capacity SATA drives (around 7200 rpm) should be used in Hadoop clusters. Around 10GB bandwidth Ethernet networks are good for Hadoop.</p> <p>Q.6- What do you know about Block and Block scanners in HDFS?</p> <p>A large file in HDFS is broken into multiple parts and each part is stored on a different Block. By default a Block is of 64 MB capacity in HDFS. Block Scanner is a program that every Data node in HDFS runs periodically to verify the checksum of every block stored on the data node. The purpose of a Block Scanner is to detect any data corruption errors on a Data node.</p> <p>Q.7- What are the default port numbers on which Nodes run in Hadoop?</p> <p>Default port numbers of Name Node, Job Tracker and Task Tracker are as follows: NameNode runs on port 50070 Task Tracker runs on port 50060 Job Tracker runs on port 50030</p> <p>Q.8- How will you disable a Block Scanner on HDFS DataNode?</p> <p>In HDFS, there is a configuration dfs.datanode.scan.period.hours in hdfs-site.xml to set the number of hours interval at which Block Scanner should run. We can set dfs.datanode.scan.period.hours=0 to disable the Block Scanner. It means it will not run on HDFS DataNode.</p> <p>Q.9- How will you get the distance between two nodes in Apache Hadoop?</p> <p>In Apache Hadoop we can use the NetworkTopology.getDistance() method to get the distance between two nodes. Distance from a node to its parent is considered as 1.</p> <p>Q.10- Why do we use commodity hardware in Hadoop?</p> <p>Hadoop does not require a very high-end server with large memory and processing power. Due to this we can use any inexpensive system with average RAM and processor. Such a system is called commodity hardware. Since there is parallel processing in Hadoop MapReduce, it is convenient to distribute a task among multiple servers and then do the execution. It saves cost as well as it is much faster compared to other options. Another benefit of using commodity hardware in Hadoop is scalability. Commodity hardware is readily available in the market. Whenever we need to scale up our operations in a Hadoop cluster we can obtain more commodity hardware. In the case of high-end machines, we have to raise purchase orders and get them built on demand.</p> <p>Q.11- How does inter cluster data copying work in Hadoop?</p> <p>In Hadoop, there is a utility called DistCP (Distributed Copy) to perform large inter/intra-cluster copying of data. This utility is also based on MapReduce. It creates Map tasks for files given as input. After every copy using DistCP, it is recommended to run cross checks to confirm that there is no data corruption and the copy is complete.</p> <p>Q.12- How can we update a file at an arbitrary location in HDFS?</p> <p>In HDFS, it is not allowed to update a file at an arbitrary location. All the files are written in append only mode. It means all writes are done at the end of a file. So there is no possibility of updating the files at any random location.</p> <p>Q.13- What is the Replication factor in HDFS?</p> <p>Replication factor in HDFS is the number of copies of a file in a file system. A Hadoop application can specify the number of replicas of a file it wants HDFS to maintain. This information is stored in NameNode. We can set the replication factor in following ways:</p> <p>We can use Hadoop fs shell, to specify the replication factor for a file. Command as follows: \u25cf   $hadoop fs \u2013setrep \u2013w 5 /file_name In the above command, the replication factor of file_name file is set as 5.</p> <p>We can also use Hadoop fs shell, to specify the replication factor of all the files in a directory. \u25cf   $hadoop fs \u2013setrep \u2013w 2 /dir_name In the above command, the replication factor of all the files under directory dir_name is set as 2.</p> <p>Q.14- What is the difference between NAS and DAS in a Hadoop cluster?</p> <p>NAS stands for Network Attached Storage and DAS stands for Direct Attached Storage. In NAS, compute and storage layers are separated. Storage is distributed over different servers on a network. In DAS, storage is attached to the node where computation takes place. Apache Hadoop is based on the principle of moving processing near the location of data. So it needs a storage disk to be local to computation. With DAS, we get very good performance on a Hadoop cluster. Also DAS can be implemented on commodity hardware. So it is more cost effective. Only when we have very high bandwidth (around 10 GbE) it is preferable to use NAS storage.</p> <p>Q.15- What are the two messages that NameNode receives from DataNode?</p> <p>NameNode receives following two messages from every DataNode:</p> <p>\u25cf       Heartbeat: This message signals that DataNode is still alive. Periodic receipt of Heartbeat is very important for NameNode to decide whether to use a DataNode or not.</p> <p>\u25cf       Block Report: This is a list of all the data blocks hosted on a DataNode. This report is also very useful for the functioning of NameNode. With this report, NameNode gets information about what data is stored on a specific DataNode.</p> <p>Q.16- How does indexing work in Hadoop?</p> <p>Indexing in Hadoop has two different levels. Index based on File URI: In this case data is indexed based on different files. When we search for data, the index will return the files that contain the data. Index based on InputSplit: In this case, data is indexed based on locations where input split is located.</p> <p>Q.17- What data is stored in a HDFS NameNode?</p> <p>NameNode is the central node of an HDFS system. It does not store any actual data on which MapReduce operations have to be done. But it has all the metadata about the data stored in HDFS DataNodes. NameNode has the directory tree of all the files in the HDFS filesystem. Using this metadata it manages all the data stored in different DataNodes.</p> <p>Q.18- What would happen if NameNode crashes in a HDFS cluster?</p> <p>There is only one NameNode in a HDFS cluster. This node maintains metadata about DataNodes. Since there is only one NameNode, it is the single point of failure in a HDFS cluster. When NameNode crashes, the system may become unavailable. We can specify a secondary NameNode in the HDFS cluster. The secondary NameNode takes the periodic checkpoints of the file system in HDFS. But it is not the backup of NameNode. We can use it to recreate the NameNode and restart it in case of a crash.</p> <p>Q.19- What are the main functions of Secondary NameNode?</p> <p>Main functions of Secondary NameNode are as follows:</p> <p>\u25cf   FsImage: It stores a copy of the FsImage file and EditLog.</p> <p>\u25cf   NameNode crash: In case NameNode crashes, we can use Secondary NameNode's FsImage to recreate the NameNode.</p> <p>\u25cf   Checkpoint: Secondary NameNode runs Checkpoint to confirm that data is not corrupt in HDFS.</p> <p>Update: It periodically applies the updates from EditLog to the FsImage file. In this way the FsImage file on Secondary NameNode is kept up to date. This helps in saving time during NameNode restart.</p> <p>Q.20- What happens if an HDFS file is set with a replication factor of 1 and DataNode crashes?</p> <p>Replication factor is the same as the number of copies of a file on HDFS. If we set the replication factor of 1, it means there is only 1 copy of the file. In case, DataNode that has this copy of file crashes, the data is lost. There is no way to recover it. It is essential to keep a replication factor of more than 1 for any business critical data.</p> <p>Q.21- What is the meaning of Rack Awareness in Hadoop?</p> <p>In Hadoop, most of the components like NameNode, DataNode etc are rack- aware. It means they have the information about the rack on which they exist. The main use of rack awareness is in implementing fault-tolerance. Any communication between nodes on the same rack is much faster than the communication between nodes on two different racks. In Hadoop, NameNode maintains information about the rack of each DataNode. While reading/writing data, NameNode tries to choose the DataNodes that are closer to each other. Due to performance reasons, it is recommended to use close data nodes for any operation. So Rack Awareness is an important concept for high performance and fault- tolerance in Hadoop. If we set Replication factor 3 for a file, does it mean any computation will also take place 3 times?\" No. Replication factor of 3 means that there are 3 copies of a file. But computation takes place only one copy of the file. If the node on which the first copy exists does not respond then computation will be done on the second copy.</p> <p>Q.22- How will you check if a file exists in HDFS?</p> <p>In Hadoop, we can run hadoop fs command with option e to check the existence of a file in HDFS. This is generally used for testing purposes. Command will be as follows: + %&gt;hadoop fs -test -ezd file_uri e is for checking the existence of file z is for checking non-zero size of File d is for checking if the path is directory</p> <p>Q.23- Why do we use the fsck command in HDFS?</p> <p>fsck command is used for getting the details of files and directories in HDFS. Main uses of fsck command in HDFS are as follows: \u25cf   delete: We use this option to delete files in HDFS.</p> <p>\u25cf   move: This option is for moving corrupt files to lost/found.</p> <p>\u25cf   locations: This option prints all the locations of a block in HDFS.</p> <p>\u25cf   racks: This option gives the network topology of data-node locations.</p> <p>\u25cf   blocks: This option gives the report of blocks in HDFS.</p> <p>Q.24- What will happen when NameNode is down and a user submits a new job?</p> <p>Since NameNode is the single point of failure in Hadoop, user jobs cannot execute. The job will fail when the NameNode is down. Users will have to wait for NameNode to restart and come up, before running a job.</p> <p>Q.25- What are the core methods of a Reducer in Hadoop?</p> <p>The main task of Reducer is to reduce a larger set of data that shares a key to a smaller set of data. In Hadoop, Reducer has following three core methods:</p> <p>\u25cf   setup(): At the start of a task, the setup() method is called to configure various parameters for Reducer.</p> <p>\u25cf   reduce(): This is the main operation of Reducer. In the reduce() method we define the task that has to be done for a set of values that share a key.</p> <p>\u25cf   cleanup(): Once the reduce() task is done, we can use cleanup() to clean any intermediate data or temporary files.</p> <p>Q.26- What are the primary phases of a Reducer in Hadoop?</p> <p>In Hadoop, there are three primary phases of a Reducer:</p> <p>\u25cf   Shuffle: In this phase, Reducer copies the sorted output from each Mapper.</p> <p>\u25cf   Sort: In this phase, Hadoop framework sorts the input to Reducer by the same key. It uses merge sort in this phase. Sometimes, shuffle and sort phases occur at the same time.</p> <p>\u25cf   Reduce: This is the phase in which output values associated with a key are reduced to give output results. Output from Reducer is not re-sorted.</p> <p>Q.27- What is the use of Context objects in Hadoop?</p> <p>Hadoop uses Context objects with Mapper to interact with the rest of the system. Context object gets the configuration of the system and job in its constructor. We use Context objects to pass the information in setup(), cleanup() and map() methods. This is an important object that makes the important information available during the map operations.</p> <p>Q.28- How does partitioning work in Hadoop?</p> <p>Partitioning is the phase between Map phase and Reduce phase in Hadoop workflow. Since the partitioner gives output to Reducer, the number of partitions is the same as the number of Reducers. Partitioner will partition the output from Map phase into distinct partitions by using a user-defined condition. Partitions can be like Hash based buckets. E.g. If we have to find the student with the maximum marks in each gender in each subject. We can first use the Map function to map the keys with each gender. Once mapping is done, the result is passed to the Partitioner. Partitioner will partition each row with gender on the basis of subject. For each subject there will be a different Reducer. Reducer will take input from each partition and find the student with the highest marks.</p> <p>Q.29- What is a Combiner in Hadoop?</p> <p>Combiner is an optional step between Map and Reduce. Combiner is also called Semi-Reducer. Combiner takes output from Map, creates Key-value pairs and passes these to Reducer. Combiner's task is to summarise the outputs from Map into summary records with the same key. By using Combiner, we can reduce the data transfer between Mapper and Reducer. Combiner does the task similar to reduce but it is done on the Map machine itself.</p> <p>Q.30- What is the default replication factor in HDFS?</p> <p>Default replication factor in HDFS is 3. It means there will be 3 copies of each data. We can configure it with dfs.replication in the hdfs-site.xml file. We can even set it from the command line in Hadoop fs command.</p> <p>Q.31- How much storage is allocated by HDFS for storing a file of 25 MB size?</p> <p>In HDFS, all the data is stored in blocks. The size of the block can be configured in HDFS.In Apache Hadoop, the default block size is 64 MB. To store a file of 25 MB size, at least one block will be allocated. This means at least 64 MB will be allocated for the file of 25 MB size.</p> <p>Q.32- Why does HDFS store data in Block structure?</p> <p>HDFS stores all the data in terms of Blocks. With Block structure there are some benefits that HDFS gets. Some of these are as follows: Fault Tolerance: With Block structure, HDFS implements replication. By replicating the same block in multiple locations, fault tolerance of the system increases. Even if some copy is not accessible, we can get the data from another copy. Large Files: We can store very large files that cannot be even stored on one disk, in HDFS by using Block structure. We just divide the data of the file in multiple Blocks. Each Block can be stored on the same or different machines. Storage management: With Block storage it is easier for Hadoop nodes to calculate the data storage as well as perform optimization in the algorithm to minimise data transfer across the network.</p> <p>Q.33- How will you create a custom Partitioner in a Hadoop job?</p> <p>Partition phase runs between Map and Reduce phase. It is an optional phase. We can create a custom partitioner by extending the org.apache.hadoop.mapreduce.Partitio class in Hadoop. In this class, we have to override the getPartition(KEY key, VALUE value, int numPartitions) method. This method takes three inputs. In this method, numPartitions is the same as the number of reducers in our job. We pass key and value to get the partition number to which this key,value record will be assigned. There will be a reducer corresponding to that partition. The reducer will further handle summarizing of the data.Once the custom Partitioner class is ready, we have to set it in the Hadoop job. We can use following method to set it: job.setPartitionerClass(CustomPartitioner)</p> <p>Q.34- What is a Checkpoint node in HDFS?</p> <p>A Checkpoint node in HDFS periodically fetches fsimage and edits from NameNode, and merges them. This merge result is called a Checkpoint. Once a Checkpoint is created, Checkpoint Node uploads the Checkpoint to NameNode. Secondary nodes also take a Checkpoint similar to Checkpoint Node. But it does not upload the Checkpoint to NameNode. Main benefit of Checkpoint Node is in case of any failure on NameNode. A NameNode does not merge its edits to fsimage automatically during the runtime. If we have a long running task, the edits will become huge. When we restart NameNode, it will take much longer time, because it will first merge the edits. In such a scenario, a Checkpoint node helps for a long running task. Checkpoint nodes performs the task of merging the edits with fsimage and then uploads these to NameNode. This saves time during the restart of NameNode.</p> <p>Q.35- What is a Backup Node in HDFS?</p> <p>Backup Node in HDFS is similar to Checkpoint Node. It takes the stream of edits from NameNode. It keeps these edits in memory and also writes these to storage to create a new checkpoint. At any point of time, Backup Node is in sync with the Name Node. The difference between Checkpoint Node and Backup Node is that Backup Node does not upload any checkpoints to Name Node. Also Backup node takes a stream instead of periodic reading of edits from Name Node.</p> <p>Q.36- What is the meaning of the term Data Locality in Hadoop?</p> <p>In a Big Data system, the size of data is huge. So it does not make sense to move data across the network. In such a scenario, Hadoop tries to move computation closer to data. So the Data remains local to the location wherever it was stored. But the computation tasks will be moved to data nodes that hold the data locally. Hadoop follows following rules for Data Locality optimization: \u25cf   Hadoop first tries to schedule the task on a node that has an HDFS file on a local disk. If it cannot be done, then Hadoop will try to schedule the task on a node on the same rack as the node that has data. If this also cannot be done, Hadoop will schedule the task on the node with the same data on a different rack. The above method works well, when we work with the default replication factor of 3 in Hadoop.</p> <p>Q.37- What is a Balancer in HDFS?</p> <p>In HDFS, data is stored in blocks on a DataNode. There can be a situation when data is not uniformly spread into blocks on a DataNode. When we add a new DataNode to a cluster, we can face such a situation. In such a case, HDFS provides a useful tool Balancer to analyze the placement of blocks on a DataNode. Some people call it a Rebalancer also. This is an administrative tool used by admin staff. We can use this tool to spread the blocks in a uniform manner on a DataNode.</p> <p>Q.38- What are the important points a NameNode considers before selecting the DataNode for placing a data block?</p> <p>Some of the important points for selecting a DataNode by NameNode are as follows: NameNode tries to keep at least one replica of a Block on the same node that is writing the block. It tries to spread the different replicas of the same block on different racks, so that in case of one rack failure, another rack has the data. One replica will be kept on a node on the same node as the one that is writing it. It is different from point 1. In Point 1, a block is written to the same node. At this point the block is written on a different node on the same rack. This is important for minimizing the network I/O. NameNode also tries to spread the blocks uniformly among all the DataNodes in a cluster.</p> <p>Q.39- What is Safemode in HDFS?</p> <p>Safemode is considered as the read-only mode of NameNode in a cluster. During the startup of NameNode, it was in SafeMode. It does not allow writing to the file-system in Safemode. At this time, it collects data and statistics from all the DataNodes. Once it has all the data on blocks, it leaves Safemode. The main reason for Safemode is to avoid the situation when NameNode starts replicating data in DataNodes before collecting all the information from DataNodes. It may erroneously assume that a block is not replicated well enough, whereas, the issue is that NameNode does not know about the whereabouts of all the replicas of a block. Therefore, in Safemode, NameNode first collects the information about how many replicas exist in a cluster and then tries to create replicas wherever the number of replicas is less than the policy.</p> <p>Q.40- How will you replace HDFS data volume before shutting down a DataNode?</p> <p>In HDFS, DataNode supports hot swappable drives. With a swappable drive we can add or replace HDFS data volumes while the DataNode is still running. The procedure for replacing a hot swappable drive is as follows: First we format and mount the new drive. We update the DataNode configuration dfs.datanode.data.dir to reflect the data volume directories. Run the \"dfsadmin -reconfig datanode HOST:PORT start\" command to start the reconfiguration process Once the reconfiguration is complete, we just unmount the old data volume After unmount we can physically remove the old disks.</p> <p>Q.41- What are the important configuration files in Hadoop?</p> <p>There are two important configuration files in a Hadoop cluster:</p> <p>\u25cf   Default Configuration: There are core-default.xml, hdfs-default.xml and mapred-default.xml files in which we specify the default configuration for Hadoop cluster. These are read only files.</p> <p>\u25cf   Custom Configuration: We have site-specific custom files like core-site.xml, hdfs-site.xml, mapred-site.xml in which we can specify the site-specific configuration.</p> <p>All the Jobs in Hadoop and HDFS implementation uses the parameters defined in the above-mentioned files. With customization we can tune these processes according to our use case. In Hadoop API, there is a Configuration class that loads these files and provides the values at run time to different jobs.</p> <p>Q.42- How will you monitor memory used in a Hadoop cluster?</p> <p>In Hadoop, TaskTracker is the one that uses high memory to perform a task. We can configure the TastTracker to monitor memory usage of the tasks it creates. It can monitor the memory usage to find the badly behaving tasks, so that these tasks do not bring the machine down with excess memory consumption. In memory monitoring we can also limit the maximum memory used by a task. We can even limit the memory usage per node. So that all the tasks executing together on a node do not consume more memory than a limit. Some of the parameters for setting memory monitoring in Hadoop are as follows: mapred.cluster.map.memory.mb, mapred.cluster.reduce.memory.mb: This is the size of virtual memory of a single map/reduce slot in a cluster of Map-Reduce framework. mapred.job.map.memory.mb, mapred.job.reduce.memory.mb: This is the default limit of memory set on each map/reduce task in Hadoop. mapred.cluster.max.map.memory.m mapred.cluster.max.reduce.memory This is the maximum limit of memory set on each map/reduce task in Hadoop.</p> <p>Q.43- Why do we need Serialization in Hadoop map reduce methods?</p> <p>In Hadoop, there are multiple data nodes that hold data. During the processing of map and reduce methods data may transfer from one node to another node. Hadoop uses serialization to convert the data from Object structure to Binary format. With serialization, data can be converted to binary format and with de-serialization data can be converted back to Object format with reliability.</p> <p>Q.44- What is the use of Distributed Cache in Hadoop?</p> <p>Hadoop provides a utility called Distributed Cache to improve the performance of jobs by caching the files used by applications. An application can specify which file it wants to cache by using JobConf configuration. Hadoop framework copies these files to the nodes one which a task has to be executed. This is done before the start of execution of a task. DistributedCache supports distribution of simple read only text files as well as complex files like jars, zips etc.</p> <p>Q.45- How will you synchronize the changes made to a file in Distributed Cache in Hadoop?</p> <p>In Distributed Cache, it is not allowed to make any changes to a file. This is a mechanism to cache read-only data across multiple nodes.Therefore, it is not possible to update a cached file or run any synchronization in Distributed Cache.</p> <p>Q.46- Can you elaborate about the Mapreduce job?</p> <p>Based on the configuration, the MapReduce Job first splits the input data into independent chunks called Blocks. These blocks are processed by Map() and Reduce() functions. First Map function processes the data, then processed by reduce function. The Framework takes care of sorting the Map outputs, scheduling the tasks.</p> <p>Q.47- Why compute nodes and storage nodes are the same?</p> <p>Compute nodes for processing the data, Storage nodes for storing the data. By default Hadoop framework tries to minimize the network wastage, to achieve that goal Framework follows the Data locality concept. The Compute code executes where the data is stored, so the data node and compute node are the same.</p> <p>Q.48- What is the configuration object importance in Mapreduce?</p> <p>It\u2019s used to set/get of parameter name &amp; value pairs in XML file.It\u2019s used to initialize values, read from external file and set as a value parameter.Parameter values in the program always overwrite with new values which are coming from external configure files.Parameter values received from Hadoop\u2019s default values.</p> <p>Q.49- Where Mapreduce is not recommended?</p> <p>Mapreduce is not recommended for Iterative kind of processing. It means repeating the output in a loop manner.To process Series of Mapreduce jobs, MapReduce is not suitable. Each job persists data in a local disk, then again loads to another job. It's a costly operation and not recommended.</p> <p>Q.50- What is Namenode and its responsibilities?</p> <p>Namenode is a logical daemon name for a particular node. It's the heart of the entire Hadoop system. Which store the metadata in FsImage and get all block information in the form of Heartbeat.</p> <p>Q.51- What is Jobtracker\u2019s responsibility?</p> <p>Scheduling the job\u2019s tasks on the slaves. Slaves execute the tasks as directed by the JobTracker. Monitoring the tasks, if failed, re-execute the failed tasks.</p> <p>Q.52- What are Jobtracker and Tasktracker?</p> <p>MapReduce Framework consists of a single Job Tracker per Cluster, one Task Tracker per node. Usually A cluster has multiple nodes, so each cluster has a single Job Tracker and multiple TaskTrackers.JobTracker can schedule the job and monitor the Task Trackers. If Task Tracker failed to execute tasks, try to re-execute the failed tasks. TaskTracker follows the JobTracker\u2019s instructions and executes the tasks. As a slave node, it reports the job status to Master JobTracker in the form of Heartbeat.</p> <p>Q.53- What is Job scheduling importance in Hadoop Mapreduce?</p> <p>Scheduling is a systematic procedure of allocating resources in the best possible way among multiple tasks. Hadoop task trackers performing many procedures, sometimes a particular procedure should finish  !!!- info \"Quickly and provide more priority, to do it few job schedulers come into the picture. Default Schedule is FIFO. Fair scheduling, FIFO and CapacityScheduler are the most popular hadoop scheduling in hadoop.</p> <p>Q.54- When to use a Reducer?</p> <p>To combine multiple mapper\u2019s output use a reducer. Reducer has 3 primary phases: sort, shuffle and reduce. It\u2019s possible to process data without a reducer, but used when the shuffle and sort is required</p> <p>Q.55- Where does the Shuffle and Sort process go?</p> <p>After Mapper generates the output, it temporarily stores the intermediate data on the local File System. Usually this temporary file is configured at coresite.xml in the Hadoop file. Hadoop Framework aggregate and sort this intermediate data, then update into Hadoop to be processed by the Reduce function. The Framework deletes this temporary data in the local system after Hadoop completes the job.</p> <p>Q.56- Java is mandatory to write Mapreduce Jobs?</p> <p>No, By default Hadoop is implemented in JavaTM, but MapReduce applications need not be written in Java. Hadoop supports Python, Ruby, C++ and other Programming languages. Hadoop Streaming API allows to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer.Hadoop Pipes allows programmers to implement MapReduce applications by using C++ programs.</p> <p>Q.57- What methods can control the Map And Reduce function\u2019s output?</p> <p>setOutputKeyClass() and setOutputValueClass() If they are different, then the map output type can be set using the methods. setMapOutputKeyClass() and setMapOutputValueClass()</p> <p>Q.58- What is the main difference between Mapper And Reducer?</p> <p>Map method is called separately for each key/value that has been processed. It processes input key/value pairs and emits intermediate key/value pairs. The Reduce method is called separately for each key/values list pair. It processes intermediate key/value pairs and emits final key/value pairs. Both are initialised and called before any other method is called. Both don\u2019t have any parameters and no output.</p> <p>Q.59- Why compute Nodes and Storage Nodes are same?</p> <p>Compute nodes are logical processing units, Storage nodes are physical storage units (Nodes). Both are running in the same node because of the \u201cdata locality\u201d issue. As a result Hadoop minimize the data network wastage and allows it to process Quickly.</p> <p>Q.60- What is the difference between map side join and reduce side join?</p> <p>Join multiple tables on the mapper side, called map side join. Please note map side join should have a strict format and be sorted properly. If the dataset is smaller tables, go through reducer phrases. Data should be partitioned properly. Join the multiple tables in the reducer side called reduce side join. If you have a large amount of data tables, plan to join both tables. One table has a large amount of rows and columns, another one has a few tables only, goes through Reduce side join. It\u2019s the best way to join the multiple tables</p> <p>Q.61- What happens if the number of Reducer is 0?</p> <p>Number of reducers = 0 also valid configuration in MapReduce. In this scenario, No reducer will execute, so mapper output is considered as output, Hadoop stores this information in a separate folder.</p> <p>Q.62- When will we go to Combiner?</p> <p>Mappers and reducers are independent; they don't talk to each other. When the functions that are commutative(a.b = b.a) and associative {a.(b.c) = (a.b).c} we go to the combiner to optimize the mapreduce process. Many mapreduce jobs are limited by the bandwidth, so by default Hadoop framework minimizes the data bandwidth network wastage. To achieve it\u2019s goal, Mapreduce allows user defined \u201cCominer function\u201d to run on the map output. It\u2019s an MapReduce optimization techni !!!- info \"Que, but it\u2019s optional.</p> <p>Q.63- What is the main difference between Mapreduce Combiner and Reducer?</p> <p>Both Combiner and Reducer are optional, but most frequently used in MapReduce. There are three main differences such as: combiner will get only one input from one Mapper. While Reducer will get multiple mappers from different mappers. If aggregation requires a used reducer, but if the function follows commutative (a.b=b.a) and associative a.(b.c)= (a.b).c law, use combiner. Input and output keys and values types must be the same in the combiner, but the reducer can follow any type input, any output format.</p> <p>Q.64- What is a partition?</p> <p>After combiner and intermediate mapoutput the Partitioner controls the keys after sort and shuffle. Partitioner divides the intermediate data according to the number of reducers so that all the data in a single partition gets executed by a single reducer. It means each partition can be executed by only a single reducer. If you call reducer, the partition is called automatically.</p> <p>Q.65- When will we go to Partition?</p> <p>By default Hive reads the entire dataset even if the application has a slice of data. It\u2019s a bottleneck for mapreduce jobs. So Hive allows special options called partitions. When you are creating a table, hive partitioning the table based on requirement.</p> <p>Q.66- What are the important steps when you are partitioning a table?</p> <p>Don\u2019t over partition the data with too small partitions, it\u2019s overhead to the namenode. if dynamic partition, at least one static partition should exist and set to strict mode by using given commands. \u25cf   SET hive.exec.dynamic.partition = true; \u25cf   SET hive.exec.dynamic.partition.mode = nonstrict;</p> <p>first load data into nonpartitioned table, then load such data into a partitioned table. It\u2019s not possible to load data from local to partitioned tables. insert overwrite table table_name partition(year) select * from nonpartitiontable;</p> <p>Q.67- Can you elaborate Mapreduce Job architecture?</p> <p>First Hadoop programmer submit Mapreduce program to JobClient. Job Client requests the JobTracker to get Job id, Job tracker provides JobID, it's in the form of Job_HadoopStartedtime_00001. It's a unique ID. Once JobClient receives a Job ID, copy the Job resources (job.xml, job.jar) to File System (HDFS) and submit the job to JobTracker. JobTracker initiates jobs and schedules the job. Based on configuration, job split the input splits and submit to HDFS. TaskTracker retrieves the job resources from HDFS and launches the Child JVM. In this Child JVM, run the map and reduce tasks and notify to the Job tracker the job status.</p> <p>Q.68- Why does Task Tracker launch child Jvm?</p> <p>Most frequently, Hadoop developers mistakenly submit wrong jobs or have bugs. If Task Tracker uses an existing JVM, it may interrupt the main JVM, so other tasks may be influenced. Whereas child JVM if it\u2019s trying to damage existing resources, TaskTracker kills that child JVM and retry or relaunch new child JVM.</p> <p>Q.69- Why does Jobclient and Job Tracker submit job resources to the file system?</p> <p>Data locality. Moving competition is cheaper than moving Data. So logic/ competition in Jar file and splits. So Where the data is available, in File System Datanodes. So every resource copy where the data is available.</p> <p>Q.70- How many Mappers and Reducers can run?</p> <p>By default Hadoop can run 2 mappers and 2 reducers in one datanode. also each node has 2 map slots and 2 reducer slots. It\u2019s possible to change the default values in Mapreduce.xml in the conf file.</p> <p>Q.71- What is Inputsplit?</p> <p>A chunk of data processed by a single mapper called InputSplit. In other words, a logical chunk of data which is processed by a single mapper called Input split, by default inputSplit = block Size.</p> <p>Q.72- How to configure the split value?</p> <p>By default block size = 64mb, but to process the data, the job tracker split the data. Hadoop architects use these formulas to know split size. + split size = min (max_splitsize, max (block_size, min_split_size)); + split size = max(min_split_size, min (block_size, max_split, size));</p> <p>by default split size = block size. Always No of splits = No of mappers. Apply above formula: + split size = Min (max_splitsize, max (64, 512kB) // max _splitsize = depends on env, may 1gb or 10gb split size = min (10gb (let assume), 64) split size = 64MB. + split size = max(min_split_size, min (block_size, max_split, size)); split size = max (512kb, min (64, 10GB)); split size = max (512kb, 64);split size = 64 MB;</p> <p>Q.73- How much ram is required to process 64mb data?</p> <p>Leg Assuming. 64 block size, system take 2 mappers, 2 reducers, so 64*4 = 256 MB memory and OS take at least 30% extra space so at least 256 + 80 = 326MB Ram required to process a chunk of data.So in this way requires more memory to process an unstructured process.</p> <p>Q.74- What is the difference between block And split?</p> <p>\u25cf   Block: How much chunk data is stored in the memory called block. \u25cf   Split: how much data to process the data called split.</p> <p>Q.75- Why Hadoop Framework reads a file parallel, why not sequential?</p> <p>To retrieve data faster, Hadoop reads data parallel, the main reason it can access data faster. While, writes in sequence, but not parallel, the main reason it might result is that one node can be overwritten by another and where the second node. Parallel processing is independent, so there is no relation between two nodes. If you write data in parallel, it\u2019s not possible where the next chunk of data has. For example 100 MB data write parallel, 64 MB one block another block 36, if data writes parallel the first block doesn\u2019t know where the remaining data is. So Hadoop reads parallel and writes sequentially.</p> <p>Q.76- If I change block size from 64 to 128?</p> <p>Even if you have changed block size, it does not affect existing data. After changing the block size, every file chunked after 128 MB of block size. It means old data is in 64 MB chunks, but new data stored in 128 MB blocks.</p> <p>Q.77- What is Issplitable()?</p> <p>By default this value is true. It is used to split the data in the input format. if unstructured data, it\u2019s not recommendable to split the data, so process the entire file as a one split. to do it first change isSplitable() to false.</p> <p>Q.78- How much Hadoop allows maximum block size and minimum block size?</p> <p>\u25cf   Minimum: 512 bytes. It\u2019s local OS file system block size. No one can decrease fewer than block size. \u25cf   Maximum: Depends on the environment. There is no upper bound.</p> <p>Q.79- What are the Job Resource files?</p> <p>job.xml and job.jar are core resources to process the Job. Job Client copy the resources to the HDFS.</p> <p>Q.80- What\u2019s the Mapreduce Job consist of?</p> <p>MapReduce job is a unit of work that client wants to be performed. It consists of input data, MapReduce program in Jar file and configuration setting in XML files. Hadoop runs this job by dividing it in different tasks with the help of JobTracker</p> <p>Q.81- What is the data locality?</p> <p>Wherever the data is there process the data, computation/process the data where the data available, this process called data locality. \u201cMoving Computation is Cheaper than Moving Data '' to achieve this goal following data locality. It\u2019s possible when the data is splittable, by default it\u2019s true.</p> <p>Q.82- What is speculative execution?</p> <p>Hadoop runs the process in commodity hardware, so it\u2019s possible to fail if the system also has low memory. So if system failed, process also failed, it\u2019s not recommendable.Speculative execution is a process performance optimization technique.Computation/logic distribute to the multiple systems and execute which system execute quickly. By default this value is true. Now even if the system crashes, not a problem, the framework chooses logic from other systems. \u25cf   Eg: logic distributed on A, B, C, D systems, completed within a time.</p> <p>System A, System B, System C, System D systems executed 10 min, 8 mins, 9 mins 12 mins simultaneously. So consider system B and kill remaining system processes, framework take care to kill the other system process.</p> <p>Q.83- What is a chain Mapper?</p> <p>Chain mapper class is a special mapper class set which runs in a chain fashion within a single map task. It means, one mapper input acts as another mapper\u2019s input, in this way n number of mapper connected in chain fashion.</p> <p>Q.84- How to do value level comparison?</p> <p>Hadoop can process key level comparison only but not in the value level comparison.</p> <p>Q.85- What are the setup and clean up methods?</p> <p>If you don\u2019t know the starting and ending points/lines, it\u2019s much more difficult to solve those problems. Setup and clean up can resolve it. N number of blocks, by default 1 mapper called to each split. Each split has one start and clean up methods. N number of methods, number of lines. Setup is initialising job resources. The purpose of cleaning up is to close the job resources. The map processes the data. Once the last map is completed, cleanup is initialized. It Improves the data transfer performance. All these block size comparisons can be done in reducer as well. If you have any key and value, compare one key value to another key value. If you compare record levels, use these setup and cleanup. It opens once and processes many times and closes once. So it saves a lot of network wastage during the process.</p> <p>Q.86- How many slots are allocated for each task?</p> <p>By default each task has 2 slots for mapper and 2 slots for reducer. So each node has 4 slots to process the data.</p> <p>Q.87- Why does Tasktracker launch a child Jvm to do a task?</p> <p>Sometimes child threads corrupt parent threads. It means because of a programmer mistake, the MapReduce task disturbed. So task trackers launch a child JVM to process individual mappers or taskers. If tasktracker uses an existing JVM, it might damage the main JVM. If any bugs occur, tasktracker kill the child process and relaunch another child JVM to do the same task. Usually task tracker relaunch and retry the task 4 times.</p> <p>Q.88- What main configuration parameters are specified in Mapreduce?</p> <p>The MapReduce programmers need to specify following configuration parameters to perform the map and reduce jobs:</p> <p>\u25cf   The input location of the job in HDFs.</p> <p>\u25cf   The output location of the job in HDFS.</p> <p>\u25cf   The input and output\u2019s format.</p> <p>\u25cf   The classes contain map and reduce functions, respectively.</p> <p>\u25cf   The .jar file for mapper, reducer and driver classes</p> <p>Q.89- What is identity Mapper?</p> <p>Identity Mapper is the default Mapper class provided by Hadoop. when no other Mapper class is defined, Identify will be executed. It only writes the input data into output and does not perform computations and calculations on the input data. The class name is org.apache.hadoop.mapred.lib.IdentityMapper.</p> <p>Q.90- What is a RecordReader in MapReduce?</p> <p>RecordReader is used to read key/value pairs from the InputSplit by converting the byte-oriented view and presenting record-oriented view to Mapper.</p> <p>Q.91- What is OutputCommitter?</p> <p>OutPutCommitter describes the commit of MapReduce task. FileOutputCommitter is the default available class available for OutputCommitter in MapReduce. It performs the following operations: Create a temporary output directory for the job during initialization. Then, it cleans the job as it removes the temporary output directory post job completion. Set up the task temporary output. Identifies whether a task needs commitment. The commit is applied if required. JobSetup, JobCleanup and TaskCleanup are important tasks during the output commit.</p> <p>Q.92- What are the parameters of Mappers and Reducers?</p> <p>The four parameters for mappers are:</p> <p>\u25cf   LongWritable (input)</p> <p>\u25cf   text (input)</p> <p>\u25cf   text (intermediate output)</p> <p>\u25cf   IntWritable (intermediate output)</p> <p>The four parameters for reducers are: \u25cf   Text (intermediate output)</p> <p>\u25cf   IntWritable (intermediate output)</p> <p>\u25cf   Text (final output)</p> <p>\u25cf   IntWritable (final output)</p> <p>Q.93- Explain Jobconf in Mapreduce?</p> <p>It is a primary interface to define a map-reduce job in Hadoop for job execution. JobConf specifies mapper, Combiner, partitioner, Reducer,InputFormat , OutputFormat implementations and other advanced job faets like Comparators.</p> <p>Q.94- Explain Job scheduling through Jobtracker?</p> <p>JobTracker communicates with NameNode to identify data location and submits the work to TaskTracker node. The TaskTracker plays a major role as it notifies the JobTracker for any job failure. It actually refers to the heartbeat reporter reassuring the JobTracker that it is still alive. Later, the JobTracker is responsible for the actions as it may either resubmit the job or mark a specific record as unreliable or blacklist it.</p> <p>Q.95- What is SeqQuenceFileInputFormat?</p> <p>A compressed binary output file format to read in sequence files and extends the FileInputFormat.It passes data between output-input (between output of one MapReduce job to input of another MapReduce job)phases of MapReduce jobs.</p> <p>Q.96- Explain how the input and output data format of the Hadoop Framework?</p> <p>The MapReduce framework operates exclusively on pairs, that is, the framework views the input to the job as a set of pairs and produces a set of pairs as the output of the job, conceivably of different types. See the flow mentioned below: \u25cf   (input) -&gt; map -&gt; -&gt; combine/sorting -&gt; -&gt; reduce -&gt; (output)</p> <p>Q.97- What are the restrictions to the Key and Value Class?</p> <p>The key and value classes have to be serialized by the framework. To make them serializable Hadoop provides a Writable interface. As you know from the java itself that the key of the Map should be comparable, hence the key has to implement one more interface Writable Comparable.</p> <p>Q.98- Explain the wordcount implementation via Hadoop Framework?</p> <p>We will count the words in all the input file flow as below input Assume there are two files each having a sentence Hello World Hello World (In file 1) Hello World Hello World (In file 2) Mapper : There would be each mapper for the a file For the given sample input the first map output: &lt; Hello, 1&gt; &lt; World, 1&gt; &lt; Hello, 1&gt; &lt; World, 1&gt; The second map output: &lt; Hello, 1&gt; &lt; World, 1&gt; &lt; Hello, 1&gt; &lt; World, 1&gt; Combiner/Sorting (This is done for each individual map) So output looks like this The output of the first map: &lt; Hello, 2&gt; &lt; World, 2&gt; The output of the second map: &lt; Hello, 2&gt; &lt; World, 2&gt; Reducer : It sums up the above output and generates the output as below &lt; Hello, 4&gt; &lt; World, 4&gt;</p> <p>Output Final output would look like Hello 4 times World 4 times</p> <p>Q.98- How Mapper is instantiated in a running Job?</p> <p>The Mapper itself is instantiated in the running job, and will be passed a MapContext object which it can use to configure itself.</p> <p>Q.99- Which are the methods in the Mapper Interface?</p> <p>The Mapper contains the run() method, which calls its own setup() method only once, it also calls a map() method for each input and finally calls it cleanup() method. All above methods you can override in your code.</p> <p>Q.100- What happens if You don't Override the Mapper methods and keep them as it is?</p> <p>If you do not override any methods (leaving even map as-is), it will act as the identity function, emitting each input record as a separate output.</p> <p>Q.101- What is the use of context objects?</p> <p>The Context object allows the mapper to interact with the rest of the Hadoop system. It Includes configuration data for the job, as well as interfaces which allow it to emit output.</p> <p>Q.102- How can you Add the arbitrary Key value pairs in your Mapper?</p> <p>You can set arbitrary (key, value) pairs of configuration data in your Job, e.g. with Job.getConfiguration().set(\"myKey\", \"myVal\"), and then retrieve this data in your mapper with Context.getConfiguration().get(\"myKey\"). This kind of functionality is typically done in the Mapper's setup() method.</p> <p>Q.103- How does Mapper's run method work?</p> <p>The Mapper.run() method then calls map(KeyInType, ValInType, Context) for each key/value pair in the InputSplit for that task</p> <p>Q.104- Which Object can be used to get the progress of a particular Job?</p> <p>Context</p> <p>Q.105- What is the next step after Mapper Or Maptask?</p> <p>The output of the Mapper is sorted and Partitions will be created for the output. Number of partitions depends on the number of reducers.</p> <p>Q.106- How can we control which particular Key should go in a specific Reducer?</p> <p>Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioned.</p> <p>Q.107- What is the use of Combiner?</p> <p>It is an optional component or class, and can be specified via Job.setCombinerClass(ClassName), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer.</p> <p>Q.107- How many Maps are there in a particular Job?</p> <p>The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files. Generally it is around 10-100 maps per-node. Task setup takes a while, so it is best if the maps take at least a minute to execute. Suppose, if you expect 10TB of input data and have a block size of 128MB, you'll end up with 82,000 maps, to control the number of blocks you can use the mapreduce.job.maps parameter (which only provides a hint to the framework). Ultimately, the number of tasks is controlled by the number of splits returned by the InputFormat.getSplits() method (which you can override).</p> <p>Q.108- What is the Reducer used for?</p> <p>Reducer reduces a set of intermediate values which share a key to a (usually smaller) set of values. The number of reductions for the job is set by the user via Job.setNumReduceTasks(int).</p> <p>Q.109- Explain the core methods of the Reducer?</p> <p>The API of Reducer is very similar to that of Mapper, there's a run() method that receives a Context containing the job's configuration as well as interfacing methods that return data from the reducer itself back to the framework. The run() method calls setup() once, reduce() once for each key associated with the reduce task, and cleanup() once at the end. Each of these methods can access the job's configuration data by using Context.getConfiguration(). As in Mapper, any or all of these methods can be overridden with custom implementations. If none of these methods are overridden, the default reducer operation is the identity function; values are passed through without further processing. The heart of Reducer is its reduce() method. This is called once per key; the second argument is an Iterable which returns all the values associated with that key.</p> <p>Q.110- What are the primary phases of the Reducer?</p> <p>Shuffle, Sort and Reduce.</p> <p>Q.111- Explain the Shuffle?</p> <p>Input to the Reducer is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.</p> <p>Q.112- Explain the Reducer's sort phase?</p> <p>The framework groups Reducer inputs by keys (since different mappers may have output the same key) in this stage. The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged (It is similar to merge-sort).</p> <p>Q.113- Explain the Reducer's reduce phase?</p> <p>In this phase the reduce(MapOutKeyType, Iterable, Context) method is called for each pair in the grouped inputs. The output of the reduce task is typically written to the FileSystem via Context.write (ReduceOutKeyType, ReduceOutValType). Applications can use the Context to report progress, set application-level status messages and update Counters, or just indicate that they are alive. The output of the Reducer is not sorted.</p> <p>Q.114- How many Reducers should be configured?</p> <p>The right number of reduces seems to be 0.95 or 1.75 multiplied by ( * mapreduce.tasktracker.reduce.tasks.maximum). With 0.95 all of the reducers can launch immediately and start transfering map outputs as the maps finish. With 1.75 the faster nodes will finish their first round of reductions and launch a second wave of reductions doing a much better job of load balancing. Increasing the number of reductions increases the framework overhead, but increases load balancing and lowers the cost of failures. <p>Q.115- It can be possible that a Job has 0 Reducers?</p> <p>It is legal to set the number of reduce-tasks to zero if no reduction is desired.</p> <p>Q.116- What happens if the number of Reducers are 0?</p> <p>In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set by setOutputPath(Path). The framework does not sort the map-outputs before writing them out to the FileSystem.</p> <p>Q.117- How many instances of Jobtracker can run on a Hadoop Cluster?</p> <p>Only one</p> <p>Q.118- What is the Jobtracker and what it performs in a Hadoop Cluster?</p> <p>JobTracker is a daemon service which submits and tracks the MapReduce tasks to the Hadoop cluster. It runs its own JVM process. And usually it runs on a separate machine, and each slave node is configured with a job tracker node location. The JobTracker is a single point of failure for the Hadoop MapReduce service. If it goes down, all running jobs are halted. JobTracker in Hadoop performs following actions Client applications submit jobs to the Job tracker. The JobTracker talks to the NameNode to determine the location of the data The JobTracker locates TaskTracker nodes with available slots at or near the data The JobTracker submits the work to the chosen TaskTracker nodes. A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to do then: it may resubmit the job elsewhere, it may mark that specific record as something to avoid, and it may even blacklist the TaskTracker as unreliable. When the work is completed, the JobTracker updates its status. The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, they are deemed to have failed and the work is scheduled on a different TaskTracker. A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to do then: it may resubmit the job elsewhere, it may mark that specific record as something to avoid, and it may even blacklist the TaskTracker as unreliable. When the work is completed, the JobTracker updates its status. Client applications can poll the JobTracker for information.</p> <p>Q.119- How is a task scheduled by a Jobtracker?</p> <p>The TaskTrackers send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These messages also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated. When the JobTracker tries to find somewhere to schedule a task within the MapReduce operations, it first looks for an empty slot on the same server that hosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in the same rack.</p> <p>Q.120- How many instances of Tasktracker run on a Hadoop Cluster?</p> <p>There is one Daemon Tasktracker process for each slave node in the Hadoop cluster.</p> <p>Q.121- How many maximum Jvm can run on a Slave Node?</p> <p>One or Multiple instances of Task Instance can run on each slave node. Each task instance is run as a separate JVM process. The number of Task instances can be controlled by configuration. Typically a high end machine is configured to run more task instances.</p> <p>Q.122- What is Nas?</p> <p>It is one kind of file system where data can reside on one centralized machine and all the cluster members will read and write data from that shared database, which would not be as efficient as HDFS.</p> <p>Q.123- How do Hdfs differ from Nfs?</p> <p>Following are differences between HDFS and NAS In HDFS Data Blocks are distributed across local drives of all machines in a cluster. Whereas in NAS data is stored on dedicated hardware. HDFS is designed to work with MapReduce System, since computation is moved to data. NAS is not suitable for MapReduce since data is stored separately from the computations. HDFS runs on a cluster of machines and provides redundancy using replication protocol. Whereas NAS is provided by a single machine therefore does not provide data redundancy.</p> <p>Q.124- How does a NameNode handle the failure of the Data Nodes?</p> <p>HDFS has master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. The NameNode and DataNode are pieces of software designed to run on commodity machines. NameNode periodically receives a Heartbeat and a Block report from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not received a heartbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replication the system begins replicating the blocks that were stored on the dead DataNode. The NameNode Orchestrates the replication of data blocks from one DataNode to another. The replication data transfer happens directly between DataNode and the data never passes through the NameNode.</p> <p>Q.125- Can Reducer talk with each other?</p> <p>No, Reducer runs in isolation.</p> <p>Q.126- Where the Mapper's intermediate data will be stored?</p> <p>The mapper output (intermediate data) is stored on the Local file system (NOT HDFS) of each individual mapper node. This is typically a temporary directory location which can be setup in config by the Hadoop administrator. The intermediate data is cleaned up after the Hadoop Job completes.</p> <p>Q.127- What is the Hadoop Mapreduce api contract for a Key and Value Class?</p> <p>The Key must implement the org.apache.hadoop.io.WritableComparable interface. The value must implement the org.apache.hadoop.io.Writable interface.</p> <p>Q.128- What is an IdentityMapper and IdentityReducer in Mapreduce?</p> <p>\u25cf   org.apache.hadoop.mapred.lib.IdentityMapper: Implements the identity function, mapping inputs directly to outputs. If a MapReduce programmer does not set the Mapper Class using JobConf.setMapperClass then IdentityMapper.class is used as a default value.</p> <p>\u25cf   org.apache.hadoop.mapred.lib.IdentityReducer : Performs no reduction, writing all input values directly to the output. If a MapReduce programmer does not set the Reducer Class using JobConf.setReducerClass then IdentityReducer.class is used as a default value.</p> <p>Q.129- What is the meaning of Speculative Execution in Hadoop?</p> <p>Speculative execution is a way of coping with individual Machine performance. In large clusters where hundreds or thousands of machines are involved there may be machines which are not performing as fast as others. This may result in delays in a full job due to only one machine not performing well. To avoid this, speculative execution in hadoop can run multiple copies of the same map or reduce tasks on different slave nodes. The results from first node to finish are used.</p> <p>Q.130- How is HDFS different from traditional File Systems?</p> <p>HDFS, the Hadoop Distributed File System, is responsible for storing huge data on the cluster. This is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and re !!!- info \"Quire these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files.</p> <p>Q.131- What is Hdfs block size and how is it different from Traditional File System block size?</p> <p>In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block is typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicate each block three times. Replicas are stored on different nodes. HDFS utilizes the local file system to store each HDFS block as a separate file. HDFS Block size can not be compared with the traditional file system block size.</p> <p>Q.132- What is a NameNode and how many instances of NameNode run on a Hadoop Cluster?</p> <p>The NameNode is the centrepiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. There is only One NameNode process run on any hadoop cluster. NameNode runs on its own JVM process. In a typical production cluster it runs on a separate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNode goes down, the file system goes offline. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add /copy /move /delete a file. The NameNode responds to successful requests by returning a list of relevant DataNode servers where the data lives.</p> <p>Q.133- How does the client communicate with Hdfs?</p> <p>The Client communication to HDFS happens using Hadoop HDFS API. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file on HDFS. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.</p> <p>Q.134- How the Hdfs blocks are replicated?</p> <p>HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. HDFS uses a rack-aware replica placement policy. In default configuration there are a total 3 copies of a data block on HDFS, 2 copies are stored on datanodes on the same rack and 3rd copy on a different rack.</p> <p>Q.135- Can you give some examples of Big Data?</p> <p>There are many real life examples of Big Data! Facebook is generating 500+ terabytes of data per day, NYSE (New York Stock Exchange) generates about 1 terabyte of new trade data per day, a jet airline collects 10 terabytes of censor data for every 30 minutes of flying time. All these are day to day examples of Big Data!</p> <p>Q.136- What is the basic difference between traditional Rdbms and Hadoop?</p> <p>Traditional RDBMS is used for transactional systems to report and archive the data, whereas Hadoop is an approach to store huge amounts of data in the distributed file system and process it. RDBMS will be useful when you want to seek one record from Big data, whereas, Hadoop will be useful when you want Big data in one shot and perform analysis on that later.</p> <p>Q.137- What is structured and unstructured Data?</p> <p>Structured data is the data that is easily identifiable as it is organized in a structure. The most common form of structured data is a database where specific information is stored in tables, that is, rows and columns. Unstructured data refers to any data that cannot be identified easily. It could be in the form of images, videos, documents, email, logs and random text. It is not in the form of rows and columns.</p> <p>Q.138- Since the data is replicated thrice in Hdfs, does it mean that any calculation done on One Node will also be replicated on the other Two?</p> <p>Since there are 3 nodes, when we send the MapReduce programs, calculations will be done only on the original data. The master node will know which node exactly has that particular data. In case, if one of the nodes is not responding, it is assumed to be failed. Only then, the re !!!- info \"Quired calculation will be done on the second replica.</p> <p>Q.139- What is throughput and how does Hdfs get a good throughput?</p> <p>Throughput is the amount of work done in a unit time. It describes how fast the data is getting accessed from the system and it is usually used to measure performance of the system. In HDFS, when we want to perform a task or an action, then the work is divided and shared among different systems. So all the systems will be executing the tasks assigned to them independently and in parallel. So the work will be completed in a very short period of time. In this way, the HDFS gives good throughput. By reading data in parallel, we decrease the actual time to read data tremendously.</p> <p>Q.140- What is streaming access?</p> <p>As HDFS works on the principle of \u2018Write Once, Read Many\u2018, the feature of streaming access is extremely important in HDFS. HDFS focuses not so much on storing the data but how to retrieve it at the fastest possible speed, especially while analyzing logs. In HDFS, reading the complete data is more important than the time taken to fetch a single record from the data.</p> <p>Q.141- What is a Commodity Hardware so does Commodity Hardware include Ram?</p> <p>Commodity hardware is a non-expensive system which is not of high quality or high-availability. Hadoop can be installed in any average commodity hardware. We don\u2019t need supercomputers or high-end hardware to work on Hadoop. Yes, Commodity hardware includes RAM because there will be some services which will be running on RAM.</p> <p>Q.142- Is NameNode also a Commodity?</p> <p>No. Namenode can never be a commodity hardware because the entire HDFS relies on it. It is the single point of failure in HDFS. Namenode has to be a high-availability machine.</p> <p>Q.143- What is Metadata?</p> <p>Metadata is the information about the data stored in data nodes such as location of the file, size of the file and so on.</p> <p>Q.144- What is a Daemon?</p> <p>Daemon is a process or service that runs in the background. In general, we use this word in the UNIX environment. The equivalent of Daemon in Windows is \u201cservices\u201d and in Dos is \u201d TSR\u201d.</p> <p>Q.145- What is a Heartbeat in Hdfs?</p> <p>A heartbeat is a signal indicating that it is alive. A datanode sends heartbeat to Namenode and task tracker will send its heart beat to job tracker. If the Namenode or job tracker does not receive a heartbeat then they will decide that there is some problem in the datanode or task tracker is unable to perform the assigned task.</p> <p>Q.146- How indexing is done in Hdfs?</p> <p>Hadoop has its own way of indexing. Depending upon the block size, once the data is stored, HDFS will keep on storing the last part of the data which will say where the next part of the data will be. In fact, this is the base of HDFS.</p> <p>Q.147- If a Data Node is full, how is it identified?</p> <p>When data is stored in datanode, then the metadata of that data will be stored in the Namenode. So Namenode will identify if the data node is full.</p> <p>Q.148- If DataNodes increase then do we need to upgrade NameNode?</p> <p>While installing the Hadoop system, Namenode is determined based on the size of the clusters. Most of the time, we do not need to upgrade the Namenode because it does not store the actual data, but just the metadata, so such a requirement rarely arises.</p> <p>Q.149- Are Job Tracker and Task Trackers present in separate machines?</p> <p>Yes, job tracker and task tracker are present in different machines. The reason is that the job tracker is a single point of failure for the Hadoop MapReduce service. If it goes down, all running jobs are halted.</p> <p>Q.150- On what basis NameNode will decide which DataNode to write on?</p> <p>As the Namenode has the metadata (information) related to all the data nodes, it knows which datanode is free.</p> <p>Q.151- Who is a user in Hdfs?</p> <p>A user is like you or me, who has some query or who needs some kind of data.</p> <p>Q.152- Is the client the end user in Hdfs?</p> <p>No, Client is an application which runs on your machine, which is used to interact with the Namenode (job tracker) or datanode (task tracker).</p> <p>Q.153- What is the Communication Channel between client and NameNode/DataNode?</p> <p>The mode of communication is SSH.</p> <p>Q.154- What is a Rack?</p> <p>Rack is a storage area with all the datanodes put together. These data nodes can be physically located at different places. Rack is a physical collection of datanodes which are stored at a single location. There can be multiple racks in a single location.</p> <p>Q.155- On what basis Data will be stored on a Rack?</p> <p>When the client is ready to load a file into the cluster, the content of the file will be divided into blocks. Now the client consults the Namenode and gets 3 datanodes for every block of the file which indicates where the block should be stored. While placing the datanodes, the key rule followed is \u201cfor every block of data, two copies will exist in one rack, and a third copy in a different rack\u201c. This rule is known as \u201cReplica Placement Policy\u201c.</p> <p>Q.156- Do we need to place 2nd and 3rd Data in Rack 2 only?</p> <p>Yes, this is to avoid datanode failure.</p> <p>Q.157- What if Rack 2 and DataNode fails?</p> <p>If both rack2 and datanode present in rack 1 fails then there is no chance of getting data from it. In order to avoid such situations, we need to replicate that data more number of times instead of replicating only thrice. This can be done by changing the value in the replication factor which is set to 3 by default.</p> <p>Q.158- What is the difference between Gen1 and Gen2 Hadoop with regards to the NameNode?</p> <p>In Gen 1 Hadoop, Namenode is the single point of failure. In Gen 2 Hadoop, we have what is known as Active and Passive Namenodes kind of a structure. If the active Namenode fails, the passive Namenode takes over the charge.</p> <p>Q.159- Do we require two servers for the NameNode and the DataNodes?</p> <p>Yes, we need two different servers for the Namenode and the datanodes. This is because Namenode requires a highly configurable system as it stores information about the location details of all the files stored in different data nodes and on the other hand, datanodes require a low configuration system.</p> <p>Q.160- Why are the number of splits equal to the number of Maps?</p> <p>The number of maps is equal to the number of input splits because we want the key and value pairs of all the input splits.</p> <p>Q.161- Is a Job split into maps?</p> <p>No, a job is not split into maps. Spilt is created for the file. The file is placed on datanodes in blocks. For each split, a map is needed.</p> <p>Q.162- Which are the two types of writes In Hdfs?</p> <p>There are two types of writes in HDFS: \u25cf   Posted and non-posted write. Posted Write is when we write it and forget about it, without worrying about the acknowledgement.</p> <p>\u25cf   It is similar to our traditional Indian post.</p> <p>\u25cf   Non-posted Write, we wait for the acknowledgement. It is similar to today's courier services. Naturally, non-posted write is more expensive than the posted write. It is much more expensive, though both writes are asynchronous.</p> <p>Q.163- Why reading is done in parallel and writing is not in Hdfs?</p> <p>Reading is done in parallel because by doing so we can access the data fast. But we do not perform the write operation in parallel. The reason is that if we perform the write operation in parallel, then it might result in data inconsistency. For example, you have a file and two nodes are trying to write data into the file in parallel, then the first node does not know what the second node has written and vice-versa. So, this makes it confusing which data to be stored and accessed.</p> <p>Q.164- Can Hadoop be compared to a Nosql Database like Cassandra?</p> <p>Though NOSQL is the closest technology that can be compared to Hadoop, it has its own pros and cons. There is no DFS in NOSQL. Hadoop is not a database. It\u2019s a file system (HDFS) and distributed programming framework (MapReduce).</p> <p>Q.165- How does JobTracker schedule a task?</p> <p>The TaskTrackers send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These messages also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated. When the JobTracker tries to find somewhere to schedule a task within the MapReduce operations, it first looks for an empty slot on the same server that hosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in the same rack.</p> <p>Q.166- What is a Task Tracker in Hadoop and how many instances of Task Tracker run on a Hadoop Cluster?</p> <p>A TaskTracker is a slave node daemon in the cluster that accepts tasks (Map, Reduce and Shuffle operations) from a JobTracker. There is only One Task Tracker process run on any hadoop slave node. Task Tracker runs on its own JVM process. Every TaskTracker is configured with a set of slots, these indicate the number of tasks that it can accept. The TaskTracker starts a separate JVM process to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. The TaskTracker monitors these task instances, capturing the output and exit codes. When the Task instances finish, successfully or not, the task tracker notifies the JobTracker. The TaskTrackers also send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These messages also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated.</p> <p>Q.167- What is a task instance in Hadoop and where does it run?</p> <p>Task instances are the actual MapReduce jobs which are run on each slave node. The TaskTracker starts a separate JVM process to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. Each Task Instance runs on its own JVM process. There can be multiple processes of task instances running on a slave node. This is based on the number of slots configured on the task tracker. By default a new task instance JVM process is spawned for a task.</p> <p>Q.168- What is the configuration of a typical Slave Node on a Hadoop Cluster and how many Jvms run on a Slave Node?</p> <p>Single instance of a Task Tracker is run on each Slave node. Task tracker is run as a separate JVM process. Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as a separate JVM process. One or Multiple instances of Task Instance is run on each slave node. Each task instance is run as a separate JVM process. The number of Task instances can be controlled by configuration. Typically a high end machine is configured to run more task instances.</p> <p>Q.169- How does NameNode handle DataNode failures?</p> <p>NameNode periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not received a heartbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replication the system begins replicating the blocks that were stored on the dead datanode. The NameNode Orchestrates the replication of data blocks from one datanode to another. The replication data transfer happens directly between datanodes and the data never passes through the namenode.</p> <p>Q.170- Does Mapreduce programming model provide a way for Reducers to communicate with each other and in a Mapreduce Job can a Reducer communicate with another Reducer?</p> <p>Nope, MapReduce programming model does not allow reducers to communicate with each other. Reducers run in isolation.</p> <p>Q.171- Can I set the number of Reducers to Zero?</p> <p>Yes, Setting the number of reducers to zero is a valid configuration in Hadoop. When you set the reducers to zero no reducers will be executed, and the output of each mapper will be stored to a separate file on HDFS.This is different from the condition when reducers are set to a number greater than zero and the Mappers output (intermediate data) is written to the Local file system(NOT HDFS) of each master slave node.</p> <p>Q.172- Where is the Mapper Output intermediate Key value data stored?</p> <p>The mapper output (intermediate data) is stored on the Local file system (NOT HDFS) of each individual mapper node. This is typically a temporary directory location which can be setup in config by the hadoop administrator. The intermediate data is cleaned up after the Hadoop Job completes.</p> <p>Q.173- If Reducers do not start before all Mappers finish then why does the progress on Mapreduce Job shows something like Map 50 percent Reduce 10 percent and why Reducers progress percentage is displayed when Mapper is not Finished yet?</p> <p>Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The progress calculation also takes into account the processing of data transfer which is done by reduce process, therefore the reduced progress starts showing up as soon as any intermediate key-value pair for a mapper is available to be transferred to reducer. Though the reducer progress is still updated, the programmer-defined reduce method is called only after all the mappers have finished.</p> <p>Q.174- Explain in brief the three Modes in which Hadoop can be run?</p> <p>The three modes in which Hadoop can be run are:</p> <p>\u25cf   Standalone (local) mode - No Hadoop daemons running, everything runs on a single Java Virtual machine only.</p> <p>\u25cf   Pseudo-distributed mode - Daemons run on the local machine, thereby simulating a cluster on a smaller scale.</p> <p>\u25cf   Fully distributed mode - Runs on a cluster of machines.</p> <p>Q.175- Explain what are the features of Standalone local Mode?</p> <p>In stand-alone or local mode there are no Hadoop daemons running, and everything runs on a single Java process. Hence, we don't get the benefit of distributing the code across a cluster of machines. Since it has no DFS, it utilizes the local file system. This mode is suitable only for running MapReduce programs by developers during various stages of development. It's the best environment for learning and good for debugging purposes.</p> <p>Q.176- What are the features of fully distributed mode?</p> <p>In Fully Distributed mode, the clusters range from a few nodes to 'n' number of nodes. It is used in production environments, where we have thousands of machines in the Hadoop cluster. The daemons of Hadoop run on these clusters. We have to configure separate masters and separate slaves in this distribution, the implementation of which is quite complex. In this configuration, Namenode and Datanode run on different hosts and there are nodes on which the task tracker runs. The root of the distribution is referred to as HADOOP_HOME.</p> <p>Q.177- Explain what are the main features Of pseudo mode?</p> <p>In Pseudo-distributed mode, each Hadoop daemon runs in a separate Java process, as such it simulates a cluster though on a small scale. This mode is used both for development and QA environments. Here, we need to do the configuration changes.</p> <p>Q.178- What are the port numbers of NameNode and JobTracker and TaskTracker?</p> <p>The port number for Namenode is \u201970\u2032, for job tracker is \u201930\u2032 and for task tracker is \u201960\u2032.</p> <p>Q.179- Tell us what is a spill factor with respect to the ram?</p> <p>Spill factor is the size after which your files move to the temp file. Hadoop-tmp directory is used for this. Default value for io.sort.spill.percent is 0.80. A value less than 0.5 is not recommended.</p> <p>Q.180- Is fs.mapr working for a single directory?</p> <p>Yes, fs.mapr.working.dir is just one directory.</p> <p>Q.181- Which are the three main Hdfs site.xml properties?</p> <p>The three main hdfs-site.xml properties are:</p> <p>\u25cf   Dfs.name.dir which gives you the location on which metadata will be stored and where DFS is located \u2013 on disk or onto the remote.</p> <p>\u25cf   Dfs.data.dir which gives you the location where the data is going to be stored.</p> <p>\u25cf   Fs.checkpoint.dir which is for secondary Namenode.</p> <p>Q.182-. How can I restart Namenode?</p> <p>Click on stop-all.sh and then click on start-all.sh OR Write sudo hdfs (press enter), su-hdfs (press enter), /etc/init.d/ha (press enter) and then /etc/init.d/hadoop-0.20-namenode start (press enter).</p> <p>Q.183- How can we check whether Namenode is working or not?</p> <p>To check whether Namenode is working or not, use the command /etc/init.d/hadoop- 0.20-namenode status or as simple as jps\u2019.</p> <p>Q.184- At times you get a connection refused Java Exception when you run the file system check command Hadoop fsck?</p> <p>The most possible reason is that the Namenode is not working on your VM.</p> <p>Q.185- What is the use of the command Mapred.job.tracker?</p> <p>The command mapred.job.tracker is used by the Job Tracker to list out which host and port that the MapReduce job tracker runs at. If it is \"local\", then jobs are run in-process as a single map and reduce tasks.</p> <p>Q.186- What does etc.init.d do?</p> <p>/etc /init.d specifies where daemons (services) are placed or to see the status of these daemons. It is very LINUX specific, and has nothing to do with Hadoop.</p> <p>Q.187- How can we look for the Namenode in the browser?</p> <p>If you have to look for Namenode in the browser, you don\u2019t have to give localhost: 8021, the port number to look for Namenode in the browser is 50070.</p> <p>Q.188- What do masters and slaves consist of?</p> <p>Masters contain a list of hosts, one per line, that are to host secondary namenode servers. Slaves consist of a list of hosts, one per line, that host datanode and task tracker servers.</p> <p>Q.189- What is the function Of Hadoop-env.sh and where is it present?</p> <p>This file contains some environment variable settings used by Hadoop; it provides the environment for Hadoop to run. The path of JAVA_HOME is set here for it to run properly. Hadoop-env.sh file is present in the conf/hadoop-env.sh location. You can also create your own custom configuration file conf/hadoop-user-env.sh, which will allow you to override the default Hadoop settings.</p> <p>Q.190- Can we have multiple entries in the master files?</p> <p>Yes, we can have multiple entries in the Master files.</p> <p>Q.191- In Hadoop_pid_dir and what does pid stands for?</p> <p>PID stands for \u2018Process ID\u2019.</p> <p>Q.192- What do Hadoop metrics and properties files do?</p> <p>Hadoop-metrics Properties is used for \u2018Reporting Purposes. It controls the reporting for hadoop. The default status is \u2018not to report\u2018.</p> <p>Q.193- What are the network requirements for hadoop?</p> <p>The Hadoop core uses Shell (SSH) to launch the server processes on the slave nodes. It requires password-less SSH connection between the master and all the slaves and the Secondary machines.</p> <p>Q.194- Why do we need a passwordless ssh in a fully distributed environment?</p> <p>We need a password-less SSH in a Fully-Distributed environment because when the cluster is LIVE and running in a Fully Distributed environment, the communication is too frequent. The job tracker should be able to send a task to the task tracker quickly.</p> <p>Q.195- What will happen if a NameNode has no data?</p> <p>If a Namenode has no data it cannot be considered as a Namenode. In practical terms, Namenode needs to have some data.</p> <p>Q.196- What happens to the job tracker when NameNode is down?</p> <p>Namenode is the main point which keeps all the metadata, keeping track of failure of datanode with the help of heart beats. As such when a namenode is down, your cluster will be completely down, because Namenode is the single point of failure in a Hadoop Installation.</p> <p>Q.197- Explain what you mean by formatting the Dfs?</p> <p>Like we do in Windows, DFS is formatted for proper structuring of data. It is not usually recommended to do as it formats the Namenode too in the process, which is not desired.</p> <p>Q.198- We use Unix variants for hadoop and can we use Microsoft Windows for the same?</p> <p>In practicality, Ubuntu and Red Hat Linux are the best Operating Systems for Hadoop. On the other hand, Windows can be used but it is not used frequently for installing Hadoop as there are many support problems related to it. The frequency of crashes and the subsequent restarts makes it unattractive. As such, Windows is not recommended as a preferred environment for Hadoop Installation, though users can give it a try for learning purposes in the initial stage.</p> <p>Q.199- Which one decides the input split hdfs client or NameNode?</p> <p>The HDFS Client does not decide. It is already specified in one of the configurations through which input split is already configured.</p> <p>Q.200- Can you tell me if we can create a hadoop cluster from scratch?</p> <p>Yes, we can definitely do that. Once we become familiar with the Apache Hadoop environment, we can create a cluster from scratch.</p> <p>Q.201- Explain the significance of ssh and what is the port on which port does ssh work and why do we need password in ssh localhost?</p> <p>SSH is a secure shell communication, is a secure protocol and the most common way of administering remote servers safely, relatively very simple and inexpensive to implement. A single SSH connection can host multiple channels and hence can transfer data in both directions. SSH works on Port No. 22, and it is the default port number. However, it can be configured to point to a new port number, but it's not recommended. In a local host, password is required in SSH for security and in a situation where password less communication is not set.</p> <p>Q.202- What is ssh and explain in detail about ssh communication between masters and the slaves?</p> <p>Secure Socket Shell or SSH is a password-less secure communication that provides administrators with a secure way to access a remote computer and data packets are sent across the slave. This network protocol also has some format into which data is sent across. SSH communication is not only between masters and slaves but also between two hosts in a network. SSH appeared in 1995 with the introduction of SSH - 1. Now SSH 2 is in use, with the vulnerabilities coming to the fore when Edward Snowden leaked information by decrypting some SSH traffic.</p> <p>Q.203- Can you tell what will happen to a NameNode and when the Job tracker is not up and running?</p> <p>When the job tracker is down, it will not be in functional mode, all running jobs will be halted because it is a single point of failure. Your whole cluster will be down but still Namenode will be present. As such the cluster will still be accessible if Namenode is working, even if the job tracker is not up and running. But you cannot run your Hadoop job.</p>"},{"location":"hadoop/hadoopscenarios/","title":"Hadoop Scenario based","text":"<p>Q. 1- If 8TB is the available disk space per node (10 disks with 1 TB, 2 disk for operating system etc. were excluded.). Assuming initial data size is 600 TB. How will you estimate the number of data nodes (n)?</p> <p>Estimating the hardware requirement is always challenging in the Hadoop environment because we never know when data storage demand can increase for a business. We must understand following factors in detail to come to a conclusion for the current scenario of adding right numbers to the cluster:</p> <p>\u25cf   The actual size of data to store \u2013 600 TB</p> <p>\u25cf   At what pace the data will increase in the future (per day/week/month/quarter/year) \u2013 Data trending analysis or business requirement justification (prediction)</p> <p>\u25cf   We are in Hadoop world, so replication factor plays an important role \u2013 default 3x replicas</p> <p>\u25cf   Hardware machine overhead (OS, logs etc.) \u2013 2 disks were considered</p> <p>\u25cf   Intermediate mapper and reducer data output on hard disk - 1x</p> <p>\u25cf   Space utilization between 60 % to 70 % - Finally, as a perfect designer we never want our hard drives to be full with their storage capacity.</p> <p>\u25cf   Compression ratio Let\u2019s do some calculation to find the number of data nodes required to store 600 TB of data:</p> <p>Rough calculation:</p> <p>Data Size \u2013 600 TB Replication factor \u2013 3 Intermediate data \u2013 1</p> <p>Total Storage requirement \u2013 (3+1) * 600 = 2400 TB Available disk size for storage \u2013 8 TB</p> <p>Total number of required data nodes (approx.): 2400/8 = 300 machines</p> <p>Actual Calculation: Rough Calculation + Disk space utilization + Compression ratio</p> <p>Disk space utilization \u2013 65 % (differ business to business) Compression ratio \u2013 2.3</p> <p>Total Storage requirement \u2013 2400/2.3 = 1043.5 TB Available disk size for storage \u2013 8*0.65 = 5.2 TB</p> <p>Total number of required data nodes (approx.): 1043.5/5.2 = 201 machines Actual usable cluster size (100 %): (201*8*2.3)/4 = 925 TB</p> <p>Case: Business has predicted 20 % data increase in a quarter and we need to predict the new machines to be added in a year</p> <p>Data increase \u2013 20 % over a quarter Additional data:</p> <p>1st quarter: 1043.5 * 0.2 = 208.7 TB</p> <p>2nd quarter: 1043.5 * 1.2 * 0.2 = 250.44 TB</p> <p>3rd quarter: 1043.5 * (1.2)^2 * 0.2 = 300.5 TB</p> <p>4th quarter: 1043.5 * (1.2)^3 * 0.2 = 360.6 TB Additional data nodes requirement (approx.): 1st quarter: 208.7/5.2 = 41 machines</p> <p>2nd quarter: 250.44/5.2 = 49 machines 3rd quarter: 300.5/5.2 = 58 machines 4th quarter: 360.6/5.2 = 70 machines With these numbers you can predict next year additional machine requirements for the cluster (last quarter + 24), (last quarter + 28) and so on.</p> <p>Q. 2 Imagine that you are uploading a file of 500MB into HDFS.100MB of data is successfully uploaded into HDFS and another client wants to read the uploaded data while the upload is still in progress. What will happen in such a scenario, will the 100 MB of data that is uploaded will be displayed?</p> <p>Although the default blocks size is 64 MB in Hadoop 1x and 128 MB in Hadoop 2x whereas in such a scenario let us consider block size to be 100 MB which means that we are going to have 5 blocks replicated 3 times (default replication factor). Let\u2019s consider an example of how does a block is written to HDFS:</p> <p>We have 5 blocks (A/B/C/D/E) for a file, a client, a namenode and a datanode. So, first the client will take Block A and will approach namenode for datanode location to store this block and the replicated copies. Once the client is aware about the datanode information, it will directly reach out to datanode and start copying Block A which will be simultaneously replicated to other 2 datanodes. Once the block is copied and replicated to the datanodes, the client will get the confirmation about the Block A storage and then, it will initiate the same process for the next block \u201cBlock B\u201d.</p> <p>So, during this process if the 1st block of 100 MB is written to HDFS and the next block has been started by the client to store then the 1st block will be visible to readers. Only the current block being written will not be visible by the readers.</p> <p>Q.3 When does a NameNode enter the safe mode?</p> <p>Namenode is responsible for managing the meta storage of the cluster and if something is missing from the cluster then Namenode will be held. This makes Namenode check all the necessary information during the safe mode before making the cluster writable to the users. There are couple of reasons for Namenode to enter the safe mode during startup such as;</p> <p>i)  Namenode loads the filesystem state from fsimage and edits log files, it then waits for datanodes to report their blocks, so it does not start replicating the blocks which already exist in the cluster.</p> <p>ii) Heartbeats from all the datanodes and also if any corrupt blocks exist in the cluster. Once Namenode verifies all this information, it will leave the safe mode and make the cluster accessible. Sometimes, we need to manually enter/leave the safe mode for Namenode which can be done using the command line \u201chdfs dfsadmin -safemode enter/leave\u201d.</p> <p>Q.4 What are the steps followed by the application while running a YARN job when calling a SubmitApplication method?</p> <p>All jobs that are submitted by a client go to the resource manager. The resource manager is provided with a scheduler, and it is the responsibility of the resource manager to determine the resources required to run that particular job.</p> <p>Once the resources and the number of resources are determined, the resource manager will then launch the application masters specific to the applications that are to be run.</p> <p>The application master associated with a specific application, also known as the application master daemon remains available until the job gets completed.</p> <p>The duty of the application master is to negotiate resources from the resource manager. This means that the application manager will ask for the number of resources as specified by the resource manager.</p> <p>The application manager will then launch the container in a different node manager once the data becomes available.</p> <p>Node managers monitor the containers, and a node manager is responsible for all containers available in that particular node. The container is responsible for giving periodic updates to the application manager regarding the job that it is executing.</p> <p>Once the job gets completed, the container and the resources get freed up, following which the application manager proceeds to update the resource manager that the job is completed. The client then receives the corresponding update from the resource manager.</p> <p>Q.5 In MapReduce tasks, each reduce task writes its output to a file named part-r-nnnnn. Here nnnnn is the partition ID associated with the reduce task. Is it possible to ultimately merge these files. Explain your answer </p> <p>The files do not get automatically merged by Hadoop. The number of files generated is equal to the number of reduced tasks that take place. If you need that as input for the next job, there is no need to worry about having separate files. Simply specify the entire directory as input for the next job. If the data from the files must be pulled out of the cluster, they can be merged while transferring the data. The following command may be used to merge the files while pulling the data off the cluster: Hadoop fs -cat //part-r-* &gt; /directory of destination path The complete merging of the output files from the reduce tasks can be delegated using the following command as well:</p> <p>Hadoop fs -getmerge / /</p> <p>Q.6 There is a YARN cluster in which the total amount of memory available is 40GB. There are two application queues, ApplicationA and ApplicationB. The queue of ApplicationA has 20 GB allocated, while that of ApplicationB has 8GB allocated. Each map task requires an allocation of 32GB. How will the fair scheduler assign the available memory resources under the DRF (Dominant Resource Finder) Scheduler?</p> <p>The allocation of resources within a particular queue is controlled separately. Within one queue:</p> <p>The FairScheduler can apply either the FIFO policy, the FairPolicy or the DominantResourceFairnessPolicy. The CapacityScheduler may use either the FIFOPolicy or the FairPolicy.</p> <p>The default scheduling policy of the FairScheduler is the Fair policy, where memory is used as a resource.The DRF policy uses both memory and CPU as resources and allocates them accordingly. DRF is quite similar to fair-scheduling. However, the difference is that DRF primarily applies to the allocation of resources among queues. This is already heavily handled by queue weights. Hence, the most crucial job of the DRF is to manage multiple resources rather than equal resource allocation. In this case, initially, both Application A And Applicaton B will have some resources allocated to various jobs present in their corresponding queues. In such a way, only 12GB (40GB - (20 GB + 8 GB)) will remain in the cluster. Each of the queues will request to run a map task of size 32GB. The total memory available is 40 GB.The rest of the required resources can be taken from the CPU. In such a case, ApplicationA currently holds 20GB. Another 12GB is</p> <p>required for the map task to get executed. Here, the fair scheduler will grant the container requesting 12GB of memory to ApplicationA. The memory allocated to ApplicationB is 8GB, and it will require another 24 GB to run a map task. Memory is not available for application, and hence the DRF will try to use 8 GB from memory, and the remaining 20GB will be used from the CPU.</p> <p>Q.7 How does a NameNode know that one of the DataNodes in a cluster is not functioning?</p> <p>Hadoop clusters follow a master-slave architecture, in which the NameNode acts as the master and the Data Nodes act as the slaves. The data nodes contain the actual data that has to be stored or processed in a cluster. The data nodes are responsible for sending heartbeat messages to the NameNode every 3 seconds to confirm that they are active or alive. Suppose the NameNode fails to receive a heartbeat message from a particular node for more than ten minutes. In that case, the NameNode considers that particular data node to be no longer active or dead. The name node then initiates replication of the data on the dead data node blocks to some of the other data nodes that are active on the cluster. Data nodes are able to talk to each other to rebalance the data and their replicas within a cluster. They can copy data around and transfer it to keep the replication valid in the cluster. The data nodes store metadata and information about which files can be mapped to which particular block location. Data nodes also maintain a checksum for each block. When data gets written to HDFS, the checksum value is written simultaneously to the data node. When the data gets read, by default, the same checksum value is used for verification.</p> <p>Data nodes are responsible for updating the name node with the block information at regular intervals of time and before verifying the checksum's value. If the checksum value is not correct for a specific block, then that particular block can be considered to have a disk-level corruption. Since there is an issue in the reporting of the block information to the name node, the name node is able to know that there is a disk-level corruption on the data and necessary steps have to be taken to copy the data to alternate locations on other active data nodes to maintain the replication factor.</p>"},{"location":"hadoop/kerberos/","title":"Kerberos","text":"<p>Kerberos is a protocol designed to provide secure authentication to services over an insecure network. It ensures that passwords are never sent across the network and encryption keys are never directly exchanged. Furthermore, you and the application can mutually authenticate each other. Many organizations utilize Kerberos as the foundation for single sign-on capabilities. The name \"Kerberos\" originates from Greek mythology, specifically from Cerberus, the three-headed dog that guards the gates to the underworld, symbolizing its role in guarding access to applications.</p> <p>Kerberos was selected over other options like SSL certificates due to its better performance and simpler user management; for instance, removing a user in Kerberos involves a simple deletion, whereas revoking an SSL certificate is a more complicated process. </p> <p>A key benefit of Kerberos is that it eliminates the need for passwords to be transmitted across the network, thereby removing the potential threat of attackers \"sniffing\" or intercepting passwords.</p> <p>To understand Kerberos, several key terms are essential:</p> <ul> <li> <p>A Kerberos realm is the domain or group of systems where Kerberos has the authority to authenticate users to services. Multiple realms can exist and be interconnected.</p> </li> <li> <p>Database: This part stores user and service identities, which are known as principles. It also holds other information like encryption keys, ticket validity durations, and expiration dates.</p> </li> <li> <p>A principle is a unique identity, which can be either a user or a service (like an application).</p> </li> <li> <p>A client is a process that accesses a service on behalf of a user, essentially the user wanting to access something.</p> </li> <li> <p>A service is a resource provided to a client, such as a file server or an application.</p> </li> <li> <p>The Key Distribution Center (KDC) is the central component of Kerberos, responsible for supplying tickets and generating temporary session keys that enable secure user authentication to a service. The KDC stores all the secret symmetric keys for users and services. It comprises two main servers: the Authentication Server (AS) and the Ticket Granting Server (TGS).</p> </li> <li> <p>The Authentication Server (AS) confirms that a known user is making an access request and issues a Ticket Granting Ticket (TGT).</p> </li> <li> <p>The Ticket Granting Server (TGS) confirms that a user is requesting access to a known service and issues service tickets.</p> </li> <li> <p>Authenticators are records that contain information provably generated recently using a session key known only to the client and the server, facilitating mutual authentication.</p> </li> <li> <p>Tickets contain crucial information such as the client's identity, service ID, session keys, timestamps, and time to live, all encrypted with a server's secret key.</p> </li> </ul> <p>The high-level communication process for a user to access a service involves a series of messages exchanged between the user, the Authentication Server (AS), the Ticket Granting Server (TGS), and the service itself, with at least two messages sent at almost every step, some in plaintext and some encrypted with a symmetric key.</p>"},{"location":"hadoop/kerberos/#explaination-part-1","title":"Explaination part 1","text":"<p>Here's a detailed breakdown of the Kerberos authentication process:</p> <ul> <li> <p>Initial Request to Authentication Server (AS): </p> <p>The user initiates the process by sending an unencrypted message to the AS. This message includes the user's ID (e.g., Rob), the ID of the service they wish to access (e.g., a CRM application), the user's IP address (which can be single, multiple, or none, depending on configuration), and the requested lifetime for the TGT. While users might desire an infinite lifetime for convenience, security considerations often lead Kerberos to override such requests.</p> </li> <li> <p>AS Processing and Response:</p> <p>Upon receiving the user's message, the AS first verifies the user ID against its list of known users and their secret keys, retrieving the user's secret client key if found. The AS then generates two messages to send back to the user. The first message contains the ID of the TGS, the message creation timestamp, and its lifetime. The second message is the Ticket Granting Ticket (TGT), which includes the user's ID, the TGS's ID, a timestamp, the user's IP address, and the TGT's lifetime (which may differ from the user's request). The AS also generates a randomly generated symmetric TGS session key and adds it to both messages. The first message is encrypted with the user's secret key, and the TGT is encrypted with the TGS's secret key. These two encrypted messages are then sent to the user.</p> </li> <li> <p>User Processing AS Response: </p> <p>The user must decrypt the first message to proceed. This is done by the user generating their secret key, which involves entering their password. Kerberos adds a salt (typically the user's username at realm name) and a key version number (KDN) to the password. The salted password is then processed through a hashing algorithm (specifically, string-to-key) to generate the user's secret key. This key is used to decrypt the first message, and this decryption step also serves to validate the user's password; an incorrect password would result in a decryption failure. If successful, the user gains access to the TGS's ID and the TGS session key. Importantly, the user cannot decrypt the TGT because they do not possess the TGS's secret key.</p> </li> <li> <p>User Request to Ticket Granting Server (TGS):</p> <p>The user then prepares two new messages. The first is a simple plaintext message indicating the desired service and its requested ticket lifetime. The second is a user Authenticator, containing the user ID and a creation timestamp, encrypted with the TGS session key. These new messages, along with the still-encrypted TGT, are sent to the TGS.</p> </li> <li> <p>TGS Processing User Request:</p> <p>The TGS starts by examining the plaintext service ID and verifying it against its list of known services in the KDC, retrieving the service's secret key if found. The TGS then decrypts the TGT using its own secret key, which reveals the TGS session key. This session key is then used to decrypt the user Authenticator. With both the TGT and Authenticator decrypted, the TGS performs several validations: ensuring user IDs match between the TGT and Authenticator, comparing timestamps (tolerating up to a two-minute difference), comparing the TGT's IP address to the user's received IP address (if applicable), and checking if the TGT has expired. For replay protection, the TGS maintains a cache of recently received authenticators and checks if the current Authenticator is already in the cache; if not, it adds it.</p> </li> <li> <p>TGS Response to User: </p> <p>If all validations pass, the TGS creates two messages for the user. The first message includes the service ID, timestamp, and message lifetime. The second message is the service ticket, containing the user ID, the service ID, a timestamp, the user's IP address, and the service ticket's lifetime. The TGS generates a random symmetric service session key and adds it to both messages. The first message is encrypted with the TGS session key, and the service ticket is encrypted with the service secret key. These two messages are then sent to the user.</p> </li> <li> <p>User Processing TGS Response: </p> <p>The user decrypts the first message using the TGS session key (which they received from the AS), gaining access to the service session key. The user then creates a new Authenticator message with their ID and a timestamp, encrypting it with the service session key. The user cannot decrypt the service ticket because it's encrypted with the service's secret key, so they simply forward the encrypted service ticket along with the newly created Authenticator message to the service.</p> </li> <li> <p>Service Processing User Request: </p> <p>The service decrypts the service ticket using its own secret key, which reveals the service session key. This service session key is then used to decrypt the user Authenticator message. Similar to the TGS, the service performs validations: matching user IDs, comparing timestamps (tolerating less than two minutes difference), checking IP addresses, and verifying ticket expiration. For replay protection, the service also maintains a cache of recently received authenticators and adds the current one if it's new.</p> </li> <li> <p>Service Response and Mutual Authentication:</p> <p>If validations are successful, the service creates its own Authenticator message, including its service ID and a timestamp, encrypted with the service session key. This service Authenticator is sent back to the user. The user decrypts this message using the same symmetric service session key. The user then verifies that the service name in the Authenticator matches the expected service, completing the mutual authentication. The user also checks the timestamp to ensure the Authenticator was recently created. Finally, the user caches a copy of the encrypted service ticket for future use with that service, given the mutual authentication. This entire exchange securely distributes a symmetric service session key that allows the user and service to communicate authentication information securely.</p> </li> </ul>"},{"location":"hadoop/kerberos/#explaination-part-2","title":"Explaination part 2","text":"<p>Here's a detailed breakdown of how Kerberos authenticates a user attempting to access a service in a Hadoop cluster, such as listing a directory from HDFS:</p> <ul> <li> <p>Initial Authentication (User to KDC - AS):</p> <p>The process begins on a Linux machine where the user executes the <code>kinit</code> tool. The <code>kinit</code> program prompts for the user's password and then sends an authentication request to the Kerberos Authentication Server (AS). Upon successful authentication, the AS responds by providing a Ticket Granting Ticket (TGT). The <code>kinit</code> tool then stores this TGT in the user's credentials cache. At this point, the user has been authenticated and is ready to execute a Hadoop command.</p> </li> <li> <p>Requesting a Service Ticket (Hadoop Client to KDC - TGS): </p> <p>When a user runs a Hadoop command (e.g., <code>hadoop fs -ls /</code>), the Hadoop client uses the cached TGT to contact the Ticket Granting Server (TGS). The client approaches the TGS to request a service ticket for the specific Hadoop service it intends to access, such as the NameNode service. The TGS grants the requested service ticket, and the Hadoop client caches it.</p> </li> <li> <p>Accessing the Service (Hadoop Client to Service): </p> <p>With the service ticket in hand, the Hadoop client can now communicate with the target service (e.g., the NameNode). The Hadoop RPC (Remote Procedure Call) mechanism uses this service ticket to reach out to the NameNode. A mutual exchange of tickets occurs between the client and the NameNode. The client's service ticket proves its identity, and the NameNode's ticket confirms its own identity, ensuring that both parties are certain they are communicating with an authenticated entity. This two-way verification is known as mutual authentication.</p> </li> <li> <p>Authorization: </p> <p>After authentication is complete, the system proceeds to authorization. This is a separate step where the NameNode checks if the authenticated user has the necessary permissions to perform the requested action, such as listing the root directory. If permissions are granted, the NameNode returns the results.</p> </li> </ul>"},{"location":"hadoop/mapreduce/","title":"MapReduce","text":""},{"location":"hadoop/mapreduce/#mapreduce","title":"MapReduce","text":"<p>MapReduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment.</p> <p>MapReduce consists of two distinct tasks \u2014 Map and Reduce.</p> <p>As the name MapReduce suggests, reducer phase takes place after the mapper phase has been completed.</p> <p>So, the first is the map job, where a block of data is read and processed to produce key-value pairs as intermediate outputs.</p> <p>The output of a Mapper or map job (key-value pairs) is input to the Reducer.</p> <p>The reducer receives the key-value pair from multiple map jobs.</p> <p>Then, the reducer aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output.</p>"},{"location":"hadoop/mapreduce/#advantages-of-mapreduce","title":"Advantages of MapReduce","text":"<ul> <li> <p>Parallel Processing: In MapReduce, we are dividing the job among multiple nodes and each node works with a part of the job simultaneously. So, MapReduce is based on Divide and Conquer paradigm which helps us to process the data using different machines very quickly.</p> </li> <li> <p>Data Locality: Instead of moving data to the processing unit, we are moving the processing unit to the data in the MapReduce Framework.  In the traditional system, we used to bring data to the processing unit and process it. But, as the data grew and became very huge, bringing this huge amount of data to the processing unit posed the following issues:</p> </li> </ul> <p>Moving huge data to processing is costly and deteriorates the network performance. Processing takes time as the data is processed by a single unit which becomes the bottleneck. The master node can get over-burdened and may fail.</p> <p>Now, MapReduce allows us to overcome the above issues by bringing the processing unit to the data. This allows us to have the following advantages:</p> <ol> <li> <p>It is very cost-effective to move processing unit to the data.</p> </li> <li> <p>The processing time is reduced as all the nodes are working with their part of the data in parallel.</p> </li> <li> <p>Every node gets a part of the data to process and therefore, there is no chance of a node getting overburdened.</p> </li> </ol>"},{"location":"hadoop/mapreduce/#mapreduce-data-flow","title":"MapReduce Data Flow","text":"<ul> <li> <p>Input Files: The data for a MapReduce task is stored in input files, and input files typically lives in HDFS.</p> </li> <li> <p>InputFormat: Now, InputFormat defines how these input files are split and read. It selects the files or other objects that are used for input.</p> </li> <li> <p>InputSplits: It is created by InputFormat, logically represent the data which will be processed by an individual Mapper. One map task is created for each split; thus the number of map tasks will be equal to the number of InputSplits.</p> </li> <li> <p>RecordReader: It communicates with the InputSplit in Hadoop MapReduce and converts the data into key-value pairs suitable for reading by the mapper.</p> </li> <li> <p>Mapper: It processes each input record (from RecordReader) and generates new key-value pair, and this key-value pair generated by Mapper is completely different from the input pair. The output of Mapper is also known as intermediate output which is written to the local disk. The output of the Mapper is not stored on HDFS as this is temporary data and writing on HDFS will create unnecessary copies.</p> </li> <li> <p>Combiner: The combiner is also known as \u2018Mini-reducer\u2019. Hadoop MapReduce Combiner performs local aggregation on the mappers\u2019 output, which helps to minimize the data transfer between mapper and reducer.</p> </li> <li> <p>Partitioner: Hadoop MapReduce, Partitioner comes into the picture if we are working on more than one reducer (for one reducer partitioner is not used). Partitioner takes the output from combiners and performs partitioning. Partitioning of output takes place on the basis of the key and then sorted. By hash function, key (or a subset of the key) is used to derive the partition. According to the key value in MapReduce, each combiner output is partitioned, and a record having the same key value goes into the same partition, and then each partition is sent to a reducer.</p> </li> <li> <p>Shuffling and Sorting: Now, the output is Shuffled to the reduce node (which is a normal slave node but reduce phase will run here hence called as reducer node). The shuffling is the physical movement of the data which is done over the network. Once all the mappers are finished and their output is shuffled on the reducer nodes, then this intermediate output is merged and sorted, which is then provided as input to reduce phase.</p> </li> <li> <p>Reducer: It takes the set of intermediate key-value pairs produced by the mappers as the input and then runs a reducer function on each of them to generate the output. The output of the reducer is the final output, which is stored in HDFS.</p> </li> <li> <p>RecordWriter: It writes these output key-value pair from the Reducer phase to the output files.</p> </li> <li> <p>OutputFormat: The way these output key-value pairs are written in output files by RecordWriter is determined by the OutputFormat.</p> </li> </ul>"},{"location":"hadoop/yarn/","title":"YARN","text":"<p>Apache Hadoop YARN (Yet Another Resource Negotiator) is a resource management layer in Hadoop. YARN came into the picture with the introduction of Hadoop 2.x. It allows various data processing engines such as interactive processing, graph processing, batch processing, and stream processing to run and process data stored in HDFS (Hadoop Distributed File System).</p> <p>In essence, YARN is responsible for managing cluster resources, which include CPU, memory, disk I/O, and network bandwidth. It provides APIs that computation engines use to request and work with Hadoop cluster resources for their task scheduling and resource requirements. It's important to note that YARN APIs are not designed for individual application developers; instead, they target teams creating new computation engines. </p> <p>The ambition of YARN is to consolidate all types of distributed computation capabilities into a single cluster, thereby eliminating the need for multiple clusters and the associated pain of data movement or duplication. Popular execution engines that operate on top of YARN include Apache Spark, Apache Storm, Apache Solr, and Apache Tez. While some NoSQL databases like Cassandra are not yet YARN-enabled, an incubating Apache project called Slider aims to integrate them into a YARN-managed Hadoop cluster, including existing implementations for HBase and Accumulo, without requiring changes to those systems themselves. Apache Slider also seeks to introduce on-demand scale-up and scale-down capabilities, offering elasticity similar to cloud providers.</p>"},{"location":"hadoop/yarn/#components-of-yarn","title":"Components of YARN","text":""},{"location":"hadoop/yarn/#resource-manager","title":"Resource Manager","text":"<p>Resource Manager is the master daemon of YARN. It is responsible for managing several other applications, along with the global assignments of resources such as CPU and memory. It is used for job scheduling.</p> <p>Resource Manager has two components:</p> <ul> <li> <p>Scheduler: Schedulers\u2019 task is to distribute resources to the running applications. It only deals with the scheduling of tasks and hence it performs no tracking and no monitoring of applications.</p> </li> <li> <p>Application Manager: The application Manager manages applications running in the cluster. Tasks, such as the starting of Application Master or monitoring, are done by the Application Manager.</p> </li> </ul>"},{"location":"hadoop/yarn/#node-manager","title":"Node Manager","text":"<p>Node Manager is the slave daemon of YARN. </p> <p>It has the following responsibilities:</p> <ul> <li> <p>Node Manager has to monitor the container\u2019s resource usage, along with reporting it to the Resource Manager.</p> </li> <li> <p>The health of the node on which YARN is running is tracked by the Node Manager.</p> </li> <li> <p>It takes care of each node in the cluster while managing the workflow, along with user jobs on a particular node.</p> </li> <li> <p>It keeps the data in the Resource Manager updated</p> </li> <li> <p>Node Manager can also destroy or kill the container if it gets an order from the Resource Manager to do so.</p> </li> </ul>"},{"location":"hadoop/yarn/#application-master","title":"Application Master","text":"<p>Every job submitted to the framework is an application, and every application has a specific Application Master associated with it. </p> <p>Application Master performs the following tasks:</p> <ul> <li> <p>It coordinates the execution of the application in the cluster, along with managing the faults.</p> </li> <li> <p>It negotiates resources from the Resource Manager.</p> </li> <li> <p>It works with the Node Manager for executing and monitoring other components\u2019 tasks.</p> </li> <li> <p>At regular intervals, heartbeats are sent to the Resource Manager for checking its health, along with updating records according to its resource demands.</p> </li> </ul> <p>Now, we will step forward with the fourth component of Apache Hadoop YARN.</p>"},{"location":"hadoop/yarn/#container","title":"Container","text":"<p>A container is a set of physical resources (CPU cores, RAM, disks, etc.) on a single node.</p> <p>The tasks of a container are listed below:</p> <ul> <li>It grants the right to an application to use a specific amount of resources (memory, CPU, etc.) on a specific host.</li> <li>YARN containers are particularly managed by a Container Launch context which is Container Life Cycle(CLC).This record contains a map of environment variables, dependencies stored in remotely accessible storage, security tokens, the payload for Node Manager services, and the command necessary to create the process.</li> </ul>"},{"location":"hadoop/yarn/#running-and-application-through-yarn","title":"Running and application through YARN","text":"<p>When an application is submitted to YARN, the request goes to the Resource Manager, which then instructs a Node Manager to launch the first container for that application, known as the Application Master. The Application Master then assumes responsibility for executing and monitoring the entire job, with its specific functionality varying depending on the application framework (e.g., MapReduce Application Master functions differently than a Spark Application Master).</p> <p>For a MapReduce application, the Application Master requests more containers from the Resource Manager to initiate map and reduce tasks. Once these containers are allocated, the Application Master directs the Node Managers to launch the containers and execute the tasks. Tasks directly report their status and progress back to the Application Master. Upon completion of all tasks, all containers, including the Application Master, perform necessary cleanup and terminate.</p> <ul> <li> <p>Application Submission: The RM accepts the application, causing the creation of an ApplicationMaster (AM) instance. The AM is responsible for negotiating resources from the RM and working with the Node Managers (NMs) to execute and monitor the tasks.</p> </li> <li> <p>Resource Request: The AM starts by requesting resources from the RM. It specifies what resources are needed, in which locations, and other constraints. These resources are encapsulated in terms of \"Resource Containers\" which include specifications like memory size, CPU cores, etc.</p> </li> <li> <p>Resource Allocation: The Scheduler in the RM, based on the current system load and capacity, as well as policies (e.g., capacity, fairness), allocates resources to the applications by granting containers. The specific strategy depends on the scheduler type (e.g., FIFO, Capacity Scheduler).</p> </li> <li> <p>Container Launching: Post-allocation, the RM communicates with relevant NMs to launch the containers. The Node Manager sets up the container's environment, then starts the container by executing the specified commands.</p> </li> <li> <p>Task Execution: Each container then runs the task assigned by the ApplicationMaster. These are actual data processing tasks, specific to the application's purpose.</p> </li> <li> <p>Monitoring and Fault Tolerance: The AM monitors the progress of each task. If a container fails, the AM requests a new container from the RM and retries the task, ensuring fault tolerance in the execution phase.</p> </li> <li> <p>Completion and Release of Resources: Upon task completion, the AM releases the allocated containers, freeing up resources. After all tasks are complete, the AM itself is terminated, and its resources are also released.</p> </li> <li> <p>Finalization: The client then polls the RM or receives a notification to know the status of the application. Once informed of the completion, the client retrieves the result and finishes the process.</p> </li> </ul>"},{"location":"hive/join/","title":"Joins","text":""},{"location":"hive/join/#primary-key-in-hive","title":"Primary key in Hive","text":"<p>Apache Hive does not enforce primary key constraints natively like traditional RDBMS systems do. Hive is designed to operate on large datasets using a distributed computing approach, which is fundamentally different from how relational databases manage data. However, from Hive 2.1.0 onwards, there is support for defining primary keys during table creation for informational purposes, but they are not enforced by Hive itself.</p>"},{"location":"hive/join/#map-side-join-broadcast-join-in-hive","title":"Map Side Join (Broadcast Join) in Hive","text":"<p>A Map Side Join (also known as Map Join) is an optimized version of the join operation in Hive, where one table is small enough to fit into memory. This smaller table (also known as the dimension table) is loaded into memory, and the larger table (also known as the fact table) is read line by line. Because the join operation occurs at the map phase and doesn't need a reduce phase, it's much faster than a traditional join operation.</p> <p>To use a Map Join, the following properties need to be set:</p> <ol> <li>hive.auto.convert.join: This property should be set to true. It allows Hive to automatically convert a common join into a Map Join based on the sizes of the tables.</li> <li>hive.mapjoin.smalltable.filesize: This property sets the maximum size for the small table that can be loaded into memory. If the size of the table exceeds the value set by this property, Hive won't perform a Map Join. The default value is 25000000 bytes (approximately 25MB).     SET hive.auto.convert.join=true;     SET hive.mapjoin.smalltable.filesize=50000000;  // setting limit to 50MB</li> </ol> <p>Also, keep in mind that if your join operation includes more than two tables, the table in the last position of the FROM clause is considered the large table (fact table), and the other tables are considered small tables  (dimension tables). This matters because Hive attempts to perform the Map Join operation using the last table as the fact table.</p>"},{"location":"hive/join/#bucket-map-join-in-hive","title":"Bucket Map Join in Hive","text":"<p>A Bucket Map Join is an optimization of a Map Join in Hive where both tables are bucketed on the join columns. Instead of loading the entire small table into memory as done in a standard Map Join, the Bucket Map Join only needs to load the relevant bucket from the small table into memory, reducing memory usage and potentially allowing larger tables to be used in a Map Join.</p> <p>To perform a Bucket Map Join, the following conditions need to be satisfied:</p> <p>Both tables should be bucketed on the join column.The number of buckets in the large table should be a multiple of the number of buckets in the small table.</p> <p>To enable Bucket Map Joins, the following properties need to be set:</p> <ol> <li>hive.auto.convert.join: This property should be set to true. It allows Hive to automatically convert a common join into a Map Join based on the sizes of the tables.</li> <li>hive.optimize.bucketmapjoin: This property should be set to true to allow Hive to convert common joins into Bucket Map Joins when possible.     SET hive.auto.convert.join=true;     SET hive.optimize.bucketmapjoin=true;</li> </ol>"},{"location":"hive/join/#sorted-merge-bucket-join-in-hive","title":"Sorted Merge Bucket Join in Hive","text":"<p>A Sorted Merge Bucket (SMB) Join in Hive is an optimization for bucketed tables where not only are the tables bucketed on the join column, but also sorted on the join column. This is similar to the bucket map join, but SMB join does not require one table to fit into memory, making it more scalable.</p> <p>For a Sorted Merge Bucket Join, the following conditions need to be satisfied:</p> <ol> <li>Both tables should be bucketed and sorted on the join column.</li> <li>Both tables should have the same number of buckets.</li> </ol> <p>When these conditions are satisfied, each mapper can read a bucket from each table at a time and perform the join, significantly reducing the disk I/O operations.</p> <p>To enable SMB Joins, the following properties need to be set:</p> <ol> <li>hive.auto.convert.sortmerge.join: This property should be set to true. It allows Hive to automatically convert common joins into Sorted Merge Bucket Joins when possible.</li> <li>hive.optimize.bucketmapjoin.sortedmerge: This property should be set to true to allow Hive to perform a SMB Join.</li> </ol>"},{"location":"hive/join/#skew-join-in-hive","title":"Skew Join in Hive","text":"<p>A Skew Join in Hive is an optimization technique for handling skewed data, where some values appear very frequently compared to others in the dataset. In a typical MapReduce job, skewed data can lead to a few reducers taking much longer to complete than others because they process a majority of the data. This can negatively impact the overall performance of the join.</p> <p>A Skew Join in Hive tries to handle this problem by performing the join in two stages:</p> <ol> <li>In the first stage, Hive identifies the skewed keys and processes all the non-skewed keys.</li> <li>In the second stage, Hive processes the skewed keys. The skewed keys are partitioned into different reducers based on a hash function, thus reducing the burden on a single reducer.</li> </ol> <p>To perform a Skew Join, the following conditions need to be satisfied:</p> <ol> <li>The join should be a two-table join. Currently, Hive does not support multi-table skew joins.</li> <li>There should be skew in key distribution. If the key distribution is uniform, a skew join may not provide any advantage and can be less efficient than a regular join.</li> </ol> <p>To enable Skew Joins, the following properties need to be set:</p> <ol> <li>hive.optimize.skewjoin: This property should be set to true. It enables the skew join optimization.</li> <li>hive.skewjoin.key: This property sets the minimum number of rows for a key to be considered skewed. The default value is 10000.</li> </ol>"},{"location":"hive/overview/","title":"Overview","text":"<p>Apache Hive is an open source data warehouse system built on top of Hadoop HA used for querying and analyzing large datasets stored in Hadoop files.</p> <p>Initially, you have to write complex Map-Reduce jobs, but now with the help of the Hive, you just need to submit merely SQL queries. Hive is mainly targeted towards users who are comfortable with SQL. Hive use language called HiveQL (HQL), which is similar to SQL. HiveQL automatically translates SQL-like queries into MapReduce jobs.</p> <p>Hive abstracts the complexity of Hadoop. The main thing to notice is that there is no need to learn java for Hive. The Hive generally runs on your workstation and converts your SQL query into a series of jobs for execution on a Hadoop cluster. Apache Hive organizes data into tables. This provides a means for attaching the structure to data stored in HDFS.</p>"},{"location":"hive/overview/#hive-architecture","title":"Hive Architecture","text":"<p>The major components of Apache Hive are:</p>"},{"location":"hive/overview/#hive-client","title":"Hive Client","text":"<ul> <li> <p>Thrift Client: The Hive server is based on Apache Thrift so that it can serve the request from a thrift client.</p> </li> <li> <p>JDBC Client: Hive allows for the Java applications to connect to it using the JDBC driver. JDBC driver uses Thrift to communicate with the Hive Server.</p> </li> <li> <p>ODBC Client: Hive ODBC driver allows applications based on the ODBC protocol to connect to Hive. Similar to the JDBC driver, the ODBC driver uses Thrift to communicate with the Hive Server.</p> </li> </ul>"},{"location":"hive/overview/#hive-services","title":"Hive Services","text":"<ul> <li> <p>Beeline: The Beeline is a command shell supported by HiveServer2, where the user can submit its queries and command to the system. It is a JDBC client that is based on SQLLINE CLI (pure Java-console-based utility for connecting with relational databases and executing SQL queries).</p> </li> <li> <p>Hive Server 2 : HiveServer2 is the successor of HiveServer1. HiveServer2 enables clients to execute queries against the Hive. It allows multiple clients to submit requests to Hive and retrieve the final results. It is basically designed to provide the best support for open API clients like JDBC and ODBC. Note: Hive server1, also called a Thrift server, is built on Apache Thrift protocol to handle the cross-platform communication with Hive. It allows different client applications to submit requests to Hive and retrieve the final results. It does not handle concurrent requests from more than one client due to which it was replaced by HiveServer2.</p> </li> <li> <p>Hive Driver: The Hive driver receives the HiveQL statements submitted by the user through the command shell. It creates the session handles for the query and sends the query to the compiler.</p> </li> <li> <p>Hive Compiler: Hive compiler parses the query. It performs semantic analysis and type-checking on the different query blocks and query expressions by using the metadata stored in metastore and generates an execution plan. The execution plan created by the compiler is the DAG(Directed Acyclic Graph), where each stage is a map/reduce job, operation on HDFS, a metadata operation.</p> </li> <li> <p>Optimizer: Optimizer performs the transformation operations on the execution plan and splits the task to improve efficiency and scalability.</p> </li> <li> <p>Execution Engine: Execution engine, after the compilation and optimization steps, executes the execution plan created by the compiler in order of their dependencies using Hadoop.</p> </li> <li> <p>Metastore: Metastore is a central repository that stores the metadata information about the structure of tables and partitions, including column and column type information. It also stores information of serializer and deserializer, required for the read/write operation, and HDFS files where data is stored. This metastore is generally a relational database. Metastore provides a Thrift interface for querying and manipulating Hive metadata.</p> </li> </ul> <p>We can configure metastore in any of the two modes:</p> <ol> <li> <p>Remote: In remote mode, metastore is a Thrift service and is useful for non-Java applications.</p> </li> <li> <p>Embedded: In embedded mode, the client can directly interact with the metastore using JDBC.</p> </li> </ol>"},{"location":"hive/overview/#hive-query-flow","title":"Hive Query Flow","text":"<ul> <li> <p>execututeQuery: Command Line or Web UI sends the query to the Driver (any Hive interface like database driver JDBC, ODBC, etc.) to execute the query.</p> </li> <li> <p>getPlan: The driver takes the help of the query compiler which parses the query to check the syntax and the query plan or the requirement of the query.</p> </li> <li> <p>getMetaData: The compiler sends a metadata request to the Metastore (any database).</p> </li> <li> <p>sendMetaData: Metastore sends the metadata to the compiler in response.</p> </li> <li> <p>sendPlan: The compiler checks the requirement and resends the plan to the driver. The parsing and compiling of a query is complete.</p> </li> <li> <p>executePlan: The driver sends the executing plan to the execution engine.</p> <ol> <li> <p>metaDataOps (On Hive): Meanwhile in execution, the execution engine can execute metadata operations with Metastore.</p> </li> <li> <p>executeJob (on Hadoop): Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in the Name node and it assigns this job to TaskTracker, which is in the Data node. Here, the query executes MapReduce job.</p> </li> <li> <p>job done: After the map-reduce process in Hadoop is finished, it sends a message that the process is finished here</p> </li> <li> <p>dfsOperations: DFS operations using the client\u2019s reported user and group permissions and worked between Execution Engine and NameNode</p> </li> </ol> </li> <li> <p>fetchResults: The driver sends the results to Hive Interfaces.</p> </li> <li> <p>sendResults: The execution engine sends those resultant values to the driver. 9.)   fetchResults: The driver sends the results to HDFS.</p> </li> <li> <p>fetchResults: The driver sends the results to HDFS.</p> </li> </ul>"},{"location":"hive/overview/#plain-text-data-storage-vs-byte-data-storage","title":"Plain Text Data Storage vs Byte Data Storage","text":"<p>Plain Text Storage:</p> <p>In plain text data storage, data is stored as readable characters. For example, consider the number 12345. In plain text, it's stored as the characters '1', '2', '3', '4', and '5'. Each character typically uses 1 byte of memory (in ASCII), or 2 bytes (in UTF-16), so this number would use 5 to 10 bytes of memory. The advantage of plain text storage is that it's human-readable and easy to interpret without any conversion. The disadvantage is that it's not space-efficient. Larger numbers or data types other than integers (like floating-point numbers) will use more space.</p> <p>Byte (Binary) Data Storage:</p> <p>In byte (or binary) data storage, data is stored as binary values, not as readable characters. Each byte consists of 8 bits, and each bit can be either 0 or 1. Using our previous example, the number 12345 can be represented in binary format as 11000000111001, which is 14 bits or 2 bytes (with 6 unused bits). In a more memory-optimized format, it could use only the necessary 14 bits. The advantage of binary storage is that it's very space-efficient. Each type of data (integer, float, etc.) has a standard size, regardless of its value. The disadvantage is that binary data is not human-readable. You need to know the type of data and how it's encoded to convert it back to a readable format.</p>"},{"location":"hive/partition/","title":"Partitioning & bucketing","text":""},{"location":"hive/partition/#partitioning-in-hive","title":"Partitioning in Hive","text":"<p>Apache Hive organizes tables into partitions. Partitioning is a way of dividing a table into related parts based on the values of particular columns like date, city, and department.</p> <p>Each table in the hive can have one or more partition keys to identify a particular partition. Using partition it is easy to do queries on slices of the data.</p> <p> </p> <p>Why is partitioning important?</p> <ol> <li>Speeds Up Data Query: Partitioning reduces data search space for queries, speeding up data retrieval.</li> <li>Reduces I/O Operations: Only relevant data partitions are scanned, reducing unnecessary I/O operations.</li> <li>Improves Query Performance: By limiting data read, partitioning boosts query performance.</li> <li>Saves Resources: Querying only relevant partitions uses fewer computational resources.</li> <li>Manages Large Data Sets: Helps handle large datasets by dividing them into smaller, manageable parts.</li> <li>Filter Data Efficiently: Speeds up queries that commonly filter by certain columns.</li> <li>Enables Scalability: As data grows, new partitions can be added without degradation in performance.</li> <li>Data Management and Archiving: Makes it easier to archive or delete data based on time or other attributes.</li> </ol>"},{"location":"hive/partition/#types-of-partitioning","title":"Types of Partitioning","text":""},{"location":"hive/partition/#static-partitioning","title":"Static Partitioning","text":"<ol> <li>Insert input data files individually into a partition table is Static Partition.</li> <li>Usually when loading files (big files) into Hive tables static partitions are preferred.</li> <li>Static Partition saves your time in loading data compared to dynamic partition.</li> <li>You \u201cstatically\u201d add a partition in the table and move the file into the partition of the table.</li> <li>We can alter the partition in the static partition.</li> <li>Static partition is the default partition strategy in hive</li> <li>Static partition is in Strict Mode.</li> <li>You should use where clause to use limit in the static partition.</li> <li>You can perform Static partition on Hive Manage table or External table.</li> </ol> <p>Note</p> <p>Syntax to load data in Static Partitioned Table</p> <p>LOAD DATA INPATH '/hdfs/path/to/datafile' INTO TABLE employees PARTITION (year='2023');</p> <p>OR</p> <p>INSERT OVERWRITE TABLE employees PARTITION (year='2023') SELECT name, age FROM emp_data WHERE year = '2023';</p>"},{"location":"hive/partition/#dynamic-partitioning","title":"Dynamic Partitioning","text":"<ol> <li>Single insert to partition table is known as a dynamic partition.</li> <li>Usually, dynamic partition loads the data from the non-partitioned table.</li> <li>Dynamic Partition takes more time in loading data compared to static partition.</li> <li>When you have large data stored in a table then the Dynamic partition is suitable.</li> <li>If you want to partition a number of columns but you don\u2019t know how many columns then also dynamic partition is suitable.</li> <li>Dynamic partition there is no required where clause to use limit.</li> <li>We can\u2019t perform alter on the Dynamic partition.</li> <li>You can perform dynamic partition on hive external table and managed table.</li> <li>If you want to use the Dynamic partition in the hive then the mode is in non-strict mode.     SET hive.exec.dynamic.partition = true;     SET hive.exec.dynamic.partition.mode = nonstrict;</li> </ol> <p>Note</p> <p>Syntax to load data in Dynamic Partitioned Table</p> <p>INSERT OVERWRITE TABLE employees PARTITION (year) SELECT name, age, year FROM emp_data</p>"},{"location":"hive/partition/#bucketing-in-hive","title":"Bucketing in Hive","text":"<p>You\u2019ve seen that partitioning gives results by segregating HIVE table data into multiple files only when there is a limited number of partitions, what if partitioning the tables results in a large number of partitions. This is where the concept of bucketing comes in. When a column has a high cardinality, we can\u2019t perform partitioning on it. A very high number of partitions will generate too many Hadoop files which would increase the load on the node. That\u2019s because the node will have to keep the metadata of every partition, and that would affect the performance of that node In simple words, You can use bucketing if you need to run queries on columns that have huge data, which makes it difficult to create partitions.</p> <ol> <li>The concept of bucketing is based on the hashing technique.</li> <li>modules of current column value and the number of required buckets is calculated (let say, F(x) % 3).</li> <li>Based on the resulted value, the data stored into the corresponding bucket.</li> <li>The Records with the same bucketed column stored in the same bucket</li> <li>This function requires you to use the Clustered By clause to divide a table into buckets.</li> </ol> <p> </p>"},{"location":"hive/partition/#difference-between-partitioning-and-bucketing","title":"Difference between Partitioning and Bucketing","text":"<p>Partitioning and bucketing are not dependent on each other, but they can be used together to improve query performance and data management. They serve different purposes and can coexist to complement each other. </p> <p>Using partitioning and bucketing together allows Hive to prune data at the partition level and further organize data within a partition by distributing it into buckets.</p>"},{"location":"hive/partition/#benefits-of-partitioning","title":"Benefits of partitioning","text":"<ol> <li>Filtering: If queries often filter data based on a certain column, partitioning on that column can significantly reduce the amount of data read, thus improving performance. For example, if a table is partitioned by date and queries frequently request data from a specific date, partitioning can speed up these queries.</li> <li>Aggregation: If you're aggregating data based on the partition column, partitioning can optimize these operations by reducing the amount of data Hive needs to read.</li> <li>Data Management: Partitioning helps manage and organize data better. It can be useful for removing or archiving data efficiently. For example, you can quickly drop a partition to delete data for a specific date. </li> </ol>"},{"location":"hive/partition/#benefits-of-bucketing","title":"Benefits of bucketing","text":"<ol> <li>Sampling: Bucketing allows efficient sampling of data. Since each bucket essentially represents a sample of data, you can quickly get a sample by querying a single bucket.</li> <li>Join Operations: Bucketing can be used to perform more efficient map-side joins when joining on the bucketed column. If two tables are bucketed on the join columns and are of similar size, Hive can perform a bucketed map join, which is much faster than a regular join.</li> <li>Handling Skew: If data is skewed (i.e., some values appear very frequently), bucketing can distribute the data more evenly across files, improving query performance.</li> </ol>"},{"location":"hive/serde/","title":"SerDe","text":""},{"location":"hive/serde/#serde-in-hive","title":"SerDe in Hive","text":"<p>SerDe stands for Serializer/Deserializer. It's a crucial component of Hive used for IO operations, specifically for reading and writing data. It helps Hive to read data in custom formats and translate it into a format Hive can process (deserialization) and vice versa (serialization).</p> <p>Role of SerDe in Hive:</p> <ol> <li> <p>When reading data from a table (input to Hive), deserialization is performed by the SerDe.</p> </li> <li> <p>When writing data to a table (output from Hive), serialization is performed by the SerDe.</p> </li> </ol> <p>Serialization - Process of converting an object in memory into bytes that can be stored in a file or transmitted over a network.</p> <p>Deserialization - Process of converting the bytes back into an object in memory.</p> <p>\u201cA select statement creates deserialized data(columns) that is understood by Hive. An insert statement creates serialized data(files) that can be stored into an external storage like HDFS\u201d.</p> <p>In any table definition, there are two important sections. The \u201cRow Format\u201d describes the libraries used to convert a given row into columns. The \u201cStored as\u201d describes the InputFormat and OutputFormat libraries used by map-reduce to read and write to HDFS files.</p> <p></p>"},{"location":"hive/serde/#file-formats-in-hive-with-serde-library","title":"File formats in Hive with SerDe Library","text":""},{"location":"hive/serde/#textfile","title":"Textfile","text":"<p>This is the default file format. Each line in the text file is a record. Hive uses the LazySimpleSerDe for serialization and deserialization. It's easy to use but doesn't provide good compression or the ability to skip over not-needed columns during read.</p>"},{"location":"hive/serde/#json","title":"JSON","text":"<p>Hive supports reading and writing JSON format data. Note that JSON files typically do not have a splittable structure, which can affect performance as only one mapper can read the data when not splittable.</p> <p>Note</p> <p>Default SerDe is org.apache.hive.hcatalog.data.JsonSerDe</p>"},{"location":"hive/serde/#csv","title":"CSV","text":"<p>CSV is a simple, human-readable file format used to store tabular data. Columns are separated by commas, and rows by new lines. CSV files can be read and written in Hive using the</p> <p>Note</p> <p>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</p> <p>org.apache.hadoop.hive.serde2.OpenCSVSerde for advanced CSV parsing.</p> <p>CSV lacks a splittable structure, which may affect performance due to limited parallel processing.</p>"},{"location":"hive/serde/#orcfile-optimized-row-columnar-file","title":"ORCFile (Optimized Row Columnar File)","text":"<p>Introduced by Hortonworks, ORC is a highly efficient way to store Hive data. It provides efficient compression and encoding schemes with enhanced performance to handle complex data types.</p> <p>Note</p> <p>Default SerDe is org.apache.hadoop.hive.ql.io.orc.OrcSerde</p> <p>Built for the Hive query engine, ORC is a columnar storage format that allows Hive to read, write, and process data faster. It allows for efficient compression, which saves storage space, and adds improvements in the speed of data retrieval, making it suitable for performing high-speed queries. It stores collections of rows, not individual rows. Each file consists of row index, column statistics, and stripes (a row of data consisting of several rows) that contain the column data.</p> <p>Supports complex types: Structs, Lists, Maps, and Unions. Also supports advanced features like bloom filters and indexing. A bloom filter is a data structure that can identify whether an element might be present in a set, or is definitely not present. In the context of ORC, bloom filters can help skip unnecessary reads when performing a lookup on a particular column value.</p> <p>An ORC file contains groups of row data called stripes, along with auxiliary information in a file footer. At the end of the file a postscript holds compression parameters and the size of the compressed footer. The default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS. The file footer contains a list of stripes in the file, the number of rows per stripe, and each column's data type. It also contains column-level aggregates count, min, max, and sum.</p> <p></p> <p>Strip structure:  As shown in the diagram, each stripe in an ORC file holds index data, row data, and a stripe footer.  ORC organizes data for each column into streams, the stripe footer records the location and size of these streams within the stripe. This allows the reader to locate and access specific parts of the stripe efficiently.  Row data is used in table scans.  Index data includes min and max values for each column and the row positions within each column. Row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block.  Note that ORC indexes are used only for the selection of stripes and row groups and not for answering queries.</p>"},{"location":"hive/serde/#parquet","title":"Parquet","text":"<p>Columnar storage format, available to any project in the Hadoop ecosystem. It's designed to bring efficient columnar storage of data compared to row-based like CSV or TSV files.</p> <p>Note</p> <p>Default SerDe is org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe</p> <p>It is columnar in nature and designed to bring efficient columnar storage of data. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in comparison to row-based files like CSV.</p> <p>Schema evolution is handled in the file metadata allowing compatible schema evolution. It supports all data types, including nested ones, and integrates well with flat data, semi-structured data, and nested data sources.</p> <p>Parquet is considered a de-facto standard for storing data nowadays</p> <p>Data compression \u2013 by applying various encoding and compression algorithms, Parquet file provides reduced memory consumption</p> <p>Columnar storage \u2013 this is of paramount importance in analytic workloads, where fast data read operation is the key requirement. But, more on that later in the article\u2026</p> <p>Language agnostic \u2013 as already mentioned previously, developers may use different programming languages to manipulate the data in the Parquet file</p> <p>Open-source format \u2013 meaning, you are not locked with a specific vendor</p> <p>Support for complex data types</p> <p>In traditional, row-based storage, the data is stored as a sequence of rows. Something like this:</p> <p></p> <p>Parquet is a columnar format that stores the data in row groups! Wait, what?! Wasn\u2019t it enough complicated even before this? Don\u2019t worry, it\u2019s much easier than it sounds:) Let\u2019s go back to our previous example and depict how Parquet will store this same chunk of data:</p> <p>Let\u2019s stop for a moment and understand above diagram, as this is exactly the structure of the Parquet file (some additional things were intentionally omitted, but we will come soon to explain that as well). Columns are still stored as separate units, but Parquet introduces additional structures, called Row group.</p> <p>Why is this additional structure super important?</p> <p>In OLAP scenarios, we are mainly concerned with two concepts: projection and predicate(s). Projection refers to a SELECT statement in SQL language \u2013 which columns are needed by the query. Back to our previous example, we need only the Product and Country columns, so the engine can skip scanning the remaining ones.</p> <p>Predicate(s) refer to the WHERE clause in SQL language \u2013 which rows satisfy criteria defined in the query. In our case, we are interested in T-Shirts only, so the engine can completely skip scanning Row group 2, where all the values in the Product column equal socks!</p> <p>This means, every Parquet file contains \u201cdata about data\u201d \u2013 information such as minimum and maximum values in the specific column within the certain row group. Furthermore, every Parquet file contains a footer, which keeps the information about the format version, schema information, column metadata, and so on.</p> <p></p>"},{"location":"hive/serde/#avro","title":"AVRO","text":"<p>It's a row-oriented format that is highly splittable. It also supports schema evolution - you can have Avro data files where each file has a different schema but all are part of the same table.</p> <p>Note</p> <p>Default SerDe is org.apache.hadoop.hive.serde2.avro.AvroSerDe</p> <ul> <li> <p>Schema-Based:      Avro uses a schema to define the structure of the data. The schema is written in JSON and is included in the serialized data, allowing data to be self-describing and ensuring that the reader can understand the data structure without external information.</p> </li> <li> <p>Compact and Fast:      Avro data is serialized in a compact binary format, which makes it highly efficient in terms of both storage and transmission.</p> </li> <li> <p>Compression:      Avro supports various compression codes such as Snappy, Deflate, Bzip2, Xz</p> </li> <li> <p>Schema Evolution:      Avro supports schema evolution, allowing the schema to change over time without breaking compatibility with old data. This is particularly useful in big data environments where data structures might evolve.</p> </li> <li> <p>Rich Data Structures:      Avro supports complex data types, including nested records, arrays, maps, and unions, allowing for flexible and powerful data modelling.</p> </li> <li> <p>Interoperability:      Avro is designed to work seamlessly with other big data tools and frameworks, especially within the Hadoop ecosystem, such as Apache Hive, Apache Pig, and Apache Spark.</p> </li> <li> <p>Language Agnostic:      Avro has libraries for many programming languages, including Java, C, C++, Python, and more, enabling cross-language data exchange.</p> </li> </ul> <p></p>"},{"location":"hive/serde/#orc-vs-parquet-vs-avro","title":"ORC vs Parquet vs AVRO","text":""},{"location":"hive/serde/#how-to-decide-which-file-format-to-choose","title":"How to decide which file format to choose?","text":"<ul> <li> <p>Columnar vs Row-based: Columnar storage like Parquet and ORC is efficient for read-heavy workloads and is especially effective for queries that only access a small subset of total columns, as it allows skipping over non-relevant data quickly. Row-based storage like Avro is typically better for write-heavy workloads and for queries that access many or all columns of a table, as all of the data in a row is located next to each other.</p> </li> <li> <p>Schema Evolution: If your data schema may change over time, Avro is a solid choice because of its support for schema evolution. Avro stores the schema and the data together, allowing you to add or remove fields over time. Parquet and ORC also support schema evolution, but with some limitations compared to Avro.</p> </li> <li> <p>Compression: Parquet and ORC, being columnar file formats, allow for better compression and improved query performance as data of the same type is stored together. Avro also supports compression but being a row-based format, it might not be as efficient as Parquet or ORC.</p> </li> <li> <p>Splittability: When compressed, splittable file formats can still be divided into smaller parts and processed in parallel. Parquet, ORC, and Avro are all splittable, even when compressed.</p> </li> <li> <p>Complex Types and Nested Data: If your data includes complex nested structures, then Parquet is a good choice because it provides efficient encoding and compression of nested data.</p> </li> </ul>"},{"location":"ignite/ignite/","title":"Apache Ignite","text":"<p>Apache Ignite provides a convenient and easy-to-use interface for developers to work with large-scale data sets in real time and other aspects of in-memory computing.</p>"},{"location":"ignite/ignite/#features","title":"Features","text":"<p>Apache Ignite has the following features:</p> <ol> <li>Data grid</li> <li>Compute grid</li> <li>Service grid</li> <li>Bigdata accelerator</li> <li>Streaming grid</li> </ol> <p>The following figure illustrates the basic features of Apache Ignite:</p>"},{"location":"ignite/ignite/#primary-capabilities","title":"Primary Capabilities","text":"<ul> <li>Elasticity: An Apache Ignite cluster can grow horizontally by adding new nodes.</li> <li>Persistence: Apache Ignite data grid can persist cache entries in RDBMS, even in NoSQL like MongoDB or Cassandra.</li> <li>Cache as a Service (CaaS): Apache Ignite supports Cache-as-a-Service across the organization which allows multiple applications from different departments to access managed in-memory cache instead of slow disk base databases.</li> <li>2\u207f Level Cache: Apache Ignite is the perfect caching tier to use as a 2\u207f level cache in Hibernate and MyBatis.</li> <li>High-performance Hadoop accelerator: Apache Ignite can replace Hadoop task tracker and job tracker and HDFS to increase the performance of big data analysis.</li> <li>Share state in-memory across Spark applications: Ignite RDD allows easily sharing of state in-memory between different Spark jobs or applications. With Ignite in-memory shared RDD's, any Spark application can put data into Ignite cache which will be accessible by another Spark application later.</li> <li>Distributed computing: Apache Ignite provides a set of simple APIs that allows a user to distribute computation and data processing across multiple nodes in the cluster to gain high performance. Apache Ignite distributed services is very useful to develop and execute microservice like architecture.</li> <li>Streaming: Apache Ignite allows processing continuous never-ending streams of data in scalable and fault-tolerant fashion in-memory, rather than analyzing the data after it has been stored in the database.</li> </ul>"},{"location":"ignite/ignite/#caching-topology","title":"Caching Topology","text":"<p>Ignite provides three different approaches to caching topology: Partitioned, Replicated and Local. A cache mode is configured for each cache individually. Every caching topology has its own goal with pros and cons. The default cache topology is partitioned, without any backup option.</p> <ol> <li>Partitioned: The goal of this topology is to get extreme scalability. In this mode, the Ignite cluster transparently partitions the cached data to distribute the load across an entire cluster evenly. By partitioning the data evenly, the size of the cache and the processing power grows linearly with the size of the cluster. The responsibility for managing the data is automatically shared across the cluster. Every node or server in the cluster contains its primary data with a backup copy if defined.</li> <li> <p>With partitioned cache topology, DML operations on the cache are extremely fast, because only one primary node (optionally 1 or more backup node) needs to be updated for every key. For high availability, a backup copy of the cache entry should be configured. The backup copy is the redundant copy of one or more primary copies, which will live in another node.</p> </li> <li> <p>Replicated: The goal of this approach is to get extreme performance. With this approach, cache data is replicated to all members of the cluster. Since the data is replicated to each cluster node, it is available for use without any waiting. This provides highest possible speed for read-access; each member accesses the data from its own memory. The downside is that frequent writes are very expensive. Updating a replicated cache requires pushing the new version to all other cluster members. This will limit the scalability if there are a high frequency of updates.</p> </li> <li> <p>Local: This is a very primitive version of cache mode; with this approach, no data is distributed to other nodes in the cluster. As far as the Local cache does not have any replication or partitioning process, data fetching is very inexpensive and fast. It provides zero latency access to recently and frequently used data. The local cache is mostly used in read-only operations. It also works very well for read/write-through behavior, where data is loaded from the data sources on cache misses. Unlike a distributed cache, local cache still has all the features of distributed cache; it provides query caching, automatic data eviction and much more.</p> </li> </ol>"},{"location":"ignite/ignite/#caching-strategy","title":"Caching Strategy","text":"<p>With the explosion of high transactions web applications and mobile apps, data storage has become the main bottleneck of performance. In most cases, persistence stores such as relational databases cannot scale out perfectly by adding more servers. In this circumstance, in-memory distributed cache offers an excellent solution to data storage bottleneck. It extends multiple servers (called a grid) to pool their memory together and keep the cache synchronized across all servers. There are two main strategies to use in a distributed in-memory cache.</p> <ol> <li>Cache-aside: In this approach, an application is responsible for reading and writing from the persistence store. The cache doesn't interact with the database at all. This is called cache-aside. The cache behaves as a fast scaling in-memory data store. The application checks the cache for data before querying the data store. Also, the application updates the cache after making any changes to the persistence store. However, even though cache-aside is very fast, there are quite a few disadvantages with this strategy. Application code can become complex and may lead to code duplication if multiple applications deal with the same data store. When there are cache data misses, the application will query the data store, update the caches and continue processing. This can result in multiple data store visits if different application threads perform this processing at the same time.</li> <li>Read-through and Write-through: This is where application treats in-memory cache as the main data store, and reads data from it and writes data to it. In-memory cache is responsible for propagating the query to the data store on cache misses. Also, the data will be updated automatically whenever it is updated in the cache. All read-through and write-through operations will participate in the overall cache transaction and will be committed or rolled back as a whole. Read-through and write-through have numerous advantages over cache-aside. First of all, it simplifies application code. Read-through allows the cache to reload objects from the database when it expires automatically. This means that your application does not have to hit the database in peak hours because the latest data is always in the cache.</li> <li>Write behind: It is also possible to use write-behind to get better write performance. Write-behind lets your application quickly update the cache and return. It then aggregates the updates and asynchronously flushes them to persistence store as a bulk operation. Also with Write-behind, you can specify throttling limits, so the database writes are not performed as fast as the cache updates and therefore the pressure on the database is lower. Additionally, you can schedule the database writes to occur during off-peak hours, which can minimize the pressure on the Database.</li> </ol>"},{"location":"ignite/ignite/#cap-theorem-and-where-does-ignite-stand","title":"CAP Theorem and Where Does Ignite Stand","text":"<ul> <li>CA system: In this approach, you sacrifice partition tolerance for getting consistency and availability. Your database system offers transactions, and the system is highly available. Most of the relational databases are classified as CA systems. This system has serious problems with scaling.</li> <li>CP system: the opposite of the CA system. In CP system availability is sacrificed for consistency and partition-tolerance. In the event of the node failure, some data will be lost.</li> <li>AP system: This system is always available and partitioned. Also this system scales easily by adding nodes to the cluster. Cassandra is a good example of this type of system.</li> </ul> <p>Now, we can return back to our question, where does Ignite stand in the CAP theorem? At first glance, Ignite can be classified by CP, because Ignite is fully ACID compliant distributed transactions with partitioned tolerance. But this is half part of the history. Apache Ignite can also be considered an AP system. But why does Ignite have two different classifications? Because it has two different transactional modes for cache operations, transactional and atomic. In transactional mode, you can group multiple DML operations in one transaction and make a commit into the cache. In this scenario, Ignite will lock data on access by a pessimistic lock. If you configure backup copy for the cache, Ignite will use 2p commit protocol for its transaction. On the other hand, in atomic mode Ignite supports multiple atomic operations, one at a time. In the atomic mode, each DML operation will either succeed or fail and neither Read nor Write operation will lock the data at all. This mode gives a higher performance than the transactional mode. When you make a write in Ignite cache, for every piece of data there will be a master copy in primary node and a backup copy (if defined). When you read data from Ignite grid, you always read from the Primary node, unless the Primary node is down, at which time data will be read from the backup. From this point of view, you gain the system availability and the partition-tolerance of the entire system as an AP system. In the atomic mode, Ignite is very similar to Apache Cassandra. However, real world systems rarely fall neatly into all of these above categories, so it's more helpful to view CAP as a continuum. Most systems will make some effort to be consistent, available, and partition tolerant, and many can be tuned depending on what's most important.</p>"},{"location":"ignite/ignite/#zero-spof","title":"Zero SPOF","text":"<p>In any distributed system, node failure should be expected, particularly as the size of the cluster grows. The Zero Single Point of Failure (SPOF) design pattern ensures that no single part of a system can stop the entire cluster or system from working. Sometimes, the system using master-slave replication or the mixed master-master system falls into this category. Prior to Hadoop 2.0.0, the Hadoop NameNode was an SPOF in an HDFS cluster. Netflix has calculated the revenue loss for each ms of downtime or latency, and it is not small at all. Most businesses do not want single points of failure for the obvious reason. Apache Ignite, as a horizontally scalable distributed system, is designed in such way that all nodes in the cluster are equal, you can read and write from any node in the cluster. There are no master-slave communications in the Ignite cluster. Data is backed up or replicated across the cluster so that failure of any node doesn't bring down the entire cluster or the application. This way Ignite provides a dynamic form of High Availability. Another benefit of this approach is the ease at which new nodes can be added. When new nodes join the cluster, they can take over a portion of data from the existing nodes. Because all nodes are the same, this communication can happen seamlessly in a running cluster.</p>"},{"location":"ignite/ignite/#how-sql-queries-work-in-ignite","title":"How SQL Queries Work in Ignite","text":"<p>In chapter one, we introduced Ignite SQL query feature very superficially. In chapter four, we will go into more details about Ignite SQL queries. It's interesting to know how a query processes under the hood of Ignite. There are two main approaches to process SQL queries in Ignite:</p> <ul> <li>In-memory Map-Reduce: If you are executing any SQL query against a Partitioned cache, Ignite under the hood splits the query into in-memory map queries and a single reduce query. The number of map queries depends on the size of the partitions and number the partitions in the cluster. Then all map queries are executed on all data nodes of the participating caches, providing results to the reducing node, which will, in turn, run the reduce query over these intermediate results. If you are not familiar with the Map-Reduce pattern, you can imagine it as a Java Fork-join process.</li> <li>H2 SQL engine: if you are executing SQL queries against Replicated or Local cache, Ignite knows that all data is available locally and runs a simple local SQL query in the H2 database engine. Note that, in replicated caches, every node contains a replica data for other nodes. H2 database is a free database written in Java and can work in an embedded mode. Depending on the configuration, every Ignite node can have an embedded H2 SQL engine.</li> </ul>"},{"location":"ignite/ignite/#performance-tuning-sql-queries","title":"Performance Tuning SQL Queries","text":"<p>There are a few principles you should follow or consider when using SQL queries against Ignite cache:</p> <ul> <li>Carefully use the index, Indexes also consumes memory (on-heap/off-heap). Also, each index needs to be updated separately. If you have a huge update on a cache, index update can seriously decrease your application performance.</li> <li>Index only fields, that are participating in SQL WHERE clause.</li> <li>Do not overuse the non-collocated distributed joins approach in practice because the performance of this type of joins is worse than the performance of the affinity collocation-based joins due to the fact that there will be much more network round-trips and data movement between the nodes to fulfill a query.</li> <li>In SQL projection statement, select fields that you exactly needs. Extra fields often increase the data roundtrip over the network.</li> <li>If the query is using operator OR then it may use indexes in a way you not would expect. For example, for query <code>select name from Person where sex='M' and (age = 20 or age = 30)</code> index on field age will not be used even if it is obviously more selective than index on field sex and thus is preferable. To workaround this issue you have to rewrite the query with UNION ALL (notice that UNION without ALL will return DISTINCT rows, which will change query semantics and introduce additional performance penalty) like <code>select name from Person where sex='M' and age = 20 UNION ALL select name from Person where sex='M' and age = 30</code>. This way indexes will be used correctly.</li> <li>If query contains operator IN then it has two problems: it is impossible to provide variable list of parameters (you have to specify the exact list in query like <code>where id in (?, ?, ?)</code>), but you can not write it like <code>where id in ?</code> and pass array or collection) and this query will not use index. To work around both problems, you can rewrite the query in the following way: <code>select p.name from Person p join table(id bigint = ?) i on p.id = i.id</code>. Here you can provide object array (Object[]) of any length as a parameter, and the query will use an index on field id. Note that primitive arrays (int[], long[], etc..) can not be used with this syntax, you have to pass an array of boxed primitives.</li> </ul>"},{"location":"ignite/ignite/#expiration-eviction-of-cache-entries-in-ignite","title":"Expiration &amp; Eviction of Cache Entries in Ignite","text":"<p>Apache Ignite caches are very powerful and can be configured and tuned to suit the needs of most applications. Apache Ignite provides two different approaches for data refreshing, which refers to an aspect of a copy of data (e.g,. entries in a cache) being up-to-date with the source version of the data. A stale copy of the data is considered to be out of use. Expiration and Evictions of cache entries is one of the key aspects of caching.</p> <p>Expiration:</p> <p>Usually, the purpose of a cache is to store short-lived data that needs to refresh regularly. You can use Apache Ignite expiry policy to store entry only for a certain period of time. Once expired, the entry is automatically removed from the cache. For instance, the cache could be configured to expire entries ten seconds after they are put in. Sometimes, it is called Time-to-live or TTL. Or to expire 20 seconds after the last time the entry was accessed or retrieve from the cache. Apache Ignite provides five differents predefined expire policy as follows:</p> <ol> <li>CreatedExpiryPolicy - Defines the expiry of a Cache Entry based on when it was created. An update does not reset the expiry time</li> <li>AccessedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was accessed. Accessed does not include a cache update.</li> <li>ModifiedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was updated. Updating includes created and changing (updating) an entry.</li> <li>TouchedExpiryPolicy - Defines the expiry of a Cache. Entry based on when it was last touched. A touch includes creation, update or, access.</li> <li>EternalExpiryPolicy - Specifies that Cache Entries won't expire. This, however, doesn't mean they won't be evicted if an underlying implementation needs to free-up resources whereby it may choose to evict entries that are not due to expire.</li> <li>CustomExpiryPolicy - Implements javax.cache.expiry interface, which defines functions to determine when cache entries will, expire based on creation, access and modification operations.</li> </ol> <p>Eviction:</p> <p>Usually, caches are unbounded, i.e. they grow indefinitely and it is up to the application to removed unneeded cache entries. A cache eviction algorithm is a way of deciding which entries to evict (removed) when the cache is full. However, when maximum on-heap memory is full, entries are evicted into the off-heap memory, if one is enabled. Some eviction policies support batch eviction and eviction by memory size limit. If a batch eviction is enabled then eviction starts when cache size (batchSize) is greater than the maximum cache size. In this cases, batchSize entries will be evicted. If eviction by memory size limit is enabled, then eviction starts when the size of cache entries in bytes becomes greater than the maximum memory size.</p> <ol> <li>LRU: This eviction policy is based on LRU algorithm. The oldest element is the Less Recently Used (LRU) element gets evicted first. The last used timestamp is updated when an element is put into the cache, or an element is retrieved from the cache with a get call. This algorithm takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This eviction policy supports batch eviction and eviction by memory size limits. This eviction policy is suitable for most of all applications and recommended by Apache Ignite.</li> <li>FIFO: Elements are evicted in the same order as they come in. When a put call is made for a new element (and assuming that the max limit is reached for the memory store), the element that was placed first (First-In) in the store is the candidate for eviction (First-Out). This algorithm is used if the use of an element makes it less likely to be used in the future. An example here would be an authentication cache. It takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This implementation is very efficient since it does not create any additional table-like data structures. The ordering information is maintained by attaching ordering metadata to cache entries. This eviction policy supports batch eviction and eviction by memory size limit.</li> <li>Sorted: Sorted eviction policy is similar to FIFO eviction policy with the difference that the entries order is defined by default or user defined comparator and ensures that the minimal entry (i.e. the entry that has integer key with the smallest value) gets evicted first. Default comparator uses cache entries keys for comparison that imposes a requirement for keys to implementing Comparable interface. The user can provide own comparator implementation which can use keys, values or both for entries comparison. Supports batch eviction and eviction by memory size limit.</li> <li>Random: This cache eviction policy selects random cache entry for eviction if cache size exceeds the getMaxSize parameter. This implementation is extremely light weight, lock-free, and does not create any data structures to maintain any order for eviction. Random eviction will provide the best performance over any key queue in which every key has the same probability of being accessed. This eviction policy implementation doesn't support near cache and doesn't work on client nodes. This eviction policy is mainly used for debugging and benchmarking purposes. </li> </ol>"},{"location":"ignite/ignite/#discovery-and-communication-mechanisms","title":"Discovery and communication mechanisms","text":"<p>Any distributed system that scales linearly has at least two mechanisms: one for communication with each other, and another to discover other nodes in the cluster. These two mechanisms or techniques are the backbone of any cluster that scale-out horizontally when needed. Also, they are responsible for forming the cluster, adding new nodes, handling failures or passing messages to the nodes.</p> <p>Discovery mechanism features:</p> <ol> <li>Connect a new node to the cluster topology.</li> <li>Disconnect a node from the cluster.</li> <li>Maintains the order in which nodes connected/disconnected to or from the cluster.</li> <li>Ability to send user messages through the cluster.</li> <li>Ability to set an authenticator that will validate the connected nodes</li> </ol> <p>Ignite provides DiscoverySpi Java interface that allows discovering remote nodes in the cluster. Moreover, Ignite provides two specific DiscoverySpi implementations: TcpDiscoverySpi and ZookeeperDiscoverySpi for different scenarios.</p> <ol> <li>TCP/IP discovery: is the default implementation of the Ignite DiscoverySpi interface that allows all nodes (with enabling multicast) to discover each other inside the same network. This specific implementation uses TCP/IP for node discovery, designed, and optimized for 10s and 100-300 of nodes deployment. When using this implementation Ignite forms a ring- shaped topology. So, almost all network exchanges (except few cases) is done through it. TcpDiscoverySpi uses IP finder (TcpDiscoveryIpFinder) to share and store information about nodes IP addresses. At startup, TcpDiscoverySpi tries to send messages to random IP address taken from the TcpDiscoveryIpFinder about self-start.TcpDiscoverySpi uses local port range to discover the nodes by default. The default local port range is 100. It is not necessary to open the entire range of discovery port in the range from 47500 to 47600 in each member of the Ignite cluster.Note that you are only required to provide at least one IP address of a remote node with the TcpDiscoveryVmIpFinder, but usually, it is advisable to provide 2 or 3 addresses of grid nodes that you plan to start at some point of time in the future. Ignite will automatically discover all other grid nodes once a connection to any of the provided IP addresses is established. The TcpDiscoveryVmIpFinder uses in non-shared mode by default. The list of IP addresses should contain an address of the local node as well if you plan to start a server node in this mode. It will let the node not to wait while other nodes join the cluster but instead it becomes the first cluster node and operate usually. Otherwise, you might get into the situation like that, where one node waits for the other nodes while joining to the cluster. However, you can use the combination of both Multicast and Static IP based discovery together.The Apache Ignite TCP/IP Discovery SPI has a few major drawbacks. The transmission time of the messages between nodes is directly proportional to the number of the nodes in the cluster. It means that an Ignite cluster with more than 100 nodes may take a few more seconds for a system message to traverse through the cluster events such as joining of a new node or detecting a split-brain situation can take a while, which can affect the overall performance of the cluster. Therefore, this implementation is not an optimal solution for a large cluster</li> <li>ZooKeeper discovery: It\u2019s proposed to move from the ring topology to the star topology to overcome the above disadvantages, in the center of which the Zookeeper\u2075\u2079 service is used. At the same time, the Zookeeper cluster appears as the connection and synchronization point for the Ignite cluster. Such an implementation allows scaling Ignite cluster to 100s and 1000s of nodes preserving linear scalability and performance.The basic idea behind the discovery service through Zookeeper is that for each node to be able to identify its current state and store that information into the centralized place. Primary usages of such storage are to provide as a minimum, IP and Port number of the node to all interested nodes that might need to communicate with it. This data is often extended with other types of the data such as sequence order of a node that how it connected to the cluster. ZooKeeper cluster is used as primary storage of information for the current topology states. It also stores attributes of the nodes (user-defined attributes),the order that a node connected to the cluster, queues for storing user-defined events. All the messages about topology exchanges through Zookeeper, nodes are not communicated directly with each other. So, ZooKeeper cluster is used as primary storage of information for the current topology states in this implementation. Also, it stores attributes of the nodes (user-defined attributes), the order that a node connected to the cluster, queues for storing user-defined events. All the messages about topology exchanges through Zookeeper, nodes are not communicated directly with each other.Another essential functionality of ZooKeeper is the mechanism of notifying clients about changes. ZooKeeper allows you to store arbitrary, client information directly in the service. The recorded information can be accessed by all ZooKeeper clients once it save. ZooKeeper also provides opportunities to handle the split-brain scenario and network segmentation.</li> </ol> <p>Communication: Besides to discover nodes in a cluster, there are still needs for some direct communication between nodes for sending and receiving messages such as task execution, monitoring partition exchanges, etc. Ignite provides a CommunicationSpi which is responsible for peer- to-peer communication and data exchanges between nodes. Ignite CommunicationSpi is one of the most crucial SPI in Ignite. It is used heavily throughout the system and provides means for all data exchanges between nodes, such as internal implementation details and user-driven messages.Ignite comes with a built-in CommunicationSpi implementation: TcpCommunicationSpi, which uses TCP/IP protocols and Java NIO to communicate with other nodes.TcpCommunicationSpi uses IP address and port of the local node attributes to communicate with other nodes. At startup, this SPI tries to start listening to a local port specified by the configuration. SPI will automatically increment the port number until it can successfully bind for listening if the local port is occupied. TcpCommunicationSpi caches connections to remote nodes, so it does not have to reconnect every time a message is sent. Idle connections are kept active for 10 minutes by default, and they are closed. You can configure the idle connection timeout by the setIdleConnectionTimeout parameter.</p>"},{"location":"ignite/ignite/#client-connectors-variety","title":"Client Connectors Variety","text":"<ul> <li>Ignite Client connectors</li> </ul>"},{"location":"ignite/igniteinstall/","title":"Igniteinstall","text":""},{"location":"ignite/igniteinstall/#prerequisites","title":"Prerequisites","text":"<ul> <li>3 VMs (Ensure pre-requisites like SELinux disabled, firewall off, THP disabled, etc.)</li> <li>Java 11 or higher</li> <li>Enough RAM for data</li> <li>Network connectivity</li> </ul>"},{"location":"ignite/igniteinstall/#ignite-installation","title":"Ignite Installation","text":"<p>Download tar file from apache ignite</p> <pre><code>Download ignite 2 bin file from https://ignite.apache.org/download.cgi\nunzip apache-ignite-2.8.1-bin\n</code></pre> <p>Set following properties</p> <pre><code>export IGNITE_JMX_PORT=9999\n#ignite global index inline size in bytes\n#default is 10\nexport IGNITE_MAX_INDEX_PAYLOAD_SIZE=100\n\n#Set the following in bin/ignite.sh\nJVM_OPTS=\"$JVM_OPTS -Xms8g -Xmx8g -server -XX:MaxMetaspaceSize=256m -XX:+UseG1GC -XX:+DisableExplicitGC -XX:+AlwaysPreTouch -XX:+ScavengeBeforeFullGC -XX:MaxDirectMemorySize=2048m\"\n</code></pre> <p>Setup igniteconfig.xml</p> <pre><code>Copy the following xml in ignitework directory\nwget https://github.com/manish-chet/BigDataSetupfiles/tree/main/ignite/config\nChange the hosts on specific server\n</code></pre> <p>start the node</p> <p>nohup ignite.sh config/igniteconfig.xml &gt; out.log 2&gt;&amp;1  &amp;</p> <p>Check the node using baseline</p> <pre><code>[ignite@hostname ~]$ control.sh --baseline\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.ignite.internal.util.GridUnsafe$2 (file:/home/ignite/apache-ignite-2.8.1-bin/libs/ignite-core-2.8.1.jar) to field java.nio.Buffer.address\nWARNING: Please consider reporting this to the maintainers of org.apache.ignite.internal.util.GridUnsafe$2\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nControl utility [ver. 2.8.1#20200521-sha1:86422096]\n2020 Copyright(C) Apache Software Foundation\nUser: ignite\nTime: 2025-05-29T19:11:31.029058\nCommand [BASELINE] started\nArguments: --baseline\n--------------------------------------------------------------------------------\nCluster state: active\nCurrent topology version: 3\nBaseline auto adjustment enabled: softTimeout=0\nBaseline auto-adjust are not scheduled\nCurrent topology version: 3 (Coordinator: ConsistentId=ign01, Order=1)\nBaseline nodes:\nConsistentId=ign01, State=ONLINE, Order=1\nConsistentId=ign02, State=ONLINE, Order=3\nConsistentId=ign03, State=ONLINE, Order=2\n--------------------------------------------------------------------------------\nNumber of baseline nodes: 3\nOther nodes not found.\nCommand [BASELINE] finished with code: 0\nControl utility has completed execution at: 2025-05-29T19:11:31.295162\nExecution time: 266 ms\n</code></pre>"},{"location":"kafka/architecture/","title":"Architecture","text":""},{"location":"kafka/architecture/#kafka-architecture","title":"Kafka Architecture","text":""},{"location":"kafka/architecture/#message","title":"Message","text":"<p>A message is a primary unit of data within Kafka. Messages sent from a producer consist of the following parts:</p> <p></p> <p>Message Key (optional): Keys are commonly used when ordering or grouping related data. For example, in a log processing system, we can use the user ID as the key to ensure that all log messages for a specific user are processed in the order they were generated.</p> <p>Message Value: It contains the actual data we want to transmit. Kafka does not interpret the content of the value. It is received and sent as it is. It can be XML, JSON, String, or anything. Kafka does not care and stores everything. Many Kafka developers favor using Apache Avro, a serialization framework initially developed for Hadoop.</p> <p>Timestamp (Optional): We can include an optional timestamp in a message indicating its creation timestamp. It is useful for tracking when events occurred, especially in scenarios where event time is important.</p> <p>Compression Type (Optional): Kafka messages are generally small in size, and sent in a standard data format, such as JSON, Avro, or Protobuf. Additionally, we can further compress them into gzip, lz4, snappy or zstd formats.</p> <p>Headers (Optional): Kafka allows adding headers that may contain additional meta-information related to the message.</p> <p>Partition and Offset Id: Once a message is sent into a Kafka topic, it also receives a partition number and offset id that is stored within the message.</p>"},{"location":"kafka/architecture/#kafka-cluster","title":"Kafka Cluster","text":"<p>A Kafka cluster is a system of multiple interconnected Kafka brokers (servers). These brokers cooperatively handle data distribution and ensure fault tolerance, thereby enabling efficient data processing and reliable storage.</p>"},{"location":"kafka/architecture/#kafka-broker","title":"Kafka Broker","text":"<p>A Kafka broker is a server in the Apache Kafka distributed system that stores and manages the data (messages). It handles requests from producers to write data, and from consumers to read data. Multiple brokers together form a Kafka cluster.</p>"},{"location":"kafka/architecture/#kafka-zookeeper","title":"Kafka Zookeeper","text":"<p>Apache ZooKeeper is a service used by Kafka for cluster coordination, failover handling, and metadata management. It keeps Kafka brokers in sync, manages topic and partition information, and aids in broker failure recovery and leader election.</p>"},{"location":"kafka/architecture/#kafka-producer","title":"Kafka Producer","text":"<p>In Apache Kafka, a producer is an application that sends messages to Kafka topics. It handles message partitioning based on specified keys, serializes data into bytes for storage, and can receive acknowledgments upon successful message delivery. Producers also feature automatic retry mechanisms and error handling capabilities for robust data transmission.</p>"},{"location":"kafka/architecture/#kafka-consumer","title":"Kafka Consumer","text":"<p>A Kafka consumer is an application that reads (or consumes) messages from Kafka topics. It can subscribe to one or more topics, deserializes the received byte data into a usable format, and has the capability to track its offset (the messages it has read) to manage the reading position within each partition. It can also be part of a consumer group to share the workload of reading messages.</p>"},{"location":"kafka/consumer/","title":"Consumers","text":""},{"location":"kafka/consumer/#consumers","title":"Consumers","text":"<p>Consumers are applications that read or consume data from the topics using the Consumer API . Consumers can read data from the topic level (accessing all partitions of a topic) or from specific partitions . Consumers are always associated with a consumer group .</p> <p>Illustrative Example: A mobile news app that displays articles to users would be a consumer. If it's configured to show \"sports news,\" it consumes messages from the sports_news topic.</p> <p>Consumer Groups</p> <p>A consumer group is a group of related consumers that perform a task . Each message in a partition is consumed by only one consumer within a consumer group, ensuring load balancing and avoiding duplicate processing within that group. Multiple groups can consume the same message.</p> <p>Illustrative Example: You might have a \"mobile app news feed\" consumer group and a \"website news archive\" consumer group. Both groups consume the same news articles, but within the \"mobile app\" group, if there are multiple instances of the app running, they will share the load of processing new articles.</p> <p>Consumer groups in Apache Kafka have several key advantages:</p> <ol> <li>Load Balancing: Consumer groups allow the messages from a topic's partitions to be divided among multiple consumers in the group. This effectively balances the load and allows for higher throughput.</li> <li>Fault Tolerance: If a consumer in a group fails, the partitions it was consuming from are automatically reassigned to other consumers in the group, ensuring no messages are lost or left unprocessed.</li> <li>Scalability: You can increase the processing speed by simply adding more consumers to a group. This makes it easy to scale your application according to the workload.</li> <li>Parallelism: Since each consumer in a group reads from a unique set of partitions, messages can be processed in parallel, improving the overall speed and efficiency of data processing.</li> <li>Ordering Guarantee: Within each partition, messages are consumed in order. As a single partition is consumed by only one consumer in the group, this preserves the order of messages as they were written into the partition.</li> </ol> <p>Consumer Configurations</p> <ol> <li>Key Deserializer: This refers to the deserializer class used for keys, which must implement the org.apache.kafka.common.serialization.Deserializer interface .</li> <li>Group ID: A unique string (group.id) identifies the consumer group to which a consumer belongs . This property is essential if the consumer utilizes group management technology, the offset commit API, or a topic-based offset management strategy .</li> <li>fetch.min.bytes: This parameter sets the minimum amount of data the server should return for a fetch request . If the available data is less than this threshold, the request will wait for more data to accumulate . This strategy reduces the number of requests to the broker . The request will block until fetch.min.bytes data is available or the fetch.max.wait.ms timeout expires . While this can cause fetches to wait for larger data amounts, it generally improves throughput at the cost of some additional latency .</li> <li>Heartbeat Interval: This defines the periodic interval at which heartbeats are sent to the consumer coordinator when using logical group management facilities . Heartbeats serve to ensure the consumer session remains active and facilitate rebalancing when consumers join or leave the group . The value for the heartbeat interval must be less than session.timeout.ms, typically not exceeding one-third of session.timeout.ms . Adjusting this can help control the expected time for normal rebalances .</li> <li>session.timeout.ms: This timeout is used to detect client failures within Kafka's group management facility. Clients send periodic heartbeats to signal their liveness. If the session times out, the consumer is removed by the brokers from the group, triggering a rebalance. The value for session.timeout.ms must fall between group.min.session.timeout.ms and group.max.session.timeout.ms.</li> <li>max.partition.fetch.bytes: This sets the maximum amount of data the server will return per partition. However, if the very first record batch is larger than this specified size, that first batch will still be returned to ensure continuous progress. The maximum fetch.batch.size accepted by brokers is determined by message.max.bytes.</li> <li>Max Bytes: This refers to the maximum amount of data the server should return for a fetch request. Records are filtered in batches by the consumer. Similar to max.partition.fetch.bytes, if the first record batch exceeds this limit, it will still be returned to ensure continuous progress.</li> </ol> <p>When a consumer interacts with Kafka to consume messages</p> <ol> <li>Consumer Poll: The consumer issues a Consumer.poll() request, which may retrieve a certain number of records (e.g., approximately 15 records) .</li> <li>Consumer Commit: After processing messages, the consumer calls Consumer.commit() to acknowledge that messages up to a certain offset (e.g., in P1, up to offset 5) have been successfully processed .</li> </ol> <p>After the consumer receives messages from a poll request, a parallel process begins to manage offset commits. Offsets represent the position of the last consumed message in a partition. Committing an offset tells Kafka which messages have been successfully processed by a consumer.</p> <p>This automatic commit mechanism is controlled by two key properties:</p> <ul> <li><code>enable.auto.commit</code>:        By default, this property is set to <code>true</code> for Python consumer APIs.        When <code>true</code>, the Kafka consumer will automatically commit offsets at regular intervals.</li> <li> <p><code>auto.commit.interval.ms</code>:        This property defines the time interval (in milliseconds) between automatic offset commits.        The default value is 5,000 milliseconds (5 seconds).</p> <p>Tip</p> <p>The commit timer starts after a polling request is completed and messages are received.</p> </li> </ul>"},{"location":"kafka/consumer/#how-consumers-in-consumer-group-read-messages","title":"How Consumers in Consumer Group read messages?","text":"<ol> <li>A single Kafka consumer can read from all partitions of a topic. This is often the case when you have only one consumer in a consumer group. </li> <li>However, when you have multiple consumers in a consumer group, the partitions of a topic are divided among the consumers. This allows Kafka to distribute the data across the consumers, enabling concurrent data processing and improving overall throughput.</li> <li>It's also important to note that while a single consumer can read from multiple partitions, a single partition can only be read by one consumer from a specific consumer group at a time. This ensures that the order of the messages in the partition is maintained when being processed.</li> </ol> <p>When a poll is complete, a parallel thread starts an \"autocommit timer\". This thread waits for the <code>auto.commit.interval.ms</code> duration to elapse. Once the configured time is over, the offset is committed to the <code>__consumer_offset</code> topic in Kafka. If an error occurs during message processing before the commit interval is over, the commit is interrupted.</p> <p>In the main thread, while the auto-commit timer runs in parallel, the consumer processes the messages received from the broker.</p> <ol> <li>Processing Records: The consumer collects all records received in a poll response and begins processing individual messages one by one. This processing can involve various activities, such as writing messages to a database or performing business logic.</li> <li>Continuous Polling: Once all messages from a particular poll response are processed successfully, the consumer automatically makes another polling request to fetch the next set of messages. This creates a continuous loop of fetching and processing.</li> </ol> <p>Consumer Group </p> <ol> <li>Consumer Group: A consumer group is a logical entity within the Kafka ecosystem that primarily facilitates parallel processing and scalable message consumption for consumer clients .        Every consumer must be associated with a consumer group .        There is no duplication of messages among consumers within the same consumer group .</li> <li> <p>Consumer Group Rebalancing: This is the process of re-distributing partitions among the consumers within a consumer group.</p> <p>Scenarios for Rebalancing: Rebalancing occurs in several situations:        A consumer joins the consumer group.        A consumer leaves the consumer group.        New partitions are added to a topic, making them available for new consumers.        Changes in connection states.</p> </li> <li> <p>Group Coordinator: In a Kafka cluster, one of the brokers is assigned the role of group coordinator to manage consumer groups.        The group coordinator maintains and manages the list of consumer groups.        It initiates a callback to communicate the new partition assignments to all consumers during rebalancing.        Important Note: Consumers within a group undergoing rebalancing will be blocked from reading messages until the rebalance process is complete.</p> </li> <li>Group Leader: The first consumer to join a consumer group is elected as the Group Leader.        The Group Leader maintains a list of active members and selects the assignment strategy.        The Group Leader is responsible for executing the rebalance process.        Once the new assignment is determined, the Group Leader sends it to the group coordinator.</li> <li>Consumer Joining a Group: When a consumer starts:        It sends a \"Find Coordinator\" request to locate the group coordinator for its group.        It then initiates the rebalance protocol by sending a \"Joining\" request.        Subsequently, members of the consumer group send a \"SyncGroup\" request to the coordinator.        Each consumer also periodically sends a \"Heartbeat\" request to the coordinator to keep its session alive.</li> </ol>"},{"location":"kafka/consumer/#rebalancing","title":"Rebalancing","text":"<p>In Apache Kafka, rebalancing refers to the process of redistributing the partitions of topics across all consumers in a consumer group. Rebalancing ensures that all consumers in the group have an equal number of partitions to consume from, thus evenly distributing the load.</p> <p>Rebalancing can be triggered by several events:</p> <ol> <li>Addition or removal of a consumer: If a new consumer joins a consumer group, or an existing consumer leaves (or crashes), a rebalance is triggered to redistribute the partitions among the available consumers.</li> <li>Addition or removal of a topic's partition: If a topic that a consumer group is consuming from has a partition added or removed, a rebalance will be triggered to ensure that the consumers in the group are consuming from the correct partitions.</li> <li>Consumer finishes consuming all messages in its partitions: When a consumer has consumed all messages in its current list of partitions and commits the offset back to Kafka, a rebalance can be triggered to assign it new partitions to consume from.</li> </ol> <p>While rebalancing ensures fair partition consumption across consumers, it's important to note that it can also cause some temporary disruption to the consuming process, as consumers may need to stop consuming during the rebalance. To minimize the impact, Kafka allows you to control when and how a consumer commits its offset, so you can ensure it happens at a point that minimizes any disruption from a rebalance.</p>"},{"location":"kafka/consumer/#read-strategies-in-kafka","title":"Read strategies in Kafka","text":"<p>In Apache Kafka, the consumer's position is referred to as the \"offset\". Kafka maintains the record of the current offset at the consumer level and provides control to the consumer to consume records from a position that suits their use case. This ability to control where to start reading records provides flexibility to the consumers. Here are the main reading strategies:</p> <ol> <li>Read From the Beginning: If a consumer wants to read from the start of a topic, it can do so by setting the consumer property auto.offset.reset to earliest. This strategy is useful for use cases where you want to process all the data in the topic.</li> <li>Read From the End (Latest): If a consumer only cares about new messages and doesn't want to read the entire history of a topic, it can start reading from the end. This is done by setting auto.offset.reset to latest.</li> <li>Read From a Specific Offset: If a consumer wants to read from a particular offset, it can do so using the seek() method on the KafkaConsumer object. This method changes the position of the consumer to the specified offset.</li> <li>Committing and Reading from Committed Offsets: The consumer can commit offsets after it has processed messages. If the consumer dies and then restarts, it can continue processing from where it left off by reading the committed offset.</li> </ol>"},{"location":"kafka/consumer/#manual-commit-at-most-once-exactly-once","title":"Manual Commit, At most once &amp; Exactly Once","text":"<p>Message Processing Loop and Manual Offset Commit</p> <p></p> <p>After receiving messages from a poll request, the consumer processes them in a continuous loop, and then explicitly commits its progress.</p> <ol> <li>Collect Records: All records received in the poll response are collected by the consumer.</li> <li>Individual Record Processing: The consumer then picks up and processes each individual record one by one. This processing might involve various operations, such as:        Storing data in a database: Persisting the message content into a data store.        Performing business logic: Executing specific application functions based on the message.        Sending to another service: Forwarding the message to another part of your system.</li> <li>Continuous Loop: If there are more records in the collected batch, the consumer loops back to process the next one until all records from that poll response are handled.</li> <li>Manual Offset Commitment: Once all the messages received in that particular poll response have been successfully processed, the consumer program explicitly issues a command to commit the offset to the Kafka broker. This action updates Kafka's record of the consumer's progress, marking the messages as processed.</li> <li>Return to Poll State: After the successful commit, the consumer then returns to the polling state, making another request for new messages, continuing the infinite loop of fetching, processing, and committing.</li> </ol> <p>Understanding \"At-Least-Once\" Processing with Manual Commits</p> <p></p> <p>Even with manual offset commits, Kafka's guarantee remains at-least-once processing. This means that a message might be processed more than once, especially if an error occurs before the offset for that message (or batch) is committed.</p> <p>Consider the following scenario:</p> <ol> <li>A consumer polls and receives a batch of 10 messages.</li> <li>The consumer successfully processes the first 3 messages.</li> <li>While attempting to process the 4th message, an error occurs (e.g., a database connection drops, or invalid data is encountered).</li> <li>Immediate Interruption: As soon as the error occurs, the processing flow is interrupted, and the consumer immediately reverts back to the poll step.</li> <li>No Offset Commit: Since not all 10 messages were successfully processed (specifically, the 4th message failed) and the manual commit step for the entire batch had not yet been reached, no offset was committed for the first 3 messages either.</li> <li>Reprocessing: When the consumer polls again, Kafka will send messages from the last committed offset. Since the offset for the previous batch was never committed, Kafka will re-send the same set of 10 messages.</li> <li>Consequently, the first 3 messages, which were already processed in the previous attempt, will be reprocessed.</li> </ol> <p>Message Processing Loop and Manual Offset Commitment for Exactly-Once </p> <p>After receiving messages from a poll request, the consumer processes them in a continuous loop. The key to the \"exactly-once\" strategy demonstrated is the immediate commitment of the offset after processing each single message.</p> <ol> <li>Iterate Records: The consumer iterates through each <code>message</code> object received from the <code>consumer.poll()</code> call (often simplified to <code>for message in consumer:</code>).</li> <li>Extract Message Properties: Important details like <code>message.value</code>, <code>message.key</code>, <code>message.topic</code>, <code>message.partition</code>, <code>message.offset</code>, and <code>message.timestamp</code> can be extracted from each <code>message</code> object.</li> <li>Process Individual Record: The consumer then performs its application logic (e.g., storing data in a database, executing business logic) using the message's content. The <code>print</code> statements in the demo code are considered the \"processing engine\" for this example.</li> <li> <p>Manual Offset Commitment (Immediately After Each Message): As soon as a single message is successfully processed, the consumer program explicitly issues a command to commit the offset to the Kafka broker. This is the core mechanism ensuring that if the consumer crashes after processing a message but before a batch commit, that specific message won't be reprocessed.</p> <p>The <code>consumer.commit()</code> Method: This method takes a dictionary where keys are <code>TopicPartition</code> objects and values are <code>OffsetAndMetadata</code> objects.        <code>TopicPartition</code>: Identifies the specific topic and partition. It's constructed using <code>message.topic</code> and <code>message.partition</code>.        <code>OffsetAndMetadata</code>: Contains the offset to commit and optional metadata.            Offset Value: The offset value provided for commit is <code>current_message_offset + 1</code>. This is because Kafka interprets a committed offset <code>X</code> as meaning messages up to <code>X-1</code> have been successfully processed, and the next message to send should be <code>X</code>.            Metadata: You can pass additional metadata (e.g., <code>message.timestamp</code>).</p> </li> </ol> <p><pre><code># Conceptual Python Code for Exactly-Once Processing (simplified)\nfor message in consumer:\n    # 1. Extract message details\n    print(f\"Topic: {message.topic}, Partition: {message.partition}, \"\n          f\"Offset: {message.offset}, Key: {message.key}, Value: {message.value}\") #\n\n    # 2. Process the message (e.g., save to DB, perform business logic)\n    # This print statement is considered the \"processing engine\"\n    print(\"Processing message:\", message.value) \n\n    # 3. Prepare TopicPartition and OffsetAndMetadata for commit\n    tp = TopicPartition(message.topic, message.partition)\n    # Commit the next offset (current_offset + 1)\n    offset_meta = OffsetAndMetadata(message.offset + 1, message.timestamp) \n\n    # 4. Create the dictionary for commit\n    offsets_to_commit = {tp: offset_meta}\n\n    # 5. Manually commit the offset for this single message\n    consumer.commit(offsets_to_commit) #\n    print(\"\"  100) # Print stars to visually differentiate processed messages\n</code></pre> 5.  Return to Poll State: After successfully processing and committing the offset for a single message, the consumer continues the loop, ready to process the next message in the fetched batch or poll for new messages if the batch is exhausted.</p> <p>This approach of committing after each message significantly reduces the window for reprocessing, making it align with the \"exactly-once\" claim by the source. If the consumer crashes after processing a message but before its specific offset is committed, that message could be re-processed. However, with per-message commits, this window is minimized to the time it takes to process and commit one message.</p>"},{"location":"kafka/consumer/#why-the-one-consumer-per-partition-rule","title":"Why the One-Consumer-Per-Partition Rule?","text":"<p>The central question addressed is: Why does Kafka not allow multiple consumers to consume messages from the same partition simultaneously?. This restriction is in place to prevent several critical issues related to data integrity and efficient processing.</p> <p>Problem 1: Load Balancing and Message Reprocessing</p> <p>The primary reason for introducing consumer groups was to achieve load balancing and accelerate message processing. However, if multiple consumers were allowed to consume from the same partition, this goal would be undermined.</p> <p>Consider a scenario where a topic has a partition, say Partition 3, which contains messages arranged in segments with offsets (unique identifiers for messages within a partition).</p> <p>Scenario: Suppose Consumer 4 and Consumer 5 are both consuming from Partition 3.</p> <p>The Issue:</p> <ol> <li>Consumer 4 processes messages from <code>offset 0</code> to <code>offset 4096</code>. Consumer 4 knows it has consumed up to <code>offset 4096</code> and expects to pull from <code>offset 4097</code> next time.</li> <li>However, Consumer 5, running in parallel, does not know what Consumer 4 has already processed. Consumer 5 might also attempt to consume messages starting from <code>offset 0</code>.</li> <li>Result: Both Consumer 4 and Consumer 5 would end up processing the same range of messages (<code>offset 0</code> to <code>offset 4096</code>).</li> </ol> <p>Consequence: This leads to reprocessing of the same messages multiple times, which is not genuine parallel processing or load balancing. True parallel processing involves different workers handling different parts of a larger task, not the same part repeatedly. The consumer group concept was designed for multiple consumers to process different chunks of messages.</p> <p>Problem 2: Violation of Message Order Guarantees</p> <p>Kafka guarantees message ordering only within a single partition. This means that messages sent to a specific partition will always be processed by consumers in the order they were written, based on their offsets. This guarantee is crucial for many applications, especially those where the sequence of events is vital.</p> <p>Consider a banking domain example:</p> <ol> <li>A customer first adds money to their account (Event A) and then withdraws money (Event B).</li> <li>It's essential that the addition of money is processed before the deduction to maintain correct account balance and display the correct sequence of events in the application.</li> <li>To ensure all events related to a specific account go to the same partition, a common strategy is to use hashing based on the account number (e.g., <code>account_number % total_partitions</code>). This ensures that if Event A goes to Partition 2, Event B (for the same account) will also go to Partition 2, and Event B will have a higher offset than Event A.</li> </ol> <p>The Issue if multiple consumers were allowed on one partition:</p> <ol> <li>Suppose Consumer 4 and Consumer 5 are both consuming from the same partition, and you somehow try to split the offset ranges (e.g., one consumes one range, the other consumes another).</li> <li>If they consume messages in parallel, the order of execution cannot be guaranteed.</li> <li>Consumer 5 might process the \"deduction of money\" event first, update the database, and display it in the front-end application. Simultaneously, Consumer 4 might process the \"addition of money\" event later.</li> <li>Consequence: This would result in a poor customer experience, as the transactions would not be displayed in the order they occurred. Kafka's design prevents this violation of crucial message ordering.</li> </ol> <p>Therefore, even if the reprocessing issue were somehow overcome by splitting offset ranges, the critical guarantee of message ordering within a partition would be lost if multiple consumers processed it concurrently.</p>"},{"location":"kafka/failures/","title":"Handling Producer Failures","text":"<p>What happens if the I/O thread fails to write a batch to the Kafka cluster? Kafka producers can be configured to retry sending the failed batches.</p> <p></p>"},{"location":"kafka/failures/#configuring-retries","title":"Configuring Retries","text":"<p>When creating the producer, you can specify whether to enable retries and for how long Kafka should attempt to rewrite a failed batch.    <code>delivery.timeout.ms</code>: This parameter configures the maximum time (in milliseconds) Kafka will attempt to write a message or retry writing if it fails initially.</p>"},{"location":"kafka/failures/#error-handling","title":"Error Handling","text":"<p>If Retries are Not Enabled: If the initial write fails and retries are disabled, the producer immediately invokes an error handler. On the client side, you will receive an error indicating that the messages were not written successfully. You would then need to handle this manually, perhaps by resending the messages from your application.</p> <p>If Retries are Enabled:    Kafka will attempt to retry writing the batch.    If the <code>delivery.timeout.ms</code> is exceeded during the retry attempts, the process will again go to the error handler, and an error will be reported to the client.    If the timeout is not over, Kafka will continue to retry writing the batch.</p>"},{"location":"kafka/failures/#ensuring-exactly-once-semantics-idempotent-producer","title":"Ensuring Exactly-Once Semantics: Idempotent Producer","text":"<p>While retries are essential for reliability, they can introduce a new problem: message duplication. An idempotent producer helps solve this.</p> <ol> <li>The Duplication Problem</li> </ol> <p>Consider this scenario:</p> <p>A batch of messages is sent to Kafka.   The batch is successfully written to the Kafka cluster.   However, the acknowledgement (ACK) or return response from Kafka back to the producer is lost or fails to be sent.   The producer, not having received the ACK, assumes the batch failed to write.   Consequently, the producer resends the exact same batch of messages to the Kafka cluster.   Without idempotence enabled, Kafka would simply write this \"new\" batch again, leading to duplicate messages in the topic.</p> <ol> <li>How Idempotence Works</li> </ol> <p>An idempotent producer prevents this duplication.</p> <p><code>enable.idempotence</code>: When this parameter is set to <code>true</code> (enabled), Kafka performs a duplicate check before writing any incoming batch. Duplicate Detection: If Kafka detects that an incoming batch is a duplicate (meaning it has already been successfully written), it intelligently understands that the producer likely didn't receive the previous acknowledgement. Action for Duplicates: Instead of rewriting the batch and causing duplication, Kafka's cluster will re-send the acknowledgement for that particular batch to the producer. Outcome: The producer receives the ACK, understands the batch was successful, and avoids resending, thus avoiding duplication.</p> <p>To avoid duplicacy in scenarios where ACKs might be lost, you should enable idempotence in your Kafka producer configuration.</p>"},{"location":"kafka/failures/#maintaining-message-order","title":"Maintaining Message Order","text":"<p>Even with retries and idempotence, another issue can arise: out-of-order messages within a single partition.</p> <p>The Ordering Problem</p> <p>Kafka guarantees message ordering within a single partition. However, with multiple \"in-flight\" requests, this guarantee can be compromised during retries:</p> <p>Consider a scenario where three batches (Batch 1, Batch 2, Batch 3) are sent sequentially to the same partition:</p> <ol> <li>Batch 1 is sent and successfully written.</li> <li>Batch 2 is sent but fails to write initially. A retry operation for Batch 2 is initiated.</li> <li>While Batch 2 is undergoing retry (which takes some time), the I/O thread proceeds to send Batch 3, which is then successfully written to the partition before Batch 2's retry completes.</li> <li>Finally, Batch 2's retry succeeds, and it is written to the partition.</li> </ol> <p>Actual order of sending: Batch 1, Batch 2, Batch 3. Resulting order in the partition: Batch 1, Batch 3, Batch 2.</p> <p>This \"screws up\" the intended message order within the partition, which is generally undesirable for many applications.</p> <p>Understanding <code>in-flight</code> Requests</p> <p>An <code>in-flight request</code> refers to a request that has been sent by the producer but has not yet received a completion response or acknowledgement. The <code>max.in.flight.requests.per.connection</code> parameter determines the maximum number of requests that can be \"in-flight\" (sent but not yet acknowledged) at any given time for a particular connection. If this is set to, say, <code>5</code>, the producer can send up to five requests concurrently without waiting for any of them to complete.</p> <p>Solution: Setting <code>max.in.flight.requests.per.connection</code> to 1</p> <p>To guarantee strict message ordering within a partition, you can set <code>max.in.flight.requests.per.connection = 1</code>.</p> <p>How it works: By setting this to <code>1</code>, the producer will send only one batch at a time and wait for its completion (i.e., receipt of the acknowledgement, even after retries) before sending the next batch.</p> <p>Effect on ordering: If a batch fails and requires a retry, no other batches will be sent to the cluster until that specific batch (and its retries) are fully completed. This ensures that messages are always written in their original sequential order within the partition.</p> <p>Trade-offs</p> <p>While <code>max.in.flight.requests.per.connection = 1</code> ensures strict ordering, it comes at a cost:</p> <p>Slower Message Writing: It effectively makes the message writing operation synchronous, meaning throughput will be reduced because the producer waits for each batch to complete before moving to the next. This is a trade-off between strict ordering and high throughput.</p>"},{"location":"kafka/failures/#ways-to-send-messages-to-kafka","title":"Ways to send messages to kafka","text":"<p>Method 1: Fire and Forget</p> <p>The \"Fire and Forget\" method is the simplest way to send messages.</p> <p>Concept: In this technique, the producer sends a message to the Kafka server and does not wait for any acknowledgment or confirmation about its successful arrival. The producer simply \"fires\" the message and \"forgets\" about it.</p> <p>Assumption: Kafka is a highly reliable system, and Kafka clusters are generally highly available. The producer also automatically retries sending messages if the first attempt fails. Therefore, most of the time (99.9%), messages will arrive successfully.</p> <p>Analogy: Imagine sending a letter by mail without requesting a delivery confirmation. You assume it will arrive because the postal service is generally reliable.</p> <p>Advantages: Very fast: Messages are sent immediately without any delay for waiting on responses. High throughput: You can send a large number of messages in a very short amount of time.</p> <p>Disadvantages:    Messages can be lost without notification: If something goes wrong (e.g., the Kafka broker goes down), the producer will not be aware that the messages failed to reach the cluster. These messages will be permanently lost.    \"The client is not even getting any information that whether really the message is written or not\".</p> <pre><code>for i in range(100):\ndata = {\"number\": i, \"message\": f\"This is message {i}\"}\nproducer.send(TOPIC_NAME, data) # - No waiting for response\nprint(f\"Sent message: {data}\")\ntime.sleep(0.5) # - Optional sleep for demonstration\n</code></pre> <p>Method 2: Synchronous Send</p> <p>The \"Synchronous Send\" method ensures that the producer receives a confirmation for each message sent before proceeding to the next.</p> <p>Concept: After sending a message, the producer waits for a response (acknowledgment or error) from the Kafka cluster before sending the next message. This makes the entire sending process sequential.</p> <p>Analogy: This is like waiting in a queue to get a movie ticket. You cannot get your ticket until the person in front of you has successfully received theirs.</p> <p>How it Works: The <code>producer.send()</code> method returns a <code>Future</code> object. By calling the <code>.get()</code> method on this <code>Future</code> object, the producer blocks and waits for the operation to complete. You can specify a <code>timeout</code> for how long to wait.</p> <p>Expected Response: If successful, the <code>get()</code> method returns <code>RecordMetadata</code> containing information like the topic name, partition number, and offset where the message was published. If it fails, an exception is raised.</p> <p>Advantages:    Guaranteed delivery notification. The producer knows whether each message was successfully written or if an error occurred. This allows for robust error handling and logging on the client side.</p> <p>Disadvantages: Very slow. This method makes the overall message sending process significantly slower because each message requires a full \"round trip\" to the Kafka cluster and back. For example, if the network round-trip time is 10 milliseconds, sending 100 messages would take at least 1 second (100 messages  10 ms/message). \"This is kind of making the whole process little bit slow\".</p> <pre><code>for i in range(100):\ndata = {\"number\": i, \"message\": f\"This is message {i}\"}\ntry:\n# send method returns a Future object, .get() waits for up to 10 seconds\nrecord_metadata = producer.send(TOPIC_NAME, data).get(timeout=10)\nprint(f\"Successfully produced message {data['number']} to topic {record_metadata.topic} \"\nf\"in partition {record_metadata.partition} and offset {record_metadata.offset}\") #\nexcept Exception as ex:\nprint(f\"Failed to write message {data['number']}: {ex}\") #\n# Log the error, retry, or take other appropriate action\ntime.sleep(0.5) # Optional sleep\n</code></pre> <p>Method 3: Asynchronous Send (with Callbacks)</p> <p>The Asynchronous Send method with callbacks offers a middle ground, combining the speed of \"Fire and Forget\" with the error-handling capabilities of \"Synchronous Send\".</p> <p>Concept: The producer sends messages rapidly without waiting for an immediate response, similar to \"Fire and Forget.\" However, it attaches callback functions that will be invoked automatically in the background once the message delivery operation (success or failure) is complete.</p> <p>How it Works: When <code>producer.send()</code> is called, it still returns a <code>Future</code> object. Instead of calling <code>.get()</code>, you use <code>.add_callback()</code> to register a function to be called on success, and <code>.add_errback()</code> to register a function to be called on failure. These callback functions receive information about the success (e.g., <code>RecordMetadata</code>) or failure (e.g., exception).</p> <p>Advantages:    High Performance: Messages are sent very quickly, as the producer does not wait for a response for each message before sending the next. This is \"ultra fast\" compared to synchronous sending.    Reliable Error Handling: Despite sending rapidly, the producer is still notified of message delivery status (success or failure) through the callback functions. This overcomes the main drawback of the \"Fire and Forget\" method.    Common in Industry: This technique is widely followed in real-world Kafka applications due to its balance of speed and reliability.</p> <pre><code># Define callback functions\ndef on_send_success(record_metadata, message_data): #\n\"\"\"\nThis function is called when a message is successfully written to Kafka.\nIt receives the RecordMetadata and the original message data.\n\"\"\"\nprint(f\"Successfully produced message {message_data} to topic {record_metadata.topic} \"\nf\"in partition {record_metadata.partition} and offset {record_metadata.offset}\") #\n\ndef on_send_error(ex, message_data): #\n\"\"\"\nThis function is called when a message fails to be written to Kafka.\nIt receives the exception and the original message data.\n\"\"\"\nprint(f\"Failed to write message {message_data}: {ex}\") #\n# Here you can log the error, store the failed message, etc.\n\n#\nfor i in range(100):\ndata = {\"number\": i, \"message\": f\"This is message {i}\"}\n\n# Send the message without blocking\nfuture = producer.send(TOPIC_NAME, data) #\n\n# Attach callback functions for success and failure\n# Note: The video's code passes `data` as a second argument to the callbacks\n# which is a useful pattern for associating context.\nfuture.add_callback(on_send_success, data) #\nfuture.add_errback(on_send_error, data) #\n\nprint(f\"Sent message (asynchronously): {data}\")\ntime.sleep(0.5) # - Optional sleep, useful for demoing failures\n</code></pre> <p>Choosing the Right Method</p> <p>The choice of sending method depends on your application's specific requirements:</p> <p>Fire and Forget: Use when maximum throughput is critical, and occasional message loss is acceptable or can be handled by downstream systems (e.g., logging, metrics, real-time analytics where exact message counts aren't critical).</p> <p>Synchronous Send: Use when guaranteed per-message delivery status is paramount, and you can tolerate lower throughput due to the blocking nature. This might be suitable for low-volume, high-value data where immediate confirmation is essential.</p> <p>Asynchronous Send (with Callbacks): This is generally the most recommended approach for most production scenarios. It offers a good balance of high throughput and reliable error handling, providing notifications without blocking the main sending thread.</p>"},{"location":"kafka/failures/#the-challenge-of-keynull","title":"The Challenge of key=null","text":"<p>When a Kafka producer sends messages to a topic, it typically uses a message key to determine which partition the message should go to. Messages with the same key are guaranteed to land in the same partition, ensuring ordered processing for those related messages.</p> <p>However, a common scenario arises when no key is provided (the <code>key</code> is <code>null</code>). In this situation, Kafka needs a strategy to distribute these messages across the available partitions. The video discusses how this distribution happens and how Kafka has optimized this process over time.</p> <p>Approach 1: Simple Round-Robin Distribution</p> <p>How it Works</p> <p>The simplest way to distribute messages when <code>key=null</code> is using a round-robin fashion.</p> <p>The first message is published to Partition 0 (P0).   The second message goes to Partition 1 (P1).   The third message goes to Partition 2 (P2), and so on.   Once all partitions have received a message, the distribution cycles back to Partition 0.</p> <p>Example: Let's assume a Kafka topic has 5 partitions (P0, P1, P2, P3, P4) and messages <code>1, 2, 3, 4, 5, 6</code> are being produced.</p> <p>Message <code>1</code> goes to P0.   Message <code>2</code> goes to P1.   Message <code>3</code> goes to P2.   Message <code>4</code> goes to P3.   Message <code>5</code> goes to P4.   Message <code>6</code> goes back to P0.</p> <p>Drawbacks:</p> <ol> <li>More Time Consuming: Each individual message is published to a different partition. This constant switching between partitions adds overhead and consumes more time.  </li> <li>High CPU Utilization for Broker: The Kafka broker, which is responsible for receiving and storing messages, has to individually send each message to a potentially different partition. This task is CPU-intensive for the broker, which also needs to handle many other important activities.</li> </ol> <p>Approach 2: Optimized Sticky Partitioning</p> <p>To overcome the drawbacks of round-robin distribution, Kafka introduced an optimized approach known as Sticky Partitioning. This method focuses on efficiency by leveraging Kafka producer's internal batching mechanism.</p> <p>How Sticky Partitioning Works When the message <code>key</code> is <code>null</code>, Kafka optimizes distribution by publishing all messages within a particular batch to a single partition.</p> <ol> <li>The producer creates a batch of messages.</li> <li>This entire batch is published to one partition (e.g., P0).</li> <li>The next batch created by the producer will then be published to the next partition (e.g., P1), and so on, in a round-robin like fashion for the batches, not individual messages.</li> </ol> <p>Example: Using the same 5 partitions (P0, P1, P2, P3, P4) and messages <code>1, 2, 3, 4, 5, 6</code>. Suppose the producer's internal batching accumulates messages <code>1, 2, 3</code> into the first batch, and <code>4, 5, 6</code> into the second batch.</p> <ol> <li>Batch 1 (<code>1, 2, 3</code>)** goes to P0.</li> <li>Batch 2 (<code>4, 5, 6</code>)** goes to P1.</li> <li>The next batch (if any) would go to P2, and so on.</li> </ol> <p>This approach is called \"Sticky Partitioning\" because the producer/broker \"sticks\" to one particular partition for an entire batch of messages, rather than switching partitions for individual messages within that batch.</p> <p>Advantages</p> <ol> <li>Faster (Lower Latency): The complete batch is written to one place (a single partition). This reduces the overhead of writing to different locations for individual messages, resulting in faster processing and lower latency.</li> <li>Less CPU Utilization for Broker: The broker is not constantly computing partition assignments for individual messages. Once a batch is created by the producer, the broker simply publishes that entire batch to a designated partition. This makes the process less CPU-intensive for the broker.</li> </ol> <p>Because of these advantages, sticky partitioning is the approach followed by Kafka's backend nowadays when no key is passed with the messages.</p>"},{"location":"kafka/iq/","title":"KAFKA_IQ","text":"<p>1. What is Apache Kafka?</p> <p>Apache Kafka is a publish-subscribe open source message broker application. This messaging application was coded in \"Scala\". Basically, this project was started by the Apache software. Kafka's design pattern is mainly based on the transactional logs design.</p> <p>2. Enlist the several components in Kafka</p> <p>The most important elements of Kafka are: - Topic \u2013 Kafka Topic is the bunch or a collection of messages. - Producer \u2013 In Kafka, Producers issue communications as well as publishes messages to a Kafka topic. - Consumer \u2013 Kafka Consumers subscribes to a topic(s) and also reads and processes messages from the topic(s). - Brokers \u2013While it comes to manage storage of messages in the topic(s) we use Kafka Brokers.</p> <p>3. Explain the role of the offset</p> <p>There is a sequential ID number given to the messages in the partitions what we call, an offset. So, to identify each message in the partition uniquely, we use these offsets.</p> <p>4. What is a Consumer Group?</p> <p>The concept of Consumer Groups is exclusive to Apache Kafka. Basically, every kafka cosumer group consists of one or more consumers that jointly consume a set of subscribed topics</p> <p>5. What is the role of the ZooKeeper in Kafka?</p> <p>Apache Kafka is a distributed system is built to use Zookeeper. Although, Zookeeper's main role here is to build coordination between different nodes in a cluster. However, we also use Zookeeper to recover from previously committed offset if any node fails because it works as periodically commit offset.</p> <p>6. Why is Kafka technology significant to use?</p> <p>There are some advantages of Kafka, which makes it significant to use: - High-throughput : We do not need any large hardware in Kafka, because it is capable of handling high-velocity and high-volume data. Moreover, it can also support message throughput of thousands of messages per second. - Low Latency : Kafka can easily handle these messages with the very low latency of the range of milliseconds, demanded by most of the new use cases. - Fault-Tolerant : Kafka is resistant to node/machine failure within a cluster. - Durability : As Kafka supports messages replication, so, messages are never lost. It is one of the reasons behind durability. - Scalability : Kafka can be scaled-out, without incurring any downtime on the fly by adding additional nodes.</p> <p>7. What are consumers or users?</p> <p>Kafka Consumer subscribes to a topic(s), and also reads and processes messages from the topic(s). Moreover, with a consumer group name, Consumers label themselves. In other words, within each subscribing consumer group, each record published to a topic is delivered to one consumer instance. Make sure it is possible that Consumer instances can be in separate processes or on separate machines.</p> <p>8. What ensures load balancing of the server in Kafka?</p> <p>As the main role of the Leader is to perform the task of all read and write requests for the partition, whereas Followers passively replicate the leader. Hence, at the time of Leader failing, one of the Followers takeover the role of the Leader. Basically, this entire process ensures load balancing of the servers.</p> <p>9. What roles do Replicas and the ISR play?</p> <p>Basically, a list of nodes that replicate the log is Replicas. Especially, for a particular partition. However, they are irrespective of whether they play the role of the Leader. In addition, ISR refers to In-Sync Replicas. On defining ISR, it is a set of message replicas that are synced to the leaders.</p> <p>10. Why are Replications critical in Kafka?</p> <p>Because of Replication, we can be sure that published messages are not lost and can be consumed in the event of any machine error, program error or frequent software upgrades.</p> <p>11. In the Producer, when does QueueFullException occur?</p> <p>Whenever the Kafka Producer attempts to send messages at a pace that the Broker cannot handle at that time QueueFullException typically occurs. However, to collaboratively handle the increased load, users will need to add enough brokers(servers, nodes), since the Producer doesn't block.</p> <p>12. What are the types of traditional method of message transfer?</p> <p>Basically, there are two methods of the traditional message transfer method, such as: - Queuing: It is a method in which a pool of consumers may read a message from the server and each message goes to one of them. - Publish-Subscribe: Whereas in Publish-Subscribe, messages are broadcasted to all consumers.</p> <p>13. What is Geo-Replication in Kafka?</p> <p>For our cluster, Kafka MirrorMaker offers geo-replication. Basically, messages are replicated across multiple data centers or cloud regions, with MirrorMaker. So, it can be used in active/passive scenarios for backup and recovery; or also to place data closer to our users, or support data locality requirements.</p> <p>14. Compare: RabbitMQ vs Apache Kafka</p> <p>One of the Apache Kafka's alternative is RabbitMQ. So, let's compare both: Features - Apache Kafka\u2013 Kafka is distributed, durable and highly available, here the data is shared as well as replicated. - RabbitMQ\u2013 There are no such features in RabbitMQ. Performance rate - Apache Kafka\u2013 To the tune of 100,000 messages/second. - RabbitMQ- In case of RabbitMQ, the performance rate is around 20,000 messages/second.</p> <p>15. Compare: Traditional queuing systems vs Apache Kafka</p> <p>Traditional queuing systems\u2013 It deletes the messages just after processing completion typically from the end of the queue. Apache Kafka\u2013 But in Kafka, messages persist even after being processed. That implies messages in Kafka don't get removed as consumers receive them. Logic-based processing - Traditional queuing systems\u2013Traditional queuing systems don't permit to process logic based on similar messages or events. - Apache Kafka\u2013 Kafka permits to process logic based on similar messages or events.</p> <p>16. What is the benefits of Apache Kafka over the traditional technique?</p> <ul> <li>Scalability: Kafka is designed for horizontal scalability. It can scale out by adding more brokers (servers) to the Kafka cluster to handle more partitions and thereby increase throughput. This scalability is seamless and can handle petabytes of data without downtime.</li> <li>Performance: Kafka provides high throughput for both publishing and subscribing to messages, even with very large volumes of data. It uses a disk structure that optimizes for batched writes and reads, significantly outperforming traditional databases in scenarios that involve high-volume, high-velocity data.</li> <li>Durability and Reliability: Kafka replicates data across multiple nodes, ensuring that data is not lost even if some brokers fail. This replication is configurable, allowing users to balance between redundancy and performance based on their requirements.</li> <li>Fault Tolerance: Kafka is designed to be fault-tolerant. The distributed nature of Kafka, combined with its replication mechanisms, ensures that the system continues to operate even when individual components .</li> <li>Real-time Processing: Kafka enables real-time data processing by allowing producers to write data into Kafka topics and consumers to read data from these topics with minimal latency. This capability is critical for applications that require real-time analytics, monitoring, and response.</li> <li>Decoupling of Data Streams: Kafka allows producers and consumers to operate independently. Producers can write data to Kafka topics without being concerned about how the data will be processed. Similarly, consumers can read data from topics without needing to coordinate with producers. This decoupling simplifies system architecture and enhances flexibility.</li> <li>Replayability: Kafka stores data for a configurable period, enabling applications to replay historical data. This is valuable for new applications that need access to historical data or for recovering from errors by reprocessing data.</li> <li>High Availability: Kafka's distributed nature and replication model ensure high availability. Even if some brokers or partitions become unavailable, the system can continue to function, ensuring continuous operation of critical applications.</li> </ul> <p>17. What is the meaning of broker in Apache Kafka?</p> <p>A broker refers to a server in the Kafka cluster that stores and manages the data. Each broker holds a set of topic partitions, allowing Kafka to efficiently handle large volumes of data by distributing the load across multiple brokers in the cluster. Brokers handle all read and write requests from Kafka producers and consumers and ensure data replication and fault tolerance to prevent data loss.</p> <p>18. What is the maximum size of a message that kafka can receive?</p> <p>The maximum size of a message that Kafka can receive is determined by the message.max.bytes configuration parameter for the broker and the max.message.bytes parameter for the topic. By default, Kafka allows messages up to 1 MB (1,048,576 bytes) in size, but both parameters can be adjusted to allow larger messages if needed.</p> <p>19. What is the Zookeeper's role in Kafka's ecosystem and can we use Kafka without Zookeeper?</p> <p>Zookeeper in Kafka is used for managing and coordinating Kafka brokers. It helps in leader election for partitions, cluster membership, and configuration management among other tasks. Historically, Kafka required Zookeeper to function. However, with the introduction of KRaft mode (Kafka Raft Metadata mode), it's possible to use Kafka without Zookeeper. KRaft mode replaces Zookeeper by using a built-in consensus mechanism for managing cluster metadata, simplifying the architecture and potentially improving performance and scalability.</p> <p>20. How are messages consumed by a consumer in apache Kafka?</p> <p>In Apache Kafka, messages are consumed by a consumer through a pull-based model. The consumer subscribes to one or more topics and polls the Kafka broker at regular intervals to fetch new messages. Messages are consumed in the order they are stored in the topic's partitions. Each consumer keeps track of its offset in each partition, which is the position of the next message to be consumed, allowing it to pick up where it left off across restarts or failures.</p> <p>21. How can you improve the throughput of a remote consumer?</p> <ul> <li>Increase Bandwidth: Ensure the network connection has sufficient bandwidth to handle the data being consumed.</li> <li>Optimize Data Serialization: Use efficient data serialization formats to reduce the size of the data being transmitted.</li> <li>Concurrency: Implement concurrency in the consumer to process data in parallel, if possible.</li> <li>Batch Processing: Where applicable, batch data together to reduce the number of roundtrip times needed.</li> <li>Caching: Cache frequently accessed data on the consumer side to reduce data retrieval times.</li> <li>Compression: Compress data before transmission to reduce the amount of data being sent over the network.</li> <li>Optimize Network Routes: Use optimized network paths or CDN services to reduce latency.</li> <li>Adjust Timeouts and Buffer Sizes: Fine-tune network settings, including timeouts and buffer sizes, for optimal data transfer rates.</li> </ul> <p>22. How can get Exactly-Once Messaging from Kafka during data production?</p> <p>During data production to get exactly once messaging from Kafka you have to follow two things: avoiding duplicates during data consumption and avoiding duplication during data production. Here are the two ways to get exactly one semantics while data production: Avail a single writer per partition, every time you get a network error checks the last message in that partition to see if your last write succeeded In the message include a primary key (UUID or something) and de-duplicate on the consumer 1. Enable Idempotence: Configure the producer for idempotence by setting <code>enable.idempotence</code> to <code>true</code>. This ensures that messages are not duplicated during network errors. 2. Transactional API: Use Kafka's Transactional API by initiating transactions on the producer. This involves setting the <code>transactional.id</code> configuration and managing transactions with <code>beginTransaction()</code>, <code>commitTransaction()</code>, and <code>abortTransaction()</code> methods. It ensures that either all messages in a transaction are successfully published, or none are in case of failure, thereby achieving exactly-once semantics. 3. Proper Configuration: Alongside enabling idempotence, adjust <code>acks</code> to <code>all</code> (or <code>-1</code>) to ensure all replicas acknowledge the messages, and set an appropriate <code>retries</code> and <code>max.in.flight.requests.per.connection</code> (should be 1 when transactions are used) to handle retries without message duplication. 4. Consistent Partitioning: Ensure that messages are partitioned consistently if the order matters. This might involve custom partitioning strategies to avoid shuffling messages among partitions upon retries.</p> <p>23. What is In-Sync Messages(ISR) in Apache Kafka?</p> <p>In Apache Kafka, ISR stands for In-Sync Replicas. It's a concept related to Kafka's high availability and fault tolerance mechanisms. For each partition, Kafka maintains a list of replicas that are considered \"in-sync\" with the leader replica. The leader replica is the one that handles all read and write requests for a specific partition, while the follower replicas replicate the leader's log. Followers that have fully caught up with the leader log are considered in-sync. This means they have replicated all messages up to the last message acknowledged by the leader. The ISR ensures data durability and availability. If the leader fails, Kafka can elect a new leader from the in-sync replicas, minimizing data loss and downtime.</p> <p>24. How can we reduce churn (frequent changes) in ISR?</p> <p>ISR is a set of message replicas that are completely synced up with the leaders, in other word ISR has all messages that are committed. ISR should always include all replicas until there is a real failure. A replica will be dropped out of ISR if it deviates from the leader. - Optimize Network Configuration: Ensure that the network connections between brokers are stable and have sufficient bandwidth. Network issues can cause followers to fall behind and drop out of the ISR. - Adjust Replica Lag Configuration: Kafka allows configuration of parameters like <code>replica.lag.time.max.ms</code> which defines how long a replica can be behind the leader before it is considered out of sync. Adjusting this value can help manage ISR churn by allowing replicas more or less time to catch up. - Monitor and Scale Resources Appropriately: Ensure that all brokers have sufficient resources (CPU, memory, disk I/O) to handle their workload. Overloaded brokers may struggle to keep up, leading to replicas falling out of the ISR. - Use Dedicated Networks for Replication Traffic: If possible, use a dedicated network for replication traffic. This can help prevent replication traffic from being impacted by other network loads.</p> <p>25. When does a broker leave ISR?</p> <p>A broker may leave the ISR for a few reasons: - Falling Behind: If a replica falls behind the leader by more than the configured thresholds (<code>replica.lag.time.max.ms</code> or <code>replica.lag.max.messages</code>), it is removed from the ISR. - Broker Failure: If a broker crashes or is otherwise disconnected from the cluster, its replicas are removed from the ISR. - Manual Intervention: An administrator can manually remove a replica from the ISR, although this is not common practice and should be done with caution.</p> <p>26. What does it indicate if replica stays out of Isr for a long time?</p> <p>If a replica stays out of the ISR (In-Sync Replicas) for a long time, it indicates that the replica is not able to keep up with the leader's log updates. This can be due to network issues, hardware failure, or high load on the broker. As a result, the replica might become a bottleneck for partition availability and durability, since it cannot participate in acknowledging writes or be elected as a leader if the current leader fails.</p> <p>27. What happens if the preferred replica is not in the ISR list?</p> <p>If the preferred replica is not in the In-Sync Replicas (ISR) for a Kafka topic, the producer will either wait for the preferred replica to become available (if configured with certain ack settings) or send messages to another available broker that is part of the ISR. This ensures data integrity by only using replicas that are fully up-to-date with the leader. Consumers might experience a delay in data availability if they are set to consume only from the preferred replica and it is not available</p> <p>28. Is it possible to get the message offset after producing to a topic?</p> <p>You cannot do that from a class that behaves as a producer like in most queue systems, its role is to fire and forget the messages. The broker will do the rest of the work like appropriate metadata handling with id\u2019s, offsets, etc. As a consumer of the message, you can get the offset from a Kafka broker. If you look in the SimpleConsumer class, you will notice it fetches MultiFetchResponse objects that include offsets as a list. In addition to that, when you iterate the Kafka Message, you will have MessageAndOffset objects that include both, the offset and the message sent. Yes, it is possible to get the message offset after producing a message in Kafka. When you send a message to a Kafka topic, the producer API can return metadata about the message, including the offset of the message in the topic partition</p> <p>29. What is the role of the offset in kafka?</p> <p>In Kafka, the offset is a unique identifier for each record within a Kafka topic's partition. It denotes the position of a record within the partition. The offset is used by consumers to track which records have been read and which haven't, allowing for fault-tolerant and scalable message consumption. Essentially, it enables consumers to pick up reading from the exact point they left off, even in the event of a failure or restart, thereby ensuring that no messages are lost or read multiple times.</p> <p>30. Can you explain the concept of leader and follower in kafka ecosystem?</p> <p>In Apache Kafka, the concepts of \"leader\" and \"follower\" refer to roles that brokers play within a Kafka cluster to manage partitions of a topic. - Leader: For each partition of a topic, there is one broker that acts as the leader. The leader is responsible for handling all read and write requests for that partition. When messages are produced to a partition, they are sent to the leader broker, which then writes the messages to its local storage. The leader broker ensures that messages are stored in the order they are received. - Follower: Followers are other brokers in the cluster that replicate the data of the leader for fault tolerance. Each follower continuously pulls messages from the leader to stay up-to-date, ensuring that it has an exact copy of the leader's data. In case the leader broker fails, one of the followers can be elected as the new leader, ensuring high availability.</p> <p>31. What do you mean by zookeeper in Kafka and what are its uses?</p> <p>Apache ZooKeeper is a naming registry for distributed applications as well as a distributed, open-source configuration and synchronization service. It keeps track of the Kafka cluster nodes' status, as well as Kafka topics, partitions, and so on. ZooKeeper is used by Kafka brokers to maintain and coordinate the Kafka cluster. When the topology of the Kafka cluster changes, such as when brokers and topics are added or removed, ZooKeeper notifies all nodes. When a new broker enters the cluster, for example, ZooKeeper notifies the cluster, as well as when a broker fails. ZooKeeper also allows brokers and topic partition pairs to elect leaders, allowing them to select which broker will be the leader for a given partition (and server read and write operations from producers and consumers), as well as which brokers contain clones of the same data. When the cluster of brokers receives a notification from ZooKeeper, they immediately begin to coordinate with one another and elect any new partition leaders that are required. This safeguards against the unexpected absence of a broker.</p> <p>32. What do you mean by a Partition in Kafka?</p> <p>Kafka topics are separated into partitions, each of which contains records in a fixed order. A unique offset is assigned and attributed to each record in a partition. Multiple partition logs can be found in a single topic. This allows several users to read from the same topic at the same time. Topics can be parallelized via partitions, which split data into a single topic among numerous brokers. Replication in Kafka is done at the partition level. A replica is the redundant element of a topic partition. Each partition often contains one or more replicas, which means that partitions contain messages that are duplicated across many Kafka brokers in the cluster. One server serves as the leader of each partition (replica), while the others function as followers. The leader replica is in charge of all read-write requests for the partition, while the followers replicate the leader. If the lead server goes down, one of the followers takes over as the leader. To disperse the burden, we should aim for a good balance of leaders, with each broker leading an equal number of partitions.</p> <p>33. What do you mean by Kafka schema registry?</p> <p>A Schema Registry is present for both producers and consumers in a Kafka cluster, and it holds Avro schemas. For easy serialization and de-serialization, Avro schemas enable the configuration of compatibility parameters between producers and consumers. The Kafka Schema Registry is used to ensure that the schema used by the consumer and the schema used by the producer are identical. The producers just need to submit the schema ID and not the whole schema when using the Confluent schema registry in Kafka. The consumer looks up the matching schema in the Schema Registry using the schema ID.</p> <p>34. Tell me about some of the use cases where Kafka is not suitable.</p> <p>Following are some of the use cases where Kafka is not suitable - Kafka is designed to manage large amounts of data. Traditional messaging systems would be more appropriate if only a small number of messages need to be processed every day. - Although Kafka includes a streaming API, it is insufficient for executing data transformations. For ETL (extract, transform, load) jobs, Kafka should be avoided. - There are superior options, such as RabbitMQ, for scenarios when a simple task queue is required. - If long-term storage is necessary, Kafka is not a good choice. It simply allows you to save data for a specific retention period and no longer.</p> <p>35. What do you understand about Kafka MirrorMaker?</p> <p>The MirrorMaker is a standalone utility for copying data from one Apache Kafka cluster to another. The MirrorMaker reads data from original cluster topics and writes it to a destination cluster with the same topic name. The source and destination clusters are separate entities that can have various partition counts and offset values.</p> <p>36. Describe message compression in Kafka. What is the need of message compression in Kafka? Also mention if there are any disadvantages of it.</p> <p>Producers transmit data to brokers in JSON format in Kafka. The JSON format stores data in string form, which can result in several duplicate records being stored in the Kafka topic. As a result, the amount of disc space used increases. As a result, before delivering messages to Kafka, compression or delaying of data is performed to save disk space. Because message compression is performed on the producer side, no changes to the consumer or broker setup are required.</p> <p>Advantages: - It decreases the latency of messages transmitted to Kafka by reducing their size. - Producers can send more net messages to the broker with less bandwidth. - When data is saved in Kafka using cloud platforms, it can save money in circumstances where cloud services are paid. - Message compression reduces the amount of data stored on disk, allowing for faster read and write operations.</p> <p>Disadvantages: - Producers must use some CPU cycles to compress their work. - Decompression takes up several CPU cycles for consumers. - Compression and decompression place a higher burden on the CPU.</p> <p>37. What do you understand about log compaction and quotas in Kafka?</p> <p>Log compaction is a way through which Kafka assures that for each topic partition, at least the last known value for each message key within the log of data is kept. This allows for the restoration of state following an application crash or a system failure. During any operational maintenance, it allows refreshing caches after an application restarts. Any consumer processing the log from the beginning will be able to see at least the final state of all records in the order in which they were written, because of the log compaction.</p> <p>A Kafka cluster can apply quotas on producers and fetch requests as of Kafka 0.9. Quotas are byte-rate limits that are set for each client-id. A client-id is a logical identifier for a request-making application. A single client-id can therefore link to numerous producers and client instances. The quota will be applied to them all as a single unit. Quotas prevent a single application from monopolizing broker resources and causing network saturation by consuming extremely large amounts of data.</p> <p>38. What do you mean by an unbalanced cluster in Kafka? How can you balance it?</p> <p>It's as simple as assigning a unique broker id, listeners, and log directory to the server.properties file to add new brokers to an existing Kafka cluster. However, these brokers will not be allocated any data partitions from the cluster's existing topics, so they won't be performing much work unless the partitions are moved or new topics are formed.</p> <p>A cluster is referred to as unbalanced if it has any of the following problems : - Leader Skew - Broker Skew</p> <p>39. What do you mean by BufferExhaustedException and OutOfMemoryException in Kafka?</p> <p>When the producer can't assign memory to a record because the buffer is full, a BufferExhaustedException is thrown. If the producer is in non-blocking mode, and the rate of production exceeds the rate at which data is transferred from the buffer for long enough, the allocated buffer will be depleted, the exception will be thrown.</p> <p>If the consumers are sending huge messages or if there is a spike in the number of messages sent at a rate quicker than the rate of downstream processing, an OutOfMemoryException may arise. As a result, the message queue fills up, consuming memory space.</p> <p>40. What are Znodes in Kafka Zookeeper? How many types of Znodes are there?</p> <p>The nodes in a ZooKeeper tree are called znodes. Version numbers for data modifications, ACL changes, and timestamps are kept by Znodes in a structure. ZooKeeper uses the version number and timestamp to verify the cache and guarantee that updates are coordinated. Each time the data on Znode changes, the version number connected with it grows.</p> <p>There are three different types of Znodes: - Persistence Znode: These are znodes that continue to function even after the client who created them has been disconnected. Unless otherwise specified, all znodes are persistent by default. - Ephemeral Znode: Ephemeral znodes are only active while the client is still alive. When the client who produced them disconnects from the ZooKeeper ensemble, the ephemeral Znodes are automatically removed. They have a significant part in the election of the leader. - Sequential Znode: When znodes are constructed, the ZooKeeper can be asked to append an increasing counter to the path's end. The parent znode's counter is unique. Sequential nodes can be either persistent or ephemeral.</p> <p>41. What is meant by the Replication Tool?</p> <p>The Replication Tool in Kafka is used for a high-level design to maintain Kafka replicas. Some of the replication tools available are</p> <ul> <li>Preferred Replica Leader Election Tool: Partitions are distributed to multiple brokers in a cluster, each copy known as a replica. The preferred replica usually refers to the leader. The brokers distribute the leader role evenly across the cluster for various partitions. Still, an imbalance can occur over time due to failures, planned shutdowns, etc. in such cases, you can use the replication tool to maintain the load balancing by reassigning the preferred replicas and hence, the leaders.</li> <li>Topics tool: Kafka topics tool is responsible for handling all management operations related to topics, which include Listing and describing topics, Creating topics, Changing topics, Adding partitions to a topic, Deleting topics</li> <li>Reassign partitions tool: This tool changes the replicas assigned to a partition. This means adding or removing followers associated with a partition.</li> <li>StateChangeLogMerger tool: This tool is used to collect data from the brokers in a particular cluster, formats it into a central log, and help to troubleshoot issues with state changes. Often, problems may arise with the leader election for a particular partition. This tool can be used to determine what caused the problem.</li> <li>Change topic configuration tool: used to Add new config options, Change existing config options, and Remove config options</li> </ul> <p>42. How can Kafka be tuned for optimal performance?</p> <p>Tuning for optimal performance involves consideration of two key measures: latency measures, which denote the amount of time taken to process one event, and throughput measures, which refer to how many events can be processed in a specific time. Most systems are optimized for either latency or throughput, while Kafka can balance both.</p> <p>Tuning Kafka for optimal performance involves the following steps: - Tuning Kafka producers: Data that the producers have to send to brokers is stored in a batch. When the batch is ready, the producer sends it to the broker. For latency and throughput, to tune the producers, two parameters must be taken care of: batch size and linger time. The batch size has to be selected very carefully. If the producer is sending messages all the time, a larger batch size is preferable to maximize throughput. However, if the batch size is chosen to be very large, then it may never get full or take a long time to fill up and, in turn, affect the latency. Batch size will have to be determined, taking into account the nature of the volume of messages sent from the producer. The linger time is added to create a delay to wait for more records to get filled up in the batch so that larger records are sent. A longer linger time will allow more messages to be sent in one batch, but this could compromise latency. On the other hand, a shorter linger time will result in fewer messages getting sent faster - reduced latency but reduced throughput as well. - Tuning Kafka broker: Each partition in a topic is associated with a leader, which will further have 0 or more followers. It is important that the leaders are balanced properly and ensure that some nodes are not overworked compared to others. - Tuning Kafka Consumers: It is recommended that the number of partitions for a topic is equal to the number of consumers so that the consumers can keep up with the producers. In the same consumer group, the partitions are split up among the consumers.</p> <p>43. How can all brokers available in a cluster be listed?</p> <p>Two ways to get the list of available brokers in an Apache Kafka cluster are as follows: - Using zookeeper-shell.sh <pre><code>zookeeper-shell.sh &lt;zookeeper_host&gt;:2181 ls /brokers/ids\n</code></pre>   Which will give an output like:   <pre><code>WATCHER:: WatchedEvent state:SyncConnected type:None path:null [0, 1, 2, 3]\n</code></pre>   This indicates that there are four alive brokers - 0,1,2 and 3 - Using zkCli.sh   First, you have to log in to the ZooKeeper client   <pre><code>zkCli.sh -server &lt;zookeeper_host&gt;:2181\nls /brokers/ids\n</code></pre>   Both the methods used above make use of the ZooKeeper to find out the list of available brokers</p> <p>44. What is the Kafka MirrorMaker?</p> <p>The Kafka MirrorMaker is a stand-alone tool that allows data to be copied from one Apache Kafka cluster to another. The Kafka MirrorMaker will read data from topics in the original cluster and write the topics to a destination cluster with the same topic name. The source and destination clusters are independent entities and can have different numbers of partitions and varying offset values.</p> <p>45. What is meant by Kafka Connect?</p> <p>Kafka Connect is a tool provided by Apache Kafka to allow scalable and reliable streaming data to move between Kafka and other systems. It makes it easier to define connectors that are responsible for moving large collections of data in and out of Kafka. Kafka Connect is able to process entire databases as input. It can also collect metrics from application servers into Kafka topics so that this data can be available for Kafka stream processing.</p> <p>46. Explain message compression in Apache Kafka.</p> <p>In Apache Kafka, producer applications write data to the brokers in JSON format. The data in the JSON format is stored in string form, which can result in several duplicated records getting stored in the Kafka topic. This leads to an increased occupation of disk space. Hence, to reduce this disk space, compression of messages or lingering the data is performed before sending the messages to Kafka. Message compression is done on the producer side, and hence there is no need to make any changes to the configuration of the consumer or the broker.</p> <p>47. What is the need for message compression in Apache Kafka?</p> <p>Message compression in Kafka does not require any changes in the configuration of the broker or the consumer. It is beneficial for the following reasons: - Due to reduced size, it reduces the latency in which messages are sent to Kafka. - Reduced bandwidth allows the producers to send more net messages to the broker. - When the data is stored in Kafka via cloud platforms, it can reduce the cost in cases where the cloud services are paid. - Message compression leads to reduced disk load, which will lead to faster read and write requests.</p> <p>48. Define consumer lag in Apache Kafka.</p> <p>Consumer lag refers to the lag between the Kafka producers and consumers. Consumer groups will have a lag if the data production rate far exceeds the rate at which the data is getting consumed. Consumer lag is the difference between the latest offset and the consumer offset.</p> <p>49. What do you know about log compaction in Kafka?</p> <p>Log compaction is a method by which Kafka ensures that at least the last known value for each message key within the log of data is retained for a single topic partition. This makes it possible to restore the state after an application crashes or in the event of a system failure. It allows cache reloading once an application restarts during any operational maintenance. Log compaction ensures that any consumer processing the log from the start can view the final state of all records in the original order they were written.</p> <p>50. When does Kafka throw a BufferExhaustedException?</p> <p>BufferExhaustedException is thrown when the producer cannot allocate memory to a record due to the buffer being too full. The exception is thrown if the producer is in non-blocking mode and the rate of data production exceeds the rate at which data is sent from the buffer for long enough for the allocated buffer to be exhausted.</p> <p>51. What are the responsibilities of a Controller Broker in Kafka?</p> <p>The main role of the Controller is to manage and coordinate the Kafka cluster, along with the Apache ZooKeeper. Any broker in the cluster can take on the role of the controller. However, once the application starts running, there can be only one controller broker in the cluster. When the broker starts, it will try to create a Controller node in ZooKeeper. The first broker that creates this controller node becomes the controller.</p> <p>The controller is responsible for - creating and deleting topics - Adding partitions and assigning leaders to the partitions - Managing the brokers in a cluster - adding new brokers, active broker shutdown, and broker failures - Leader Election - Reallocation of partitions.</p> <p>52. What causes OutOfMemoryException?</p> <p>OutOfMemoryException can occur if the consumers are sending large messages or if there is a spike in the number of messages wherein the consumer is sending messages at a rate faster than the rate of downstream processing. This causes the message queue to fill up, taking up memory.</p> <p>53. Explain the graceful shutdown in Kafka.</p> <p>Any broker shutdown or failure will automatically be detected by the Apache cluster. In such a case, new leaders will be elected for partitions that were previously handled by that machine. This can occur due to server failure and even if it is intentionally brought down for maintenance or any configuration changes. In cases where the server is intentionally brought down, Kafka supports a graceful mechanism for stopping the server rather than just killing it.</p> <p>Whenever a server is stopped: - Kafka ensures that all of its logs are synced onto a disk to avoid needing any log recovery when it is restarted. Since log recovery takes time, this can speed up intentional restarts. - Any partitions for which the server is the leader will be migrated to the replicas prior to shutting down. This ensures that the leadership transfer is faster, and the time during which each partition is unavailable will be reduced to a few milliseconds.</p> <p>54. How can a cluster be expanded in Kafka?</p> <p>In order to add a server to a Kafka cluster, it just has to be assigned a unique broker id, and Kafka has to be started on this new server. However, a new server will not automatically be assigned any of the data partitions until a new topic is created. Hence, when a new machine is added to the cluster, it becomes necessary to migrate some existing data to these machines. The partition reassignment tool can be used to move some partitions to the new broker. Kafka will add the new server as a follower of the partition that it is migrating to and allow it to completely replicate the data on that particular partition. When this data is fully replicated, the new server can join the ISR; one of the existing replicas will delete the data that it has with respect to that particular partition.</p> <p>55. What is meant by the Kafka schema registry?</p> <p>For both the producers and consumers associated with a Kafka cluster, a Schema Registry is present, which stores Avro schemas. Avro schemas allow the configuration of compatibility settings between the producers and the consumers for seamless serialization and deserialization. Kafka Schema Registry is used to ensure that there is no difference in the schema that is being used by the consumer and the one that is being used by the producer. While using the Confluent schema registry in Kafka, the producers only need to send the schema ID and not the entire schema. The consumer uses the schema ID to look up the corresponding schema in the Schema Registry.</p> <p>56. Name the various types of Kafka producer API.</p> <p>There are three types of Kafka producer API available: - Fire and Forget - Synchronous producer - Asynchronous produce</p> <p>57. What is the ZooKeeper ensemble?</p> <p>ZooKeeper works as a coordination system for distributed systems and is a distributed system on its own. It follows a simple client-server model, where clients are the machines that make use of the service, and the servers are nodes that provide the service. The collection of ZooKeeper servers forms the ZooKeeper ensemble. Each ZooKeeper server is capable of handling a large number of clients.</p> <p>58. What are Znodes?</p> <p>Nodes in a ZooKeeper tree are referred to as znodes. Znodes maintain a structure that contains version numbers for data changes, acl changes, and also timestamps. The version number, along with the timestamp, allows ZooKeeper to validate the cache and ensure that updates are coordinated. The version number associated with Znode increases each time the znode's data changes.</p> <p>59. What are the types of Znodes?</p> <p>There are three types of Znodes, namely: - Persistence Znode: these are the znodes that remain alive even after the client who created that particular znode is disconnected. All znodes are persistent by default unless otherwise specified. - Ephemeral Znode: Ephemeral znodes remain active only until the client is alive. Ephemeral Znodes get deleted whenever the client that created them gets disconnected from the ZooKeeper ensemble. They play an important role in the leader election. - Sequential Znode: when znodes are created, it is possible to request the ZooKeeper to add an increasing counter to the end of the path. This counter is unique to the parent znode. Sequential nodes may be persistent or ephemeral.</p> <p>60. How can we create Znodes?</p> <p>Znodes are created within the given path. Syntax: <pre><code>create /path/data\n</code></pre> Flags can be used to specify whether the znode created will be persistent, ephemeral, or sequential. <pre><code>create -e /path/data   # creates an ephemeral znode.\ncreate -s /path/data   # creates a sequential znode.\n</code></pre> All znodes are persistent by default.</p> <p>61. Suppose you are sending messages to a Kafka topic using kafkaTemplate. You come across a requirement that states that if a failure occurs while delivering messages to a Kafka topic, you must retry sending the messages on the same partition with the same offset. How can you achieve this using kafkatemplate?</p> <p>If you give the key while delivering the message, it will be stored in the same partition regardless of how many times you send it. The hashed key is used by Kafka to decide which partition needs to be updated. The only way to ensure that a failed message has the same offset when retried is to ensure that nothing is put into the topic before retrying it.</p> <p>62. Assume your brokers are hosted on AWS EC2. If you're a producer or consumer outside of the Kafka cluster network, you will only be capable of reaching the brokers over their public DNS, not their private DNS. Now, assume your client (producer or consumer) is outside your Kafka cluster's network, and you can only reach the brokers via their public DNS. The private DNS of the brokers hosting the leader partitions, not the public DNS, will be returned by the broker. Unfortunately, since your client is not present on your Kafka cluster's network, they will be unable to resolve the private DNS, resulting in the LEADER NOT AVAILABLE error. How will you resolve this network error?</p> <p>When you first start using Kafka brokers, you might have many listeners. Listeners are just a combination of hostname or IP, port, and protocol. Each Kafka broker's server.properties file contains the properties listed below. The important property that will enable you to resolve this network error is advertised.listeners. - listeners \u2013 a list of comma-separated hostnames and ports that Kafka brokers listen to. - advertised.listeners \u2013 a list of comma-separated hostnames and ports that will be returned to clients. Only include hostnames that will be resolved at the client (producer or consumer) level, such as public DNS. - inter.broker.listener.name \u2013 listeners used for internal traffic across brokers. These hostnames do not need to be resolved on the client side, but all of the cluster's brokers must resolve them. - listener.security.protocol.map \u2013 lists the supported protocols for each listener.</p> <p>63. Let's suppose a producer writes records to a Kafka topic at a rate of 10000 messages per second, but the consumer can only read 2500 messages per second. What are the various strategies for expanding your consumer group?</p> <p>The solution to this question has two parts: topic partitions and consumer groups. Partitions are used to split a Kafka topic. The producer's message is divided among the topic's partitions based on the message key. You can suppose that the key is chosen in such a way that messages are spread evenly between the partitions. Consumer groups are a method of grouping consumers together to maximize a consumer application's throughput. Each consumer in a consumer group holds on to a topic partition. If the Kafka topic has four partitions and the consumer group has four consumers, each consumer will read from a single partition. If there are six partitions and four consumers, the data will be read in parallel from only four partitions. As a result, maintaining a 1-to-1 mapping of partition to the consumer in the consumer group is preferable. Now, you can do two things to increase processing on the consumer side: - You can increase the topic's partition count (say from existing 1 to 4). - You can build a Kafka consumer group with four consumer instances tied to it. This would enable the consumers to read data from the topic in parallel, allowing it to expand from 2500 to 10000 messages per second.</p> <p>64. What is Kafka's producer acknowledgment? What are the various types of acknowledgment settings that Kafka provides?</p> <p>A broker sends an ack or acknowledgment to the producer to verify the reception of the message. Ack level is a configuration parameter in the Producer that specifies how many acknowledgments the producer must receive from the leader before a request is considered successful. The following types of acknowledgment are available: - acks=0: In this setting, the producer does not wait for the broker's acknowledgment. There is no way to know if the broker has received the record. - acks=1: In this situation, the leader logs the record to its local log file and answers without waiting for all of its followers to acknowledge it. The message can only be lost in this instance if the leader fails shortly after accepting the record but before the followers have copied it; otherwise, the record would be lost. - acks=all: A set leader in this situation waits for all in-sync replica sets to acknowledge the record. As long as one replica is alive, the record will not be lost, and the best possible guarantee will be provided. However, because a leader must wait for all followers to acknowledge before replying, the throughput is significantly lower.</p> <p>65. How do you get Kafka to perform in a FIFO manner?</p> <p>Kafka organizes messages into topics, which are then divided into partitions. The partition is an immutable list of ordered messages that is updated regularly. A message in the partition is uniquely recognized by a sequential number called offset. FIFO behavior is possible only within the partitions. Following the methods below will help you achieve FIFO behavior: - To begin, we first set the enable the auto-commit property to be false:   <code>Set enable.auto.commit=false</code> - We should not call the <code>consumer.commitSync();</code> method after the messages have been processed. - Then we may \"subscribe\" to the topic and ensure that the consumer system's register is updated. - You should use Listener consumerRebalance, and call a consumer inside a listener.   <code>seek(topicPartition, offset)</code>. - The offset related to the message should be kept together with the processed message once it has been processed.</p> <p>66. Explain Kafka's message delivery semantics.</p> <p>Kafka offers three message delivery semantics: At most once, At least once, and Exactly once, ensuring different trade-offs between message delivery and duplication.</p> <p>67. Explain the role of log segments in Kafka.</p> <p>Log segments are files that store Kafka messages. They are immutable and are used to manage disk space by performing log segment rolling and deletion.</p> <p>68. What is the purpose of the offset in Kafka?</p> <p>The offset is a unique identifier of a record within a partition. It denotes the position of the consumer in the partition. Kafka maintains this offset per partition, per consumer group, allowing each consumer group to read from a different position in the partition. This enables Kafka to provide both queue and publish-subscribe messaging models.</p> <p>69. How would you secure a Kafka cluster?</p> <p>Top candidates would use multiple layers of security and strategies such as: - SSL/TLS for encryption of data in transit - SASL/SCRAM for authentication - A Kerberos integration - Network policies for controlling access to the Kafka cluster - ACLs (Access Control Lists) for authorizing actions by users or groups on specific topics</p> <p>70. Explain the concept of Kafka MirrorMaker.</p> <p>Kafka MirrorMaker is a tool used for cross-cluster data replication. It enables data mirroring between two Kafka clusters, which is particularly useful for disaster recovery and geo-replication scenarios. MirrorMaker works by using consumer and producer configurations to pull data from a source cluster and push it to a destination cluster.</p> <p>71. What are ISR in Kafka?</p> <p>ISR (short for In-Sync Replicas) are replicas of a Kafka partition that are fully in sync with the leader. They're critical for ensuring data durability and consistency. If a leader fails, one of the ISRs can become the new leader.</p> <p>72. What authentication mechanisms can you use in Kafka?</p> <p>Kafka supports: - SSL/TLS for encrypting data and optionally authenticating clients using certificates - SASL (Simple Authentication and Security Layer) which supports mechanisms like GSSAPI (Kerberos), PLAIN, and SCRAM to secure Kafka brokers against unauthorized access - Integration with enterprise authentication systems like LDAP</p> <p>73. Describe an instance where Kafka might lose data and how you would prevent it.</p> <p>A good response will mention cases such as unclean leader elections, broker failures, or configuration errors that lead to data loss. Candidates should explain how they'd configure Kafka's replication factors, min.insync.replicas, and acknowledgment settings to prevent data loss. They should also mention they'd do regular backups and set up consistent monitoring to prevent issues.</p> <p>74. What is linger.ms in Kafka producers?</p> <p>Definition: linger.ms is a producer configuration that specifies the time (in milliseconds) the producer waits before sending a batch of messages.</p> <p>Behavior: - If the batch is full (batch.size reached), it is sent immediately. - If the batch is not full, the producer waits for the linger.ms time before sending the batch, hoping more records will arrive.</p> <p>Purpose: - To improve throughput by batching more records into a single request. - Reduces the number of network calls but may slightly increase latency.</p> Aspect commitSync() commitAsync() Type Synchronous Asynchronous Blocking Blocks until the broker acknowledges the commit Does not block; continues processing Reliability Highly reliable; throws exception on failure Less reliable; errors may be ignored Performance Slower due to waiting for acknowledgment Faster due to non-blocking behavior Use Case Critical systems (e.g., financial transactions) High-throughput systems (e.g., analytics) <p>Default Value: 0 , meaning no waiting and the producer sends records as soon as possible.</p> <p>Example Scenario: If linger.ms = 10 and batch.size isn't reached, the producer will wait 10ms before sending the batch, potentially grouping more messages together.</p> <p>75. How does Kafka manage backpressure?</p> <p>Kafka handles backpressure by controlling the flow of data between producers, brokers, and consumers through these mechanisms:</p> <p>Producer-Side: - Buffering: Producers buffer records up to buffer.memory . If the buffer is full, the producer blocks or throws an exception (based on max.block.ms ). - Batching: Producers optimize sending data in batches ( batch.size ) to handle high-throughput workloads efficiently.</p> <p>Broker-Side: - Replication Quotas: Kafka enforces quotas for replication to ensure brokers aren't overwhelmed. - I/O Throttling: Limits disk and network I/O rates to maintain cluster stability.</p> <p>Consumer-Side: - Pause and Resume: Consumers can pause fetching records if they can't process fast enough, avoiding memory overload. - Fetch Min/Max Bytes: Controls how much data is fetched at a time to prevent excessive resource usage.</p> <p>76. CommitSync() vs CommitAsync() in Kafka consumers</p> Aspect commitSync() commitAsync() Type Synchronous Asynchronous Blocking Blocks until the broker acknowledges the commit Does not block; continues processing Error Handling Direct exception handling Handle via a callback function Reliability Highly reliable; throws exception on failure Less reliable; errors may be ignored Performance Slower due to waiting for acknowledgment Faster due to non-blocking behavior Use Case Critical systems (e.g., financial transactions) High-throughput systems (e.g., analytics) <p>78. Explain the term Log Anatomy</p> <p>We view logs as the partitions. Basically, a data source writes messages to the log. One of the advantages is, at any time one or more consumers read from the log they select.</p> <p>79. What is a Data Log in Kafka?</p> <p>As we know, messages are retained for a considerable amount of time in Kafka. Moreover, there is flexibility for consumers that they can read as per their convenience. Although, there is a possible case that if Kafka is configured to keep messages for 24 hours and possibly that time the consumer is down for a time greater than 24 hours, then the consumer may lose those messages. However, still, we can read those messages from the last known offset, but only at a condition that the downtime on part of the consumer is just 60 minutes. Moreover, on what consumers are reading from a topic Kafka doesn\u2019t keep state.</p> <p>80. How can you ensure message delivery order across multiple Kafka partitions?</p> <p>Kafka doesn't guarantee message delivery order across partitions. However, you can use a single partition or key-based partitioning to maintain order within a partition.</p> <p>81. What is the role of a Kafka producer callback?</p> <p>A producer callback is invoked after a record is sent to Kafka. It allows for handling success or failure notifications.</p> <p>82. Explain the concept of message retention policies in Kafka.</p> <p>Kafka provides two retention policies: log compaction and delete. Log compaction retains the latest message for each key, while delete retains messages based on time or size.</p> <p>83. How does Kafka handle partition rebalancing during a consumer group rebalance?</p> <p>Kafka follows a group coordination protocol, where the group coordinator orchestrates the partition assignment and rebalancing process during consumer group rebalances.</p> <p>84. How does Kafka handle message deduplication?</p> <p>Kafka provides idempotent producers, which assign a unique identifier (message key) to each message. By using the message key, Kafka can identify and filter out duplicate messages during processing.</p> <p>85. How does Kafka Streams handle state restoration after a failure?</p> <p>Kafka Streams leverages Kafka's built-in log compaction and changelog topics to persist and restore the state of stream processing applications. The application's state is continuously maintained and can be restored in case of failure.</p> <p>86. What are the considerations for scaling Kafka clusters and applications in a production environment?</p> <p>Scaling Kafka clusters and applications involves considerations such as adding more brokers, increasing the number of partitions, optimizing hardware resources, and fine-tuning configuration parameters. Load balancing and monitoring tools are also essential for managing scalability effectively.</p>"},{"location":"kafka/kafka_install_zookeeper/","title":"Kafka Installation with ZooKeeper","text":""},{"location":"kafka/kafka_install_zookeeper/#prerequisites","title":"Prerequisites","text":"<ul> <li>3 VMs (Ensure pre-requisites like SELinux disabled, firewall off, THP disabled, etc.)</li> <li>Java 8 or higher</li> <li>Sufficient disk space</li> <li>Network connectivity</li> </ul>"},{"location":"kafka/kafka_install_zookeeper/#vm-settings","title":"VM settings","text":"<p>Kernel &amp; OS tuning</p> <pre><code>vm.swappiness = 1\nvm.dirty_background_ratio = 10 # Consider 5 for certain workloads\nvm.dirty_ratio = 20\n</code></pre> <p>Networking Parameters </p> <pre><code>net.core.wmem_default = 131072\nnet.core.rmem_default = 131072\nnet.core.wmem_max  = 2097152\nnet.core.rmem_max  = 2097152\nnet.ipv4.tcp_window_scaling = 1\nnet.ipv4.tcp_wmem = 4096 65536 2048000\nnet.ipv4.tcp_rmem = 4096 65536 2048000\nnet.ipv4.tcp_max_syn_backlog = 4096\nnet.core.netdev_max_backlog = 5000\n</code></pre> <p>GC Tuning</p> <pre><code>(for 64GB system with 5GB heap)\n-XX:MaxGCPauseMillis=20\n-XX:InitiatingHeapOccupancyPercent=35\n</code></pre>"},{"location":"kafka/kafka_install_zookeeper/#certs-creation","title":"Certs Creation","text":"<p>Creating certificates for SASL_SSL</p> <pre><code>#!/bin/\n# Generates several self-signed keys &lt;name&gt;.cer, &lt;name&gt;.jks, and &lt;name&gt;.p12.\n# Truststore is set with name truststore.jks and set password of password12345\n# Usage: createKey.sh &lt;user&gt; &lt;password&gt;\n#createKey.sh somebody password123\n# -ext \"SAN=DNS:\"\nexport NAME=$1\nexport IP1=$2\nexport PASSWORD=7ecETGlHjzs\nexport STORE_PASSWORD=7ecETGlHjzs\necho \"Creating key for $NAME using password $PASSWORD\"\nkeytool -genkey -alias $NAME -keyalg RSA -keysize 4096 -dname \"CN=$NAME,OU=RRA,O=ABC,L=ABC,ST=ABC,C=IN\" -ext \"SAN=DNS:$NAME,IP:$IP1\" -keypass $PASSWORD -keystore $NAME.jks -storepass $PASSWORD -validity 7200\nkeytool -export -keystore $NAME.jks -storepass $PASSWORD -alias $NAME -file $NAME.cer\nkeytool -import -trustcacerts -file $NAME.cer -alias $NAME -keystore truststore.jks -storepass $STORE_PASSWORD -noprompt\necho \"Done creating key for $NAME\"\nkeytool -list -keystore truststore.jks -storepass $STORE_PASSWORD -noprompt\n\n-------------JKStoPEM--------------\n/opt/jdk1.8.0_151/bin/keytool -exportcert -alias hostname1.com -keystore truststore.jks -storepass 7ecETGlHjzs -file hostname1.crt\n/opt/jdk1.8.0_151/bin/keytool -exportcert -alias hostname2.com -keystore truststore.jks -storepass 7ecETGlHjzs -file hostname2.crt\n/opt/jdk1.8.0_151/bin/keytool -exportcert -alias hostname3.com -keystore truststore.jks -storepass 7ecETGlHjzs -file hostname3.crt\nopenssl x509 -inform der -in hostname1.crt -out hostname1.pem\nopenssl x509 -inform der -in hostname2.crt -out hostname2.pem\nopenssl x509 -inform der -in hostname3.crt -out hostname3.pem\ncat *.pem &gt; truststore_combined.pem\n\nTo execute - use  cert.sh hostname\n</code></pre>"},{"location":"kafka/kafka_install_zookeeper/#zookeeper-installation","title":"Zookeeper Installation","text":"<p>Download tar file from apache zookeeper</p> <pre><code>wget https://downloads.apache.org/zookeeper/zookeeper-3.8.1/apache-zookeeper-3.8.1-bin.tar.gz\ntar -xzf apache-zookeeper-3.8.1-bin.tar.gz\ncd apache-zookeeper-3.8.1-bin/conf/\n</code></pre> <p>Configure zookeeper properties</p> <pre><code>tickTime=1000\ninitLimit=10\nsyncLimit=5\ndataDir=/home/testing/apache-zookeeper-3.8.1-bin/zkdata\nmaxClientCnxns=120\nmaxCnxns=120\nssl.client.enable=true\n#portUnification=true\n#client.portUnification=true\n#multiAddress.reachabilityCheckEnabled=true\n#quorumListenOnAllIPs=true\n#4lw.commands.whitelist=*\nadmin.enableServer=false\nauthProvider.sasl=org.apache.zookeeper.server.auth.SASLAuthenticationProvider\nzookeeper.superUser=superadmin\nsecureClientPort=12182\nclientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty\nserverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory \nauthProvider.x509=org.apache.zookeeper.server.auth.X509AuthenticationProvider\nssl.keyStore.location=/home/testing/certs/localhost1.jks\nssl.keyStore.password=7ecETGlHjzs\nssl.trustStore.location=/home/testing/certs/truststore.jks\nssl.trustStore.password=7ecETGlHjzs\nsslQuorum=true\nssl.quorum.keyStore.location=/home/testing/certs/localhost1.jks\nssl.quorum.keyStore.password=7ecETGlHjzs\nssl.quorum.trustStore.location=/home/testing/certs/truststore.jks\nssl.quorum.trustStore.password=7ecETGlHjzs\nsessionRequireClientSASLAuth=true\n#jute.maxbuffer=50000000\nDigestAuthenticationProvider.digestAlg=SHA3-512\nsecureClientPortAddress=localhost1\nserver.1=localhost1:4888:5888\nserver.2=localhost2:4888:5888\nserver.3=localhost3:4888:5888\n</code></pre> <p>create jaas conf file for authentication</p> <pre><code>Server{\norg.apache.zookeeper.server.auth.DigestLoginModule required\nuser_superadmin=\"SuperSecret123\"\nuser_bob=\"bobsecret\"\nuser_kafka=\"kafkasecret\";\n};\nClient{\norg.apache.zookeeper.server.auth.DigestLoginModule required\nusername=\"bob\"\npassword=\"bobsecret\";\n};\n</code></pre> <p>Configure the Java Env variables</p> <pre><code>export ZOO_LOG_DIR=/home/testing/apache-zookeeper-3.8.1-bin/zklogs\n\nexport ZK_SERVER_HEAP=1024\n\nexport SERVER_JVM_FLAGS=\"$SERVER_JVMFLAGS -Dzookeeper.db.autocreate=false -Djava.security.auth.login.config=/home/testing/apache-zookeeper-3.8.1-bin/conf/jaas.conf\"\n\n#export ZOO_DATADIR_AUTOCREATE_DISABLE=1\n\nexport CLIENT_JVMFLAGS=\"$CLIENT_JVMFLAGS -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.ssl.trustStore.location=/home/testing/certs/truststore.jks -Dzookeeper.ssl.trustStore.password=7ecETGlHjzs -Dzookeeper.ssl.keyStore.location=/home/testing/certs/localhost1.jks -Dzookeeper.ssl.keyStore.password=7ecETGlHjzs -Dzookeeper.client.secure=true -Djava.security.auth.login.config=/home/testing/apache-zookeeper-3.8.1-bin/conf/jaas.conf\"\n\nexport JVMFLAGS=\"-Djava.security.auth.login.config=/home/testing/apache-zookeeper-3.8.1-bin/conf/jaas.conf\"\n</code></pre> <p>Create Id for each zk node</p> <pre><code>echo 1 &gt; /data/kafka/zookeeper/data/myid\n# Change the value for each node (e.g., 1, 2, 3)\n</code></pre> <p>Start zookeper server one each node</p> <pre><code>bin/zkServer.sh start\n</code></pre> <p>Create ZK TLS properties file</p> <pre><code>zookeeper.ssl.client.enable=true\nzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty\nzookeeper.ssl.keystore.location=/root/certs/localhost.jks\nzookeeper.ssl.keystore.password=7ecETGlHjzs\nzookeeper.ssl.truststore.location=/root/certs/truststore.jks\nzookeeper.ssl.truststore.password=7ecETGlHjzs\n</code></pre> <p>Login to cli and verify the status </p> <pre><code>bin/zkCli.sh -server hostname1:12182\n</code></pre>"},{"location":"kafka/kafka_install_zookeeper/#kafka-installation","title":"Kafka Installation","text":"<p>Download tar file from apache kafka</p> <pre><code>wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\ntar -xzf kafka_2.13-3.6.1.tgz\ncd kafka_2.13-3.6.1\n</code></pre> <p>Configure server.properties</p> <pre><code>broker.id=1\nlisteners=SASL_SSL://hostname:6667\nlistener.security.protocol.map=SASL_SSL:SASL_SSL\nadvertised.listeners=SASL_SSL://hostname:6667\nauthorizer.class.name=kafka.security.authorizer.AclAuthorizer\nsasl.enabled.mechanisms=SCRAM-SHA-512\nsasl.mechanism.inter.broker.protocol=SCRAM-SHA-512\nsecurity.inter.broker.protocol=SASL_SSL\nssl.client.auth=required\n#ssl.endpoint.identification.algorithm=\nssl.keystore.location=/root/certs/hostname.jks\nssl.keystore.password=7ecETGlHjzs\nssl.truststore.location=/root/certs/truststore.jks\nssl.truststore.password=7ecETGlHjzs\nsuper.users=User:admin\nzookeeper.connect=hostname:12182,hostname2:12182,hostnamedb:12182\nzookeeper.ssl.client.enable=true\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=18000 \nzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty\nzookeeper.ssl.keystore.location=/root/certs/hostname.jks\nzookeeper.ssl.keystore.password=7ecETGlHjzs\nzookeeper.ssl.truststore.location=/root/certs/truststore.jks\nzookeeper.ssl.truststore.password=7ecETGlHjzs\n</code></pre> <p>Create kafka_jaas.conf file for authentication</p> <pre><code>Client{\norg.apache.zookeeper.server.auth.DigestLoginModule required\nusername=\"bob\"\npassword=\"bobsecret\";\n};\nKafkaServer{\norg.apache.kafka.common.security.scram.ScramLoginModule required\nusername=\"admin\"\npassword=\"password\";\n};\nKafkaClient{\norg.apache.kafka.common.security.scram.ScramLoginModule required\nusername=\"admin\"\npassword=\"password\";\n};\n</code></pre> <p>Configure KAFKA_OPTS and KAFKA-ENV properties</p> <pre><code>export KAFKA_HOME=/root/kafka_2.13-3.4.0\n\nexport KAFKA_OPTS=\"-Djava.security.auth.login.config=/root/kafka_2.13-3.4.0/config/kafka_jaas.conf -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty  -Dzookeeper.client.secure=true  -Dzookeeper.ssl.truststore.location=/root/certs/truststore.jks -Dzookeeper.ssl.truststore.password=7ecETGlHjzs\"\n\nexport KAFKA_HEAP_OPTS=\"-Xmx8G -Xms8G\"\n\n#export JMX_PORT=9999\n\n#export JMX_PROMETHEUS_PORT=9991\n\n#export KAFKA_JMX_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -javaagent:/root/certs/jmx_prometheus_javaagent-0.20.0.jar=$JMX_PROMETHEUS_PORT:/root/certs/kafka_broker.yml\"\n</code></pre> <p>Create Admin user for Kafka Server</p> <pre><code>bin/kafka-configs.sh --zk-tls-config-file /home/testing/certs/zk_tls_config.properties --zookeeper zkhost:12182 --alter --add-config 'SCRAM-SHA-512=[password='password']' --entity-type users --entity-name admin\n</code></pre> <p>Start the Kafka Server</p> <pre><code>bin/kafka-server-start.sh -daemon config/server.properties\n</code></pre> <p>Kafka ACl commands for authorization</p> <pre><code># Create Admin User\nkafka-configs.sh --zookeeper hostname1:12182 \\\n--alter --add-config 'SCRAM-SHA-512=[password=\"password\"]' \\\n--entity-type users --entity-name admin\n\n# Grant Producer Rights\nkafka-acls.sh --authorizer-properties zookeeper.connect=hostname1:12182 \\\n--add --allow-principal User:dlkdeveloper --producer \\\n--topic TEST --resource-pattern-type prefixed\n\n# Grant Consumer Rights\nkafka-acls.sh --authorizer-properties zookeeper.connect=hostname1:12182 \\\n--add --allow-principal User:$1 --consumer \\\n--group $1 --topic $2 --resource-pattern-type prefixed\n\n# List ACLs\nkafka-acls.sh --list --authorizer-properties zookeeper.connect=hostname1:12182\n\n# List Topics\nkafka-topics.sh --list \\\n--command-config /data1/kafkacerts/admin.properties \\\n--bootstrap-server hostname2:6667\n\n# Delete Topics\nkafka-topics.sh --delete --topic DL_TEST \\\n--bootstrap-server hostname1:6667 \\\n--command-config /data1/kafkacerts/admin.properties\n</code></pre>"},{"location":"kafka/logsegments/","title":"Log segments","text":""},{"location":"kafka/logsegments/#what-are-kafka-log-segments","title":"What are Kafka Log Segments?","text":"<p>Kafka Log Segments are a powerful mechanism that allows Kafka to efficiently manage and store vast amounts of streaming data. By breaking down large logs into smaller, configurable segments, Kafka ensures high performance, manageability, and robust data retention policies.</p> <p>All messages published by producers to a Kafka topic are stored within Kafka logs.</p> <p>These logs are the primary location where messages reside, playing a vital role in enabling communication between producers and consumers via the Kafka cluster.</p> <p>Traditionally, one might imagine all messages for a topic's partition being stored in a single, ever-growing log file. However, Kafka takes a more efficient approach:</p> <ol> <li>Instead of creating one single, large log file for a particular partition, Kafka creates several smaller files to store all messages.</li> <li>These small, individual files within a partition on a server are called segments.</li> </ol>"},{"location":"kafka/logsegments/#why-segments","title":"Why Segments?","text":"<p>Imagine a very large book that keeps growing infinitely. If you needed to find a specific page, or if the book became corrupted, managing one massive file would be incredibly difficult and inefficient.</p> <p>Kafka segments address this by:</p> <ol> <li>Managing Large Volumes of Data: By breaking down a single massive log into smaller, manageable segments, Kafka can handle terabytes or petabytes of data more effectively.</li> <li>Efficient Retention Policies: Older segments can be easily deleted or archived without affecting the active segments where new messages are being appended.</li> <li>Improved Recovery: In case of corruption or failure, smaller segments are faster to recover or replicate.</li> </ol>"},{"location":"kafka/logsegments/#how-new-segments-are-created","title":"How New Segments are Created?","text":"<p>Messages are continuously appended to the currently active log segment in a given partition. Kafka is configured with a maximum size limit for each log segment file. Once the current segment file reaches this configured size (in bytes), Kafka automatically creates a new, empty log segment file for subsequent messages. This ensures that no single log file becomes excessively large.</p>"},{"location":"kafka/logsegments/#why-segmentation","title":"Why Segmentation","text":"<p>Kafka doesn't write all messages into a single, ever-growing log file. This would become unwieldy and inefficient for operations like deletion or replication. Instead, Kafka divides its log files into multiple smaller segments.</p> <p>This segmentation is controlled by the <code>log.segment.bytes</code> property. When a log segment reaches a configured size limit (e.g., 2000 bytes as set in a demo), Kafka closes the current segment and starts a new one. Each segment has its own <code>.log</code>, <code>.index</code>, and <code>.timeindex</code> files.</p> <p>File Naming Convention</p> <p>A key pattern to observe in Kafka's segmented logs is how the files are named.</p> <p>Each log file, its corresponding <code>.index</code> file, and <code>.timeindex</code> file within a partition directory will share a common name prefix.    This prefix is actually the starting offset of the first message contained within that log segment.</p> <p>Examples:</p> <p><code>00000000000000000000.log</code> indicates the segment starts from <code>offset 0</code>.    <code>00000000000000000027.log</code> indicates the segment starts from <code>offset 27</code>.    <code>00000000000000000090.log</code> indicates the segment starts from <code>offset 90</code>.</p> <p>This naming convention is crucial for quickly identifying which segment contains a particular message.</p>"},{"location":"kafka/logsegments/#how-lookup-works-with-multiple-segments","title":"How Lookup Works with Multiple Segments","text":"<p>When a consumer requests a message by offset in a multi-segment environment, Kafka follows a three-step process:</p> <ol> <li>Locate the Segment File (by filename): The Kafka broker first determines which log segment contains the requested offset. It does this by checking the file names in the partition directory. Since file names indicate the starting offset of each segment, the broker can quickly identify the correct <code>.log</code> file without opening any files. For example, if <code>offset 100</code> is requested, the broker knows it must be in the <code>00000000000000000090.log</code> file because messages start from <code>offset 90</code> in this segment, and the next segment starts from <code>offset 109</code>.</li> <li>Lookup in the Segment's <code>.index</code> File: Once the correct log segment (<code>.log</code> file) is identified, the broker then goes to its corresponding <code>.index</code> file (e.g., <code>00000000000000000090.index</code>). It performs a binary search within this specific <code>.index</code> file to find the nearest offset and its byte <code>position</code>.</li> <li>Scan within the Segment's <code>.log</code> File: Finally, with the approximate byte <code>position</code> from the index file, the broker navigates to that position within the actual <code>.log</code> file and starts scanning from there to find the exact message(s) requested.</li> </ol> <p>This multi-step approach ensures that even with hundreds or thousands of gigabytes of messages, Kafka can locate any message with minimal disk I/O and latency.</p>"},{"location":"kafka/logsegments/#dumping-and-reading-contents-of-log-index-or-timeindex-files","title":"Dumping and Reading Contents of Log, Index, or TimeIndex Files","text":"<p>This command helps you view the structured content of these binary files. <pre><code>kafka-run-class.bat kafka.tools.DumpLogSegments --files [path_to_log_file.log] --print-data-log\n\nExample for .log file:\nkafka-run-class.bat kafka.tools.DumpLogSegments --files C:\\kafka\\kafka-logs\\my-topic-0\\00000000000000000000.log --print-data-log\n\nExample for .index file:\nkafka-run-class.bat kafka.tools.DumpLogSegments --files C:\\kafka\\kafka-logs\\my-topic-0\\00000000000000000000.index --print-data-log\n\nExample for .timeindex file:\nkafka-run-class.bat kafka.tools.DumpLogSegments --files C:\\kafka\\kafka-logs\\my-topic-0\\00000000000000000000.timeindex --print-data-log\n</code></pre></p>"},{"location":"kafka/offsets/","title":"Offsets","text":""},{"location":"kafka/offsets/#offsets","title":"Offsets","text":"<p>Offsets represent the position of each message within a partition and are uniquely identifiable, ever-increasing integers . There are three main variations of offsets :</p> <ol> <li>Log End Offset: This refers to the offset of the last message written to any given partition .</li> <li>Current Offset: This is a pointer to the last record that Kafka has already sent to the consumer in the current poll .</li> <li>Committed Offset: This indicates the offset of a message that a consumer has successfully consumed .</li> <li>Relationship: The committed offset is typically less than the current offset .</li> </ol> <p>In Apache Kafka, consumer offset management \u2013 that is, tracking what messages have been consumed \u2013 is handled by Kafka itself.</p> <p>When a consumer in a consumer group reads a message from a partition, it commits the offset of that message back to Kafka. This allows Kafka to keep track of what has been consumed, and what messages should be delivered if a new consumer starts consuming, or an existing consumer restarts.</p> <p>Earlier versions of Kafka used Apache ZooKeeper for offset tracking, but since version 0.9, Kafka uses an internal topic named \"__consumer_offsets\" to manage these offsets. This change has helped to improve scalability and durability of consumer offsets.</p> <p>Kafka maintains two types of offsets:</p> <ol> <li>Current Offset : The current offset is a reference to the most recent record that Kafka has already provided to a consumer. As a result of the current offset, the consumer does not receive the same record twice.</li> <li>Committed Offset : The committed offset is a pointer to the last record that a consumer has successfully processed. We work with the committed offset in case of any failure in application or replaying from a certain point in event stream.</li> </ol>"},{"location":"kafka/offsets/#committing-an-offset","title":"Committing an offset","text":"<ol> <li> <p>Auto Commit: By default, the consumer is configured to use an automatic commit policy, which triggers a commit on a periodic interval. This feature is controlled by setting two properties:  enable.auto.commit &amp; auto.commit.interval.ms</p> <p>Although auto-commit is a helpful feature, it may result in duplicate data being processed.</p> <p>Let\u2019s have a look at an example. You\u2019ve got some messages in the partition, and you\u2019ve requested your first poll. Because you received ten messages, the consumer raises the current offset to ten. You process these ten messages and initiate a new call in four seconds. Since five seconds have not passed yet, the consumer will not commit the offset. Then again, you\u2019ve got a new batch of records, and rebalancing has been triggered for some reason.  The first ten records have already been processed, but nothing has yet been committed. Right? The rebalancing process has begun. As a result, the partition is assigned to a different consumer. Because we don\u2019t have a committed offset, the new partition owner should begin reading from the beginning and process the first ten entries all over again. A manual commit is the solution to this particular situation. As a result, we may turn off auto-commit and manually commit the records after processing them.</p> </li> <li> <p>Manual Commit: With Manual Commits, you take the control in your hands as to what offset you\u2019ll commit and when. You can enable manual commit by setting the enable.auto.commit property to false. There are two ways to implement manual commits :</p> <ol> <li>Commit Sync: The synchronous commit method is simple and dependable, but it is a blocking mechanism. It will pause your call while it completes a commit process, and if there are any recoverable mistakes, it will retry. Kafka Consumer API provides this as a prebuilt method.</li> <li>Commit Async: The request will be sent and the process will continue if you use asynchronous commit. The disadvantage is that commitAsync does not attempt to retry. However, there is a legitimate justification for such behavior. Let\u2019s have a look at an example. Assume you\u2019re attempting to commit an offset as 70. It failed for whatever reason that can be fixed, and you wish to try again in a few seconds. Because this was an asynchronous request, you launched another commit without realizing your prior commit was still waiting. It\u2019s time to commit-100 this time. Commit-100 is successful, however commit-75 is awaiting a retry. Now how would we handle this? Since you don\u2019t want an older offset to be committed. This could cause issues. As a result, they created asynchronous commit to avoid retrying. This behavior, however, is unproblematic since you know that if one commit fails for a reason that can be recovered, the following higher level commit will succeed.</li> </ol> </li> </ol>"},{"location":"kafka/offsets/#what-if-asynccommit-failure-is-non-retryable","title":"What if AsyncCommit failure is non-retryable?","text":"<p>Asynchronous commits can fail for a variety of reasons. For example, the Kafka broker might be temporarily down, the consumer may be considered dead by the group coordinator and kicked out of the group, the committed offset may be larger than the last offset the broker has, and so on.</p> <p>When the commit fails with a non-retryable error, the commitAsync method doesn't retry the commit, and your application doesn't get a direct notification about it, because it runs in the background. However, you can provide a callback function that gets triggered upon a commit failure or success, which can log the error and you can take appropriate actions based on it.</p> <p>But keep in mind, even if you handle the error in the callback, the commit has failed and it's not retried, which means the consumer offset hasn't been updated in Kafka. The consumer will continue to consume messages from the failed offset. In such scenarios, manual intervention or alerts might be necessary to identify the root cause and resolve the issue.</p> <p>On the other hand, synchronous commits (commitSync) will retry indefinitely until the commit succeeds or encounters a  non-retryable failure, at which point it throws an exception that your application can catch and handle directly. This is why it's often recommended to have a final synchronous commit when you're done consuming messages.</p> <p>As a general strategy, it's crucial to monitor your consumers and Kafka infrastructure for such failures and handle them appropriately to ensure smooth data processing and prevent data loss or duplication.</p>"},{"location":"kafka/offsets/#when-to-use-synccommit-vs-asynccommit","title":"When to use SyncCommit vs AsyncCommit?","text":"<p>Choosing between synchronous and asynchronous commit in Apache Kafka largely depends on your application's requirements around data reliability and processing efficiency.</p> <p>Here are some factors to consider when deciding between synchronous and asynchronous commit:</p> <ol> <li> <p>Synchronous commit (commitSync): Use it when data reliability is critical, as it retries indefinitely until successful or a fatal error occurs. However, it can block your consumer, slowing down processing speed.</p> </li> <li> <p>Asynchronous commit (commitAsync): Use it when processing speed is important and some data loss is tolerable. It doesn't block your consumer but doesn't retry upon failures.</p> </li> </ol> <p>Combination: Many applications use commitAsync for regular commits and commitSync before shutting down to ensure the final offset is committed. This approach balances speed and reliability.</p>"},{"location":"kafka/offsets/#what-is-out-of-order-commit","title":"What is Out-of-Order Commit?","text":"<p>Normally, you might expect offsets to be committed sequentially (e.g., commit for message 1, then 2, then 3, and so on). However, Kafka's design allows for out-of-order commits, meaning a consumer can commit a later offset even if earlier messages in the sequence haven't been explicitly committed.</p> <p>A Simple Scenario</p> <p>Consider a Kafka topic with messages 1, 2, 3, 4, 5, 6, etc..</p> <ol> <li>A consumer polls messages and receives 1, 2, 3, and 4.</li> <li>However, for some reason, the consumer only commits the offset for message 4 to the <code>__consumer_offset</code> topic. It does not send explicit commits for messages 1, 2, or 3.</li> <li>Then, the consumer goes down.</li> </ol> <p>The Broker's Behavior</p> <p>When the consumer spins up again, a crucial question arises: Will the Kafka broker re-send messages 1, 2, and 3 (for which no explicit commit was received), or will it start from message 5?</p> <p>The correct answer is: The Kafka broker will not re-send messages 1, 2, or 3.</p> <ol> <li>The broker simply checks the <code>__consumer_offset</code> topic for the latest committed offset for that particular consumer group and topic.</li> <li>In our scenario, the latest committed offset is for message 4 (which means the next message to read is 5).</li> <li>The broker will then start sending messages from message 5 onwards (i.e., 5, 6, 7, etc.).</li> <li>Kafka assumes that all messages prior to the latest committed offset have been successfully processed, even if individual commits for those messages were not received. This committed offset acts like a \"bookmark\".</li> </ol> <p>This behavior is termed \"out-of-order commit\" because, ideally, commits should be sequential (1, then 2, then 3, then 4). However, in this scenario, a commit for message 4 is received directly, without commits for messages 1, 2, or 3.</p>"},{"location":"kafka/offsets/#advantages-of-out-of-order-commit","title":"Advantages of Out-of-Order Commit","text":"<p>The primary advantage of out-of-order commit is reduced overhead.</p> <ol> <li>Committing an offset for every single message individually can produce a lot of overhead, as it's a complex operation.</li> <li>Instead, consumers can consume a batch of messages (e.g., 1, 2, 3, 4).</li> <li>After processing the entire batch, the consumer only needs to commit the offset of the last message in that batch (e.g., message 4).</li> <li>This way, the entire batch is effectively acknowledged, and the broker will not re-send any messages within that batch, understanding them as successfully processed. This significantly improves efficiency by reducing the number of commit operations.</li> </ol>"},{"location":"kafka/offsets/#disadvantages-of-out-of-order-commit","title":"Disadvantages of Out-of-Order Commit","text":"<p>While efficient, out-of-order commit has a significant disadvantage: potential message loss.</p> <ol> <li>Imagine a scenario where a consumer processes messages using multiple threads or a complex backend system.</li> <li>If messages 1, 2, and 3 fail during processing, but message 4 (which was processed by a separate, successful thread) is committed.</li> <li>Even though messages 1, 2, and 3 failed, because message 4's offset was committed, the broker will not re-send those failed messages when the consumer restarts.</li> <li>This can lead to data loss or inconsistent processing if not handled carefully at the application level.</li> </ol>"},{"location":"kafka/overview/","title":"Overview","text":"<p>Apache Kafka is a distributed streaming platform that was developed by LinkedIn in 2010 and later donated to the Apache Software Foundation. It's designed to handle high-throughput, fault-tolerant, and real-time data streaming. It is often referred to as the distributed commit log.</p>"},{"location":"kafka/overview/#messaging-systems","title":"Messaging Systems","text":"<p>The main task of messaging system is to transfer data from one application to another so that the applications can mainly work on data without worrying about sharing it. Distributed messaging is based on the reliable message queuing process. Messages are queued non-synchronously between the messaging system and client applications.</p> <p>There are two types of messaging patterns available:</p> <ol> <li> <p>Point to point messaging system:      In this messaging system, messages continue to remain in a queue. More than one consumer can consume the messages in the queue but only one consumer can consume a particular message. After the consumer reads the message in the queue, the message disappears from that queue.</p> </li> <li> <p>Publish-subscribe messaging system:     In this messaging system, messages continue to remain in a Topic. Contrary to Point to point messaging system, consumers can take more than one topic and consume every message in that topic. Message producers are known as publishers and Kafka consumers are known as subscribers.</p> </li> </ol>"},{"location":"kafka/overview/#key-characteristics","title":"Key characteristics","text":"<ol> <li>Scalable: It supports horizontal scaling by allowing you to add new brokers (servers) to the clusters.</li> <li>Fault-tolerant: It can handle failures effectively due to its distributed nature and replication mechanisms.</li> <li>Durable: Kafka uses a \"distributed commit log,\" which means messages are persisted on disk . This ensures data is not lost even if a server goes down.</li> <li>Fast: Designed to be as fast as possible.</li> <li>Performance: Achieves high throughput for both publishing (producers) and subscribing (consumers).</li> <li>No data loss: Guarantees that messages are not lost once they are committed to Kafka.</li> <li>Zero down time: Designed for continuous operation without interruption.</li> <li>Reliability: Provides reliable message delivery.</li> </ol>"},{"location":"kafka/overview/#kafka-vs-traditional-messaging-systems","title":"Kafka vs Traditional Messaging systems","text":"Feature Traditional Messaging System Kafka Streaming Platform Message Persistence The broker is responsible for keeping track of consumed messages and removing them when messages are read. Messages are typically retained in Kafka topics for a configurable period of time, even after they have been consumed. Kafka offers message persistence, ensuring data durability. Scalability Not a distributed system, so it is not possible to scale horizontally. It is a distributed streaming system, so by adding more partitions, we can scale horizontally. Data Model Primarily point-to-point (queues/topics) messaging model. Built around a publish-subscribe (logs) model, which enables multiple consumers to subscribe to topics and process data concurrently. Ordering of Messages Message ordering can be guaranteed within a single queue or topic but may not be guaranteed across different queues or topics. Kafka maintains message order within a partition, ensuring that messages within a partition are processed in the order they were received. Message Replay Limited or no built-in support for message replay. Once consumed, messages may be lost unless custom solutions are implemented. Supports message replay from a specified offset, allowing consumers to reprocess past data, which is valuable for debugging, analytics, and data reprocessing. Use Cases Typically used for traditional enterprise messaging, remote procedure calls (RPC), and task queues. Well-suited for real-time analytics, log aggregation, event sourcing, and building data-intensive, real-time applications."},{"location":"kafka/partition/","title":"Partitions & Replciations","text":""},{"location":"kafka/partition/#partitions","title":"Partitions","text":"<p>Topics are split into partitions. All messages within a specific partition are ordered and immutable (meaning they cannot be changed after being written). Each message within a partition has a unique ID called an Offset. This offset denotes the message's position within that specific partition.</p> <p>Illustrative Example: If your sports_news topic has three partitions (P0, P1, P2), articles related to football might go to P0, basketball to P1, and tennis to P2. Within P0, all football articles will appear in the exact order they were published, each with its unique offset.</p> <p>Partitions play a crucial role in Kafka's functionality and scalability. </p> <ol> <li> <p>Parallelism: Partitions enable parallelism. Since each partition can be placed on a separate machine (broker), a topic can handle an amount of data that exceeds a single server's capacity. This allows producers and consumers to read and write data to a topic concurrently, thus increasing throughput.</p> </li> <li> <p>Ordering: Kafka guarantees that messages within a single partition will be kept in the exact order they were produced. However, if order is important across partitions, additional design considerations are needed.</p> </li> <li> <p>Replication: Partitions of a topic can be replicated across multiple brokers based on the topic's replication factor. This increases data reliability and availability.</p> </li> <li> <p>Failover: In case of a broker failure, the leadership of the partitions owned by that broker will be automatically taken over by another broker, which has the replica of these partitions.</p> </li> <li> <p>Consumer Groups: Each partition can be consumed by one consumer within a consumer group at a time. If more than one consumer is needed to read data from a topic simultaneously, the topic needs to have more than one partition.</p> </li> <li> <p>Offset: Every message in a partition is assigned a unique (per partition) and sequential ID called an offset. Consumers use this offset to keep track of their position in the partition.</p> </li> </ol>"},{"location":"kafka/partition/#kafka-partition-assignment-strategies","title":"Kafka Partition Assignment Strategies","text":"<p>When rebalancing happens, Kafka uses specific algorithms to determine how partitions are assigned to consumers.</p> <p>Range Partitioner: The Range Partitioner assigns a contiguous \"range\" of partitions to each consumer. It sorts partitions numerically (e.g., 0, 1, 2, 3, 4, 5).It then divides the total number of partitions by the number of consumers to determine the number of partitions each consumer should handle. A contiguous block of partitions (a \"range\") is assigned to each consumer.</p> <p></p> <p>Example: Suppose a topic has 6 partitions (0, 1, 2, 3, 4, 5) and there are 2 consumers in the group.    Consumer 1 would be assigned partitions 0, 1, 2 (a range of three partitions).    Consumer 2 would be assigned partitions 3, 4, 5 (the next range of three partitions).</p> <p>This strategy ensures a relatively uniform distribution of the number of partitions per consumer, though not necessarily the load if data is skewed across partitions.</p> <p>Round Robin Partitioner</p> <p>The Round Robin Partitioner distributes partitions among consumers in a rotating, round-robin fashion.    It iterates through the sorted list of partitions (0, 1, 2, 3, 4, 5).    It assigns the first partition to Consumer 1, the second to Consumer 2, the third back to Consumer 1, and so on.</p> <p></p> <p>Example: Suppose a topic has 6 partitions (0, 1, 2, 3, 4, 5) and there are 2 consumers in the group.    Consumer 1 would be assigned partitions 0, 2, 4.    Consumer 2 would be assigned partitions 1, 3, 5.</p> <p>This strategy aims for a more even distribution of partitions, which can sometimes lead to better load balancing if the message load per partition is relatively uniform.</p>"},{"location":"kafka/partition/#kafka-cluster-partition-reassignment","title":"Kafka Cluster &amp; Partition Reassignment","text":"<ol> <li>Kafka Cluster Controller: In a Kafka cluster, one of the brokers is designated as the controller. This controller is responsible for managing the states of partitions and replicas and for performing administrative tasks such as reassigning partitions.</li> <li>Partition Growth: It is important to note that the partition count of a Kafka topic can always be increased, but never decreased. This is because reducing partitions could lead to data loss.</li> <li>Partition Reassignment Use Cases: Partition reassignment is used in several scenarios:        Moving a partition across different brokers.        Rebalancing the replicas of a partition to a specific set of brokers.        Increasing the replication factor of a topic.</li> </ol>"},{"location":"kafka/partition/#replications","title":"Replications","text":"<p>Replicas are essentially backups of partitions. They are not directly read as raw data. Their primary purpose is to prevent data loss and provide fault tolerance. If the server hosting an active partition fails, a replica can take over.</p> <p>Illustrative Example: If the server hosting Partition P0 of sports_news crashes, a replica of P0 on another server immediately takes over, ensuring that no sports news articles are lost and the news feed remains continuous.</p> <p>One broker is marked leader and other brokers are called followers for a specific partition. This designated broker assumes the role of the leader for the topic partition. On the other hand, any additional broker that keeps track of the leader partition is called a follower and it stores replicated data for that partition.</p> <p>Tip</p> <p>Note that the leader receives and serves all incoming messages from producers and serves them to consumers. Followers do not serve read or write requests directly from producers or consumers. Followers just act as backups and can take over as the leader in case the current leader fails.</p> <p>Therefore, each partition has one leader and multiple followers.</p>"},{"location":"kafka/partition/#in-sync-replicas-isr","title":"In-Sync Replicas (ISR)","text":"<p>When a partition is replicated across multiple brokers, not all replicas are necessarily in sync with the leader at all times. The in-sync replicas represent the number of replicas that are always up-to-date and synchronized with the partition\u2019s leader. The leader continuously sends messages to the in-sync replicas, and they acknowledge the receipt of those messages.</p> <p>The recommended value for ISR is always greater than 1.</p> <p>Tip</p> <p>The ideal value of ISR is equal to the replication factor.</p>"},{"location":"kafka/producer/","title":"Producers","text":""},{"location":"kafka/producer/#producers","title":"Producers","text":"<p>Producers are applications that write or publish data to the topics within a Kafka cluster . They use the Producer API to send data . Producers can choose to write data either at the topic level (letting Kafka distribute it across partitions) or to specific partitions of a topic.</p> <p>Illustrative Example: A journalist writing a new article would be a producer, publishing the article to the news topic.</p>"},{"location":"kafka/producer/#producer-configurations","title":"Producer Configurations","text":"<p>When configuring a Kafka producer, several important settings are available:</p> <ol> <li>Bootstrap Servers:    Used to connect to the Kafka Cluster.    This specifies the host/port for establishing the initial connection.</li> <li>Client ID:    Used to track requests and is mainly for debugging purposes.    Primarily used for server-side logging.</li> <li>Key Serializer (and Value Serializer):    Converts the key/value into a stream of bytes.    Kafka producers send objects as a stream of bytes, and this setting is used to persist and transmit the object across the network.</li> <li>Connection Max Idle Ms:    Specifies the maximum number of milliseconds for an idle connection.    After this period, if the producer sends to the broker, it will use a disconnected connection.</li> <li> <p>Acks (Acknowledgements):    Determines the acknowledgement behavior when a producer sends records.     Three settings are possible:      Acks = 0: Producers will not wait for any acknowledgement from the server.  </p> <p></p> <p>Note</p> <pre><code>Behavior: The producer does not wait for any reply from the broker after sending a message. It assumes the message is successfully written immediately.\nRisk: This is the riskiest approach regarding data loss. If the broker goes offline, an exception occurs, or the message is simply not received due to network issues, the producer will not be aware of it, and the message will be lost.\nLatency: Offers the lowest latency because there's no waiting period for acknowledgements.\n</code></pre> <p>Acks = 1 (Default): The leader broker will write the record to its local log and respond without waiting for full acknowledgement from all followers. In this case, if the leader fails immediately after acknowledgement (before followers replicate), the record will be lost.</p> <p></p> <p>Note</p> <pre><code>Behavior: The producer waits for a success response from the broker only when the leader partition has received the message. Once the leader has written the message, it sends an acknowledgment back to the producer, allowing the producer to proceed.\nData Safety: Improves data safety compared to `acks=0`. If the message cannot be written to the leader (e.g., leader crashes), the producer receives an error and can retry sending the message, thus avoiding potential data loss.\nDisadvantage: While better, `acks=1` does not guarantee that the message has been replicated to other in-sync replicas. If the leader partition fails after acknowledging the message to the producer but before the message is replicated to its followers, the message could still be lost.\n</code></pre> <p>Acks = -1: The leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This provides the strongest guarantee and is equivalent to acks=all.</p> <p> </p> <p>Note</p> <pre><code>Behavior: The producer receives a success response from the broker only when the message has been successfully written not only to the leader partition but also to all configured in-sync replicas (ISRs).\n    The leader receives the message and writes it.\n    The leader then sends the message to all its in-sync follower replicas.\n    Once all in-sync replicas confirm they have written the message, they send acknowledgements back to the leader.\n    Only after the leader accumulates acknowledgements from all in-sync replicas, does it send the final success response to the producer.\nData Safety: This approach offers the highest level of data safety and guarantees against message loss. The chance of message loss is very low.\nDisadvantage: The primary drawback is added latency. The producer has to wait longer for the message to be fully replicated across all replicas before it gets the final acknowledgment. This makes it a slower process compared to `acks=0` or `acks=1`.\n</code></pre> </li> <li> <p>Compression Type:        Used to compress the messages.        The default value is \"none\".        Supported types include gzip, snappy, lz4, and zstd.        Compression is particularly useful for batches of data, and the efficiency of batching also affects the compression ratio.</p> </li> <li>Max Request Size:        Defines the maximum size of a request in bytes.        This setting impacts the number of record batches the producer will send in a single request.        It is used for sending large requests and also affects the cap on the maximum compressed record\u00a0batch\u00a0size\u00a0.</li> <li> <p>Batching:        Producer Batching: The producer collects records together and sends them in a single batch. This approach aims to reduce network latency and CPU load, thereby improving overall performance.</p> <p>Batch Size Configuration: The size of a batch is determined by a configuration parameter called batch.size, with a default value of 16KB. Each partition will contain multiple records within a batch. Importantly, all records within a single batch will be sent to the same topic and partition.</p> <p>linger.ms: This parameter specifies a duration the producer will wait to allow more records to accumulate and be sent together in a batch, further optimizing the sending process.    Memory Usage: Configuring larger batches may lead the producer to use more memory. There's also a concept of \"pre-allocation\" where a specific batch size is anticipated for additional records.</p> </li> <li> <p>Buffer Memory:        Producers use buffer memory to facilitate batching records.        Records ranging from 1MB to 2MB can be delivered to the server.        If the buffer memory becomes full, the producer will block for a duration specified by max.block.ms until memory becomes available. This max.block.ms represents the maximum time the producer will wait.        It is crucial to compare this max.block.ms setting with the total memory the producer is configured to use.        It's noted that not all producer memory is exclusively used for batching; some additional memory is allocated for compression and insight requests.</p> </li> </ol>"},{"location":"kafka/producer/#how-keys-determine-partition-assignment","title":"How Keys Determine Partition Assignment","text":"<p>Kafka uses the message key to decide which partition an incoming message will be written to.</p> <ol> <li> <p>With a Key (Hashing Algorithm):</p> <p>When a key is provided, Kafka applies a hashing algorithm to the key.    The output of the hashing function (which is based on the key) is then mapped to a specific partition out of the topic's available partitions.</p> <p> </p> <p>Example: If the hashing concept is \"divide by three and use the remainder,\" a key of <code>7</code> would result in a remainder of <code>1</code>, so the message would go to partition 1. A key of <code>5</code> would result in a remainder of <code>2</code>, so that message would go to partition 2.</p> <p>Crucial Point: All messages that share the same key will always go to the same partition. This is because the same key will consistently produce the same hash output, leading to the same partition assignment. This property is vital for maintaining message order for a specific logical entity (e.g., all events related to a particular user ID).</p> <p>Tip</p> <p>It's possible for different keys to end up in the same partition if their hash outputs happen to be the same, but messages with an identical key will always map to the same partition.           </p> </li> <li> <p>Without a Key (Null Key - Round Robin):</p> <p>    If the message key is <code>null</code> (i.e., no key is provided), Kafka uses a round-robin fashion to distribute messages among partitions.    This means the first message goes to partition 0, the second to partition 1, the third to partition 2, and then it cycles back to partition 0, and so on. This ensures an even distribution of messages when order per key is not a concern.</p> </li> </ol>"},{"location":"kafka/producer/#when-a-producer-sends-messages-in-kafka-the-process-involves","title":"When a producer sends messages in Kafka, the process involves","text":"<p>When an application produces a message using a Kafka API (e.g., Java or Python API), the record goes through a series of internal steps before being sent to the Kafka cluster.</p> <p>From Application to Producer</p> <p>An application sends a \"record\". A record typically contains a topic (where the record should go), an optional partition (though rarely specified directly by the user), a key, and the value (the actual message content). The key is primarily used for calculating the target partition.</p> <p>Step 1: Serialization</p> <p>The first thing the producer does is serialization for both the key and value.</p> <p>Purpose: Messages need to be sent over a network, which requires them to be converted into a binary format (a stream of zeros and ones or a byte array). The serializer handles this conversion from an object (your message data) to a byte array.</p> <p>Step 2: Partitioning</p> <p>After serialization, the binary data is sent to the partitioner. At this stage, the producer determines to which partition of the topic the record will be sent.</p> <p>How it works:</p> <p>If a key is provided: A hashing algorithm is applied to the key. The output of this hash is then typically divided by the total number of partitions for the topic, and the remainder determines the destination partition. Example: If a key <code>7</code> is hashed and then processed with \"divide by three and use the remainder\", it might map to partition <code>1</code>. A key <code>5</code> might map to partition <code>2</code> [based on video explanation in previous context]. The key ensures that messages with the same key consistently go to the same partition. If no key is provided (null key): Messages are distributed in a round-robin fashion across all available partitions, ensuring an even distribution.</p> <p>Step 3: Buffering</p> <p>Once the destination partition for a record is determined, the record is not immediately sent to the Kafka cluster. Instead, it is written into an internal buffer specifically assigned to that partition. Buffers accumulate multiple messages. </p> <p>This accumulation allows the producer to:</p> <p>Perform I/O operations more efficiently. Sending many small messages individually is less efficient than sending them in larger chunks. Apply compression more effectively. Compression algorithms often work better with larger blocks of data, as they can identify more patterns and redundancies. The producer aims to \"patch\" (batch) records for this efficiency.</p> <p>Step 4: Batching</p> <p>From the internal buffer, multiple messages are \"clubbed\" together into batches. These batches are then sent to the Kafka cluster.</p> <p>Two important configuration parameters control when a batch is sent:</p> <p><code>linger.ms</code>: This parameter instructs the producer to wait up to a certain number of milliseconds (e.g., 5 milliseconds) before sending the accumulated content. This allows more messages to collect in the buffer, potentially forming a larger, more efficient batch.</p> <p><code>batch.size</code>: This parameter defines the maximum size (in bytes) that a batch can reach (e.g., 5 MB).</p> <p>Batch Sending Logic: A batch is sent to the Kafka cluster when either the <code>linger.ms</code> timeout expires OR the <code>batch.size</code> limit is reached, whichever condition is satisfied first.</p> <p>Step 5: Sending to Kafka Cluster</p> <p>Once a batch is formed based on <code>linger.ms</code> or <code>batch.size</code> conditions, the complete batch is sent to the Kafka cluster.</p> <p>Step 6: Retries</p> <p>It's possible for message writing to fail due to networking issues or other problems.    Kafka producers can be configured to retry sending a message if the initial attempt fails.    You can set the number of retries (e.g., <code>retry=5</code>), meaning the producer will attempt to rewrite the message up to five times before throwing an exception.</p> <p>Step 7: Receiving Record Metadata</p> <p>If the message writing is successful (after retries, if any), the Kafka cluster sends <code>RecordMetadata</code> back to the producing application.</p> <p>This <code>RecordMetadata</code> provides crucial information about the successfully written record, including:     Partition: The specific partition where the data was written.     Offset: The offset (position) of the record within that partition.     Timestamp: The time when the record arrived or was written to Kafka.</p>"},{"location":"kafka/producer/#importance-of-understanding-producer-internals","title":"Importance of Understanding Producer Internals","text":"<p>Having a clear understanding of these internal mechanisms is vital for:</p> <ol> <li> <p>Troubleshooting: For instance, if messages are being produced at a very high rate and exceeding the producer's internal buffer volume (e.g., 32 MB), messages might not be written to Kafka. Knowing this allows you to increase the buffer volume to accommodate the incoming message flow.</p> </li> <li> <p>Performance Tuning: Properly configuring parameters like <code>linger.ms</code> and <code>batch.size</code> can significantly impact throughput and latency.</p> </li> <li> <p>Reliability: Understanding how retries work helps in building resilient applications that can handle transient failures.</p> </li> </ol>"},{"location":"kafka/producer/#producer-corner-cases-in-kafka-tuning-replica-management","title":"Producer Corner Cases in Kafka Tuning (Replica Management)","text":"<ol> <li>Replica Fetchers: This setting determines the number of threads responsible for replicating data from leaders. It's crucial to have a sufficient number of replica fetchers to enable complete parallel replication if multiple threads are available.</li> <li>replica.fetch.max.bytes: This parameter dictates the maximum amount of data used to fetch from any partition in each fetch request. It is generally beneficial to increase this parameter.</li> <li>replica.socket.receive.buffer.bytes: The size of buffers can be increased, especially if more threads are available.</li> <li>Creating Replicas: Increasing the level of parallelism allows for data to be written in parallel, which automatically leads to an increase in throughput.</li> <li>num.io.threads: This parameter is determined by the amount of disk available in the cluster and directly influences the value for I/O\u00a0threads\u00a0.</li> </ol>"},{"location":"kafka/producer/#experiment-scenario","title":"Experiment Scenario","text":"<p>A Kafka producer is configured with default settings, including <code>linger.ms=0</code> and <code>buffer.memory=32MB</code> (which is sufficient for small messages). The producer attempts to send 1000 messages (numbered 0 to 999) rapidly in a loop.</p> <p>Observed Result: The consumer only receives messages up to number <code>997</code>, and the Kafka logs confirm messages only up to <code>997</code> were written. Messages <code>998</code> and <code>999</code> are lost.</p> <p>Reason: The messages <code>998</code> and <code>999</code> were still present in the Kafka producer's internal buffer when the Python code finished execution (<code>Process finished with exit code 0</code>). The I/O thread did not get enough time to take these remaining messages from the buffer and publish them to the Kafka cluster before the application terminated.</p> <p>Solution: To ensure all messages are delivered, even when sending rapidly, you must explicitly tell the producer to flush its buffer before exiting or closing the connection. This is done using <code>producer.flush()</code> and <code>producer.close()</code> methods.</p> <p><code>producer.flush()</code>: This method explicitly flushes all accumulated messages from the producer's buffer to the Kafka cluster. It blocks until all messages in the buffer have been successfully sent and acknowledged by the brokers.</p> <p><code>producer.close()</code>: This method closes the producer connection, releasing any resources it holds. It implicitly calls <code>flush()</code> before closing, but it's often good practice to call <code>flush()</code> explicitly beforehand, especially if there's any risk of the <code>close()</code> method being interrupted.</p> <p>Before Fix (Potential Message Loss): <pre><code>from kafka import KafkaProducer\nimport json\nimport time\n\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\ntopic_name = 'hello-world'\nnum_messages = 1000\n\nfor i in range(num_messages):\n    data = {\"number\": i}\n    # print(f\"Sending: {data}\") # For debugging\n    producer.send(topic_name, value=data)\n\n# Application ends here. If messages are still in buffer, they are lost.\nprint(\"Producer finished sending messages.\")\n</code></pre></p> <p>After Fix (Ensured Delivery): <pre><code>from kafka import KafkaProducer\nimport json\nimport time\n\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\ntopic_name = 'hello-world'\nnum_messages = 1000\n\nfor i in range(num_messages):\n    data = {\"number\": i}\n    producer.send(topic_name, value=data)\n\n# Ensure all messages are sent from buffer to Kafka cluster\nproducer.flush()\n# Close the producer connection\nproducer.close()\n\nprint(\"Producer finished sending all messages and closed.\")\n</code></pre></p>"},{"location":"kafka/time/","title":"Index & Timeindex","text":"<p>When you set up a Kafka cluster, all the Kafka and ZooKeeper logs are stored in a dedicated <code>kafka-logs</code> folder. Within this folder, each topic and its partitions have their own directories. </p> <p>For instance, a topic named <code>my-topic</code> with one partition (partition 0) will have a folder named <code>my-topic-0</code>.</p> <p>Inside these partition-specific folders, Kafka stores all its messages in log files. These messages are appended sequentially to the log files, meaning new messages are always added to the end. This append-only design is fundamental to Kafka's performance.</p> <p>However, alongside these main log files (often with a <code>.log</code> extension), you'll notice other files with <code>.index</code> and <code>.timeindex</code> extensions. These are not where the actual message data resides, but they play a critical role in making message retrieval highly efficient.</p>"},{"location":"kafka/time/#the-challenge-of-large-log-files","title":"The Challenge of Large Log Files","text":"<p>Imagine a scenario where a Kafka log file grows to be enormously large as more and more messages are produced. If a consumer requests messages starting from a specific offset (e.g., <code>offset 1500</code>), the Kafka broker would have to scan the entire log file from the beginning until it finds the requested offset. This full-file scanning is highly inefficient, especially with high message throughput and retention. It's analogous to searching for a specific record in a traditional database without any indexes \u2013 you'd have to read every single row until you find what you're looking for. To avoid such inefficiencies, databases use indexing. Kafka implements a similar concept for its log files.</p>"},{"location":"kafka/time/#the-role-of-index-files-offset-index","title":"The Role of <code>.index</code> Files (Offset Index)","text":"<p>Purpose and Structure:     The <code>.index</code> files are Kafka's solution to the large log file scanning problem. Their primary purpose is to help the Kafka broker quickly find the exact position (byte offset) of a message for a given offset within a log file.</p> <p>An <code>.index</code> file contains a mapping of <code>offset</code> to <code>position</code> (byte offset) within the corresponding <code>.log</code> file.</p> <p>Example Content of an <code>.index</code> file:    <pre><code>offset: 831 position: 17165\noffset: 925 position: 19165\noffset: 1480 position: 30165\noffset: 1587 position: 32165\n</code></pre>    This indicates that the message with <code>offset 831</code> is located at byte <code>position 17165</code> in the actual log file.</p> <p>How it Works</p> <ol> <li>When a consumer requests messages from a specific offset (e.g., <code>offset 1500</code>).</li> <li>The Kafka broker first consults the <code>.index</code> file.</li> <li>Since the offsets in the index file are stored in sorted (ascending) order, the broker can perform a binary search on the offset values.</li> <li>This binary search quickly identifies the range where the requested offset should be. For <code>offset 1500</code>, the broker would find that it falls between <code>offset 1480</code> and <code>offset 1587</code>.</li> <li> <p>Knowing the byte <code>position</code> for <code>offset 1480</code> (which is <code>30165</code>), the broker knows it only needs to scan the actual log file from that approximate position onwards, drastically reducing the search area instead of starting from the beginning of the file.</p> </li> <li> <p>Configuration: <code>log.index.interval.bytes</code>    Kafka doesn't write an entry into the <code>.index</code> file for every single message. Instead, it writes entries periodically based on the accumulated data size.</p> <p>The default configuration property that controls this behavior is <code>log.index.interval.bytes</code>.   Its default value is 4096 bytes.   This means that roughly every 4096 bytes of new data accumulated in the Kafka topic, a new <code>offset</code> and its corresponding <code>position</code> will be written to the <code>.index</code> file.</p> </li> </ol> <p>Relative Offsets for Efficiency</p> <p>To save space and improve efficiency, the <code>.index</code> file stores relative offsets instead of absolute offsets.</p> <p>Every log segment (which we'll discuss next) has a base offset \u2013 the starting offset of messages within that segment.    In the <code>.index</code> file, the offsets are stored as the difference (or \"shift\") from this base offset.     For example, if a segment starts at <code>offset 90</code> and a message has an absolute offset of <code>108</code>, the index file would store <code>18</code> (108 - 90) as the offset value.    When the broker performs a lookup, it adds this relative offset to the segment's base offset to get the actual absolute offset.    However, when you use tools like <code>kafka-run-class.bat</code> to inspect an index file, the tool performs this calculation in the backend and displays the absolute offsets for readability.</p>"},{"location":"kafka/time/#the-role-of-timeindex-files-time-index","title":"The Role of <code>.timeindex</code> Files (Time Index)","text":"<p>Purpose and Structure:    The <code>.timeindex</code> files complement the <code>.index</code> files by allowing Kafka to efficiently locate messages based on their timestamp. This is particularly useful for business requirements where consumers need messages published after a certain point in time.</p> <p>A <code>.timeindex</code> file contains a mapping of <code>timestamp</code> to <code>offset</code>.</p> <p>Example Content of a <code>.timeindex</code> file:    <pre><code>timestamp: 1678886400000 offset: 925\ntimestamp: 1678886401000 offset: 1587\n</code></pre></p> <p>How it Works</p> <ol> <li>When a consumer requests messages published after a specific timestamp.</li> <li>The broker consults the <code>.timeindex</code> file.</li> <li>Similar to the offset index, timestamps in the <code>.timeindex</code> are sorted, allowing for a binary search to quickly find the approximate offset corresponding to the requested timestamp.</li> <li>Once an approximate offset is found from the <code>.timeindex</code> (e.g., <code>offset 925</code> for a timestamp).</li> <li>The broker then uses this offset to perform a lookup in the <code>.index</code> file (offset index).</li> <li>The <code>.index</code> file provides the exact byte position in the log file where that offset begins.</li> <li>Finally, the broker starts scanning the actual log file from that byte position, checking the message timestamps (which are part of the message payload) to ensure they meet the time requirement.</li> </ol>"},{"location":"kafka/trade/","title":"Unclean Leader Election & Availability vs Durability Trade-Off","text":""},{"location":"kafka/trade/#the-scenario-leader-broker-failure-and-data-loss-risk","title":"The Scenario: Leader Broker Failure and Data Loss Risk","text":"<p>Consider a situation where you have a topic partition <code>P2</code> with replicas on <code>Broker 1</code>, <code>Broker 2</code>, and <code>Broker 3</code>. Suppose <code>Broker 3</code> holds the leader for <code>P2</code>, and <code>Broker 1</code> and <code>Broker 2</code> hold its followers. Initially, all three are in-sync (ISR: 3, 2, 1).</p> <p>New Leader Election</p> <p>If the leader broker (<code>Broker 3</code>) goes down, a new leader must be chosen from the remaining in-sync replicas. For example, <code>Broker 2</code> might be elected as the new leader. At this point, the ISR would only include the active, in-sync replicas (e.g., ISR: 2, 1), as the original leader's broker is down.</p> <p>The Critical Data Loss Scenario</p> <p>Now, let's say the new leader (<code>Broker 2</code>) receives <code>Message 3</code> from a producer, but before <code>Message 3</code> can be fully replicated to the remaining follower (<code>Broker 1</code>), <code>Broker 2</code> also goes down.</p> <p>At this point:</p> <p><code>Message 3</code> was written to <code>Broker 2</code>'s partition.   <code>Broker 2</code> is now down, making <code>Message 3</code> inaccessible via its original location.   <code>Broker 1</code> is still up, but it only has <code>Message 1</code> and <code>Message 2</code>, not <code>Message 3</code> because replication was incomplete.</p> <p>Result: There is currently no in-sync replica that holds all the latest data, including <code>Message 3</code>. This is where the trade-off comes in.</p>"},{"location":"kafka/trade/#the-trade-off-availability-vs-durability","title":"The Trade-Off: Availability vs. Durability","text":"<p>When there are no in-sync replicas available for a partition, Kafka faces a critical decision:</p> <p>Option 1: Prioritize Durability (No Data Loss)</p> <p>Action: The partition <code>P2</code> becomes unavailable, and producers cannot publish new messages to it.</p> <p>Outcome: Data loss is prevented because the system waits for the original leader or a lagging replica to come back online and fully synchronize before accepting new writes.</p> <p>System State: The system is highly durable (reliable) but potentially less available.</p> <p>Option 2: Prioritize Availability (Potential Data Loss)</p> <p>Action: A partition that is not fully in-sync (e.g., <code>Broker 1</code>'s partition <code>P2</code>, which is missing <code>Message 3</code>) is chosen as the new leader. Producers can then immediately start publishing new messages (<code>Message 4</code>, <code>Message 5</code>, etc.) to this new leader.</p> <p>Outcome: <code>Message 3</code> is permanently lost because it was only present on the now-down <code>Broker 2</code> and never fully replicated to <code>Broker 1</code>.</p> <p>System State: The system is highly available (continuous operation) but potentially less durable due to data loss.</p> <p>This choice is controlled by a Kafka configuration property: <code>unclean.leader.election.enable</code>.</p>"},{"location":"kafka/trade/#uncleanleaderelectionenable-configuration","title":"unclean.leader.election.enable Configuration","text":"<p>This crucial Kafka property determines which of the two options (durability or availability) Kafka will prioritize during a leader failure when no in-sync replicas are available.</p> <ol> <li>unclean.leader.election.enable = true</li> </ol> <p>Behavior: Allows Kafka to elect a replica that is not in-sync (i.e., it's \"unclean\" because it doesn't have all the latest data) as the new leader.</p> <p>Benefit: Keeps the system highly available. Producers can continue writing immediately.</p> <p>Risk: There might be some amount of data loss (as seen with <code>Message 3</code> in our example).</p> <p>Use Cases: Suitable for scenarios where a small amount of data loss is acceptable, such as:     Log aggregation     Metrics calculation</p> <ol> <li>unclean.leader.election.enable = false</li> </ol> <p>Behavior: Prevents a replica that is not in-sync from becoming the leader. The system will wait until an in-sync replica becomes available or the original leader recovers and synchronizes.</p> <p>Benefit: Ensures the system is highly durable, meaning no data will be lost.</p> <p>Risk: The partition will be unavailable for a period, blocking producers from publishing messages until an in-sync leader is established.</p> <p>Use Cases: Essential for scenarios where data loss is absolutely unacceptable, such as:     Transaction-related information in banking or financial sectors     Any system where monetary transactions are involved</p>"},{"location":"kafka/zookeeper/","title":"Role of Zookeeper","text":""},{"location":"kafka/zookeeper/#role-of-zookeeper-in-kafka","title":"Role of Zookeeper in Kafka","text":"<p>Zookeeper is a critical component used to monitor Kafka clusters and coordinate with them . It stores all the metadata information related to Kafka clusters, including the status of replicas and leaders . This metadata is crucial for configuration information, cluster health, and leader election within the cluster . Zookeeper nodes working together to manage distributed systems are known as a Zookeeper Cluster or Zookeeper Ensemble .</p> <p>Illustrative Example: If a Kafka server hosting a partition's leader fails, Zookeeper quickly identifies this and coordinates the election of a new leader from the available replicas, ensuring continuous operation. Zookeeper uses specific parameters and maintains various internal states to manage Kafka.</p>"},{"location":"kafka/zookeeper/#zookeeper-configuration-concepts","title":"Zookeeper Configuration Concepts","text":"<ol> <li>initLimit: Defines the time in milliseconds that a Zookeeper follower node can take to initially connect to a leader. For example, 5  2 seconds means 10 seconds. If a node doesn't get in sync within this limit, it's considered out of time.</li> <li>syncLimit: Defines the time in milliseconds that a Zookeeper follower can be out of sync with the leader. For example, 10  2 seconds means 20 seconds. If a node doesn't sync within this limit, it's considered out of time.</li> <li>clientPort: This is the port number (e.g., 2181) where Zookeeper clients connect. It refers to the data directory used to store client node server details.</li> <li>maxClientCnxns: This parameter sets the maximum number of client connections that a single Zookeeper server can handle at once.</li> <li>server.1, server.2, server.3: These entries define the server IDs and their IP addresses/ports within the Zookeeper ensemble (e.g., server.1: 2888:3888). These are crucial for leader election among the Zookeeper servers.</li> </ol>"},{"location":"kafka/zookeeper/#kafka-partition-states-as-managed-by-zookeeper","title":"Kafka Partition States (as managed by Zookeeper)","text":"<ol> <li>New Nonexistent Partition: This state indicates that a partition was either never created or was created and then subsequently deleted.</li> <li>Nonexistent Partition (after deletion): This state specifically means the partition was deleted.</li> <li>Offline Partition: A partition is in this state when it should have replicas assigned but has no leader elected.</li> <li>Online Partition: A partition enters this state when a leader is successfully elected for it. If all leader election processes are successful, the partition transitions from Offline Partition to Online Partition.</li> </ol>"},{"location":"kafka/zookeeper/#kafka-replica-states-as-managed-by-zookeeper","title":"Kafka Replica States (as managed by Zookeeper)","text":"<ol> <li>New Replica: Replicas are created during topic creation or partition reassignment. In this state, a replica can only receive follower state change requests.</li> <li>Online Replica: A replica is considered Online when it is started and has assigned replicas for its partition. In this state, it can either become a leader or become a follower based on state change requests.</li> <li>Offline Replica: If a replica dies (becomes unavailable), it moves to this state. This typically happens when the replica is down.</li> <li>Nonexistent Replica: If a replica is deleted, it moves into this\u00a0state\u00a0[3].</li> </ol>"},{"location":"kafka/zookeeper/#what-does-zookeeper-do-in-kafka-cluster","title":"What does ZooKeeper do in Kafka Cluster?","text":"<ol> <li>Broker Management: It helps manage and coordinate the Kafka brokers, and keeps a list of them.</li> <li>Topic Configuration Management: ZooKeeper maintains a list of topics, number of partitions for each topic, location of each partition and the list of consumer groups.</li> <li>Leader Election: If a leader (the node managing write and read operations for a partition) fails, ZooKeeper can trigger leader election and choose a new leader.</li> <li>Cluster Membership: It keeps track of all nodes in the cluster, and notifies if any of these nodes fail.</li> <li>Synchronization: ZooKeeper helps in coordinating and synchronizing between different nodes in a Kafka cluster.</li> </ol>"},{"location":"projects/flink-ignite/","title":"Flink ignite","text":"<p>This project draws inspiration from this Medium article: Enhancing Real-Time Analytics with Apache Ignite and Flink SQL</p> <p>A custom Apache Flink JDBC dialect implementation for Apache Ignite, enabling seamless integration between Flink and Ignite via the JDBC connector. This dialect allows Flink to read from and write to Apache Ignite tables using SQL and the Table API.</p>"},{"location":"projects/flink-ignite/#features","title":"Features","text":"<ul> <li>Custom <code>JdbcDialect</code> for Apache Ignite</li> <li>Supports upsert (MERGE) statements for idempotent writes</li> <li>Compatible with Flink Table &amp; SQL API</li> <li>Auto-discovery via Java SPI (no manual registration required)</li> <li>Supports a wide range of SQL types</li> </ul>"},{"location":"projects/flink-ignite/#requirements","title":"Requirements","text":"<ul> <li>Java 8+</li> <li>Apache Maven</li> <li>Apache Flink 1.16.1</li> <li>Apache Ignite 2.15.0</li> </ul>"},{"location":"projects/flink-ignite/#build-instructions","title":"Build Instructions","text":"<ol> <li>Clone this repository:    <pre><code>git clone &lt;repo-url&gt;\ncd flink-ignite-custom-diaelect\n</code></pre></li> <li>Build the JAR using Maven:    <pre><code>mvn clean package\n</code></pre>    The resulting JAR will be in <code>target/flink-ignite-dialect-1.0.0.jar</code>.</li> </ol>"},{"location":"projects/flink-ignite/#usage","title":"Usage","text":"<ol> <li>Add the JAR to Flink:</li> <li>Copy <code>flink-ignite-dialect-1.0.0.jar</code> to the <code>lib/</code> directory of your Flink distribution or add it to your job\u2019s classpath.</li> <li>Ensure Ignite JDBC Driver is available:</li> <li>The dialect uses <code>org.apache.ignite.IgniteJdbcThinDriver</code>. Make sure the Ignite JDBC driver JAR is also present in the Flink <code>lib/</code> directory.</li> <li>Define a Flink Table with Ignite:    Example Flink SQL DDL:    <pre><code>CREATE TABLE ignite_table (\n  id INT PRIMARY KEY NOT ENFORCED,\n  name STRING,\n  value DOUBLE\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:ignite:thin://&lt;ignite-host&gt;:10800',\n  'table-name' = 'my_table',\n  'driver' = 'org.apache.ignite.IgniteJdbcThinDriver'\n);\n</code></pre></li> <li> <p>Replace <code>&lt;ignite-host&gt;</code> and <code>my_table</code> with your Ignite node and table name.</p> </li> <li> <p>Use in Flink SQL or Table API:</p> </li> <li>You can now read from and write to Ignite tables using Flink SQL or the Table API.</li> </ol>"},{"location":"projects/flink-ignite/#configuration-options","title":"Configuration Options","text":"<p>Common connector options (see Flink JDBC connector docs):</p> <ul> <li><code>'connector' = 'jdbc'</code></li> <li><code>'url' = 'jdbc:ignite:thin://&lt;ignite-host&gt;:10800'</code></li> <li><code>'table-name' = '&lt;ignite-table&gt;'</code></li> <li><code>'driver' = 'org.apache.ignite.IgniteJdbcThinDriver'</code></li> <li><code>'username'</code> / <code>'password'</code> (if authentication is enabled)</li> <li><code>'sink.buffer-flush.max-rows'</code>, <code>'sink.buffer-flush.interval'</code>, etc.</li> </ul>"},{"location":"projects/flink-ignite/#how-it-works","title":"How It Works","text":"<ul> <li>The dialect is auto-registered via the Java Service Provider Interface (SPI) using the file:</li> <li><code>src/main/resources/META-INF/services/org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory</code></li> <li>Flink will automatically use this dialect for any JDBC URL starting with <code>jdbc:ignite:thin:</code>.</li> </ul>"},{"location":"projects/kafkaapi/","title":"FastAPI Kafka Producer API","text":"<p>A production-ready FastAPI-based REST API for publishing JSON messages to Apache Kafka topics. Designed for high availability, containerized with Docker, and includes Prometheus metrics support for observability.</p> <p></p>"},{"location":"projects/kafkaapi/#features","title":"Features","text":"<ul> <li>REST to Kafka: Publish JSON data to Kafka via POST endpoint.</li> <li>Health Check: <code>/health</code> endpoint for service monitoring.</li> <li>Prometheus Metrics: Available at <code>/metrics</code>.</li> <li>Logging: Rotating file logs stored at <code>/var/log/fastapi_kafka_api.log</code>.</li> <li>Containerized: Docker &amp; Docker Compose support for fast deployment.</li> </ul> <p>Tip</p> <p>For better understanding, you should clone the repo <code>fastapipythonkafka</code> <pre><code>git clone https://github.com/manish-chet/fastapipythonkafka\n</code></pre></p> <p>Note</p> <p>If you're looking for the full FastAPI + Kafka implementation, see main.py</p>"},{"location":"projects/kafkaapi/#docker-usage","title":"Docker Usage","text":"<p>Build and run the container <pre><code>docker build -t fastapi-kafka .\ndocker run -p 5000:5000 fastapi-kafka\n</code></pre></p> <p>With Docker Compose use fast-api.yaml and kafka-docker.yaml to spin up FastAPI with Kafka locally: <pre><code>docker-compose -f kafka-docker.yaml -f fast-api.yaml up -d\n</code></pre></p>"},{"location":"projects/kafkaapi/#kubernetes-usage","title":"Kubernetes Usage","text":"<p>Deployment.yaml: 3 replicas with environment variables for Kafka.</p> <p>Service.yaml: LoadBalancer (or NodePort for local) to expose FastAPI.</p> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre>"},{"location":"projects/kafkaapi/#publish-to-kafka","title":"Publish to Kafka","text":"<pre><code>curl -X POST \\\n  http://localhost:5000/kafka/publish/mytopic \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"key\": \"value\"}'\n</code></pre> <ul> <li> <p>Request</p> <p>Path Param: topic_name \u2013 Kafka topic to publish to</p> <p>Body: JSON object (arbitrary schema)</p> </li> <li> <p>Responses</p> <p>200 OK: Successfully published</p> <p>400: Empty body</p> <p>413: Message exceeds 10MB</p> <p>500: Internal server error</p> </li> </ul>"},{"location":"projects/kafkaapi/#screenshots-of-local-deployment","title":"Screenshots of local deployment","text":""},{"location":"projects/kafkaapi/#metrics","title":"Metrics","text":"<p>Prometheus metrics are exposed at <code>/metrics</code> (enabled by prometheus-fastapi-instrumentator).</p>"},{"location":"projects/readme/","title":"List of Projects","text":"<p>Hi, you will find list of projects which I had worked on so far:</p> <ul> <li>FastAPI Kafka Producer</li> <li>Flink-Ignite Custom Dialect</li> </ul>"},{"location":"spark/Rdd/","title":"Resilient Distributed Datasets","text":"<p>RDDs are the building blocks of any Spark application. </p> <p>RDDs Stands for</p> <ul> <li>Resilient: Fault tolerant and is capable of rebuilding data on failure</li> <li>Distributed: Distributed data among the multiple nodes in a cluster</li> <li>Dataset: Collection of partitioned data with values</li> </ul> <p>Here are some key points about RDDs and their properties:</p> <ul> <li> <p>Fundamental Data Structure: RDD is the fundamental data structure of Spark, which allows it to efficiently operate on large-scale data across a distributed environment.</p> </li> <li> <p>Immutability: Once an RDD is created, it cannot be changed. Any transformation applied to an RDD creates a new RDD, leaving the original one untouched.</p> </li> <li> <p>Resilience: RDDs are fault-tolerant, meaning they can recover from node failures. This resilience is provided through a feature known as lineage, a record of all the transformations applied to the base data.</p> </li> <li> <p>Lazy Evaluation: RDDs follow a lazy evaluation approach, meaning transformations on RDDs are not executed immediately, but computed only when an action (like count, collect) is performed. This leads to optimized computation.</p> </li> <li> <p>Partitioning: RDDs are partitioned across nodes in the cluster, allowing for parallel computation on separate portions of the dataset.</p> </li> <li> <p>In-Memory Computation: RDDs can be stored in the memory of worker nodes, making them readily available for repeated access, and thereby speeding up computations.</p> </li> <li> <p>Distributed Nature: RDDs can be processed in parallel across a Spark cluster, contributing to the overall speed and scalability of Spark.</p> </li> <li> <p>Persistence: Users can manually persist an RDD in memory, allowing it to be reused across parallel operations. This is useful for iterative algorithms and fast interactive use.</p> </li> <li> <p>Operations: Two types of operations can be performed on RDDs - transformations (which create a new RDD) and actions (which return a value to the driver program or write data to an external storage system).</p> </li> </ul>"},{"location":"spark/Rdd/#when-to-use-rdds-advantages","title":"When to Use RDDs (Advantages)","text":"<p>Despite the general recommendation to use DataFrames/Datasets, RDDs have specific use cases where they are advantageous:</p> <ul> <li> <p>Unstructured Data: RDDs are particularly well-suited for processing unstructured data where there is no predefined schema, such as streams of text, media, or arbitrary bytes. For structured data, DataFrames and Datasets are generally better.</p> </li> <li> <p>Full Control and Flexibility: If you need fine-grained control over data processing at a very low level and want to optimize the code manually, RDDs provide that flexibility. This means the developer has more control over how data is transformed and distributed.</p> </li> <li> <p>Type Safety (Compile-Time Errors): RDDs are type-safe. This means that if there's a type mismatch (e.g., trying to add an integer to a string), you will get an error during compile time, before the code even runs. This can help catch errors earlier in the development cycle, unlike DataFrames or SQL queries which might only show errors at runtime</p> </li> </ul>"},{"location":"spark/Rdd/#why-you-should-not-use-rdds-disadvantages","title":"Why You Should NOT Use RDDs (Disadvantages)","text":"<p>For most modern Spark applications, especially with structured or semi-structured data, RDDs are generally discouraged due to several drawbacks:</p> <ul> <li> <p>No Automatic Optimization by Spark: Spark's powerful Catalyst Optimizer does not perform optimizations automatically for RDD operations. This means the responsibility for writing optimized and efficient code falls entirely on the developer.</p> </li> <li> <p>Complex and Less Readable Code: Writing RDD code can be complex and less readable compared to DataFrames, Datasets, or SQL. The code often requires explicit handling of data transformations and aggregations, which can be verbose.</p> </li> <li> <p>Potential for Inefficient Operations: Expensive Shuffling: Without Spark's internal optimizations, RDD operations can lead to inefficient data shuffling. In contrast, DataFrames/Datasets using the \"what to\" approach allow Spark to rearrange operations (e.g., filter first, then shuffle) to optimize performance, saving significant computational resources.</p> </li> </ul> <p>Example</p> <p>If you perform a reduceByKey (which requires shuffling data across nodes) before a filter operation, Spark will shuffle all the data first, then filter it. If the filter significantly reduces the dataset size, shuffling the larger pre-filtered dataset becomes a very expensive operation.</p> <ul> <li>Developer Burden: Because Spark doesn't optimize RDDs, the developer must have a deep understanding of distributed computing and Spark's internals to write performant RDD code. This makes development harder and slower compared to using higher-level APIs</li> </ul>"},{"location":"spark/Rdd/#difference","title":"Difference","text":"Criteria RDD (Resilient Distributed Dataset) DataFrame DataSet Abstraction Low level, provides a basic and simple abstraction. High level, built on top of RDDs. Provides a structured and tabular view on data. High level, built on top of DataFrames. Provides a structured and strongly-typed view on data. Type Safety Provides compile-time type safety, since it is based on objects. Doesn't provide compile-time type safety, as it deals with semi-structured data. Provides compile-time type safety, as it deals with structured data. Optimization Optimization needs to be manually done by the developer (like using <code>mapreduce</code>). Makes use of Catalyst Optimizer for optimization of query plans, leading to efficient execution. Makes use of Catalyst Optimizer for optimization. Processing Speed Slower, as operations are not optimized. Faster than RDDs due to optimization by Catalyst Optimizer. Similar to DataFrame, it's faster due to Catalyst Optimizer. Ease of Use Less easy to use due to the need of manual optimization. Easier to use than RDDs due to high-level abstraction and SQL-like syntax. Similar to DataFrame, it provides SQL-like syntax which makes it easier to use. Interoperability Easy to convert to and from other types like DataFrame and DataSet. Easy to convert to and from other types like RDD and DataSet. Easy to convert to and from other types like DataFrame and RDD."},{"location":"spark/adaptive/","title":"Adaptive Query Execution","text":""},{"location":"spark/adaptive/#understanding-adaptive-query-execution-aqe-in-spark","title":"Understanding Adaptive Query Execution (AQE) in Spark","text":"<p>Adaptive Query Execution (AQE) is a powerful feature introduced in Spark 3.0 and later versions that provides flexibility to dynamically change query execution plans at runtime. This means that Spark can adapt and optimize your query execution based on actual data characteristics and runtime statistics, rather than relying solely on initial, compile-time plans.</p> <p>The primary purpose and benefit of AQE is to significantly improve query performance. For instance, if an initial plan decides on a Sort-Merge Join for two large tables, but after intermediate transformations (like filters) one table becomes small enough to be broadcasted, AQE can dynamically switch to a Broadcast Join, which is much faster.</p>"},{"location":"spark/adaptive/#why-aqe-is-needed","title":"Why AQE is Needed","text":"<p>AQE addresses several common performance issues in Spark queries, especially those arising from data skew or suboptimal initial query plans:</p> <ul> <li>Dynamic Optimization: Initial query plans might be suboptimal because they are formed based on estimated data sizes. Data sizes can change significantly after transformations like filters are applied. AQE continuously monitors runtime statistics to make better decisions.</li> <li>Improved Performance: By dynamically adapting strategies (like join types) and optimizing partition management, AQE can drastically reduce query execution time.</li> <li>Resource Efficiency: It helps in better utilization of cluster resources by reducing wasted CPU cores and managing tasks more effectively.</li> <li>Simplified Partition Management: It removes the burden from users to manually define the optimal number of partitions, as AQE can dynamically coalesce them.</li> </ul>"},{"location":"spark/adaptive/#key-capabilities-of-aqe","title":"Key Capabilities of AQE","text":"<p>AQE provides three main capabilities to dynamically optimize Spark queries at runtime:</p>"},{"location":"spark/adaptive/#dynamically-coalescing-shuffle-partitions","title":"Dynamically Coalescing Shuffle Partitions","text":"<p>This feature helps manage the number and size of partitions after a shuffle operation, particularly when dealing with data skew.</p> <p>The Problem Before AQE (or without this feature enabled):</p> <ul> <li>When a wide dependency transformation like groupBy (which involves a shuffle) is performed, Spark typically creates 200 partitions by default.</li> <li>If your data is skewed (e.g., 80% of sales come from one product like \"sugar,\" while other products contribute very little), after shuffling, you'll end up with a few very large partitions (for the skewed key, like sugar) and many very small or empty partitions.</li> <li>For example, if you have 5 distinct product keys and 200 default shuffle partitions, you might have 5 non-empty partitions and 195 empty partitions.</li> <li>The Spark scheduler still has to schedule and monitor tasks for all 200 partitions, even the empty ones, leading to resource wastage (CPU cores) and increased overhead.</li> <li>Furthermore, the single large partition for the skewed key (e.g., \"sugar\") will take a significantly longer time to process than the smaller ones, leading to a situation where \"199 out of 200 tasks complete, but one task takes a very long time\".</li> </ul> <p>How AQE Solves It (Dynamically Coalescing):</p> <ul> <li>AQE's entry point is exactly where data begins to shuffle. It has an \"AQE shuffle reader\" that reads the shuffled data.</li> <li>AQE dynamically merges (coalesces) these small and empty shuffle partitions into a fewer, more manageable number of partitions.</li> <li>Conceptual Example:<ul> <li>Imagine you have two initial data partitions (P1, P2) where \"sugar\" data is prevalent, and other products (yellow, blue, white, red) are less frequent.</li> <li>After a groupBy and shuffle, \"sugar\" data might consolidate into one large partition (P1), while other products form smaller partitions (P2, P3, P4, P5), but many (195 out of 200) default partitions remain empty.</li> <li>AQE observes these small, manageable partitions (P2, P3, P4, P5) and merges them together into a single, larger, more balanced partition.</li> <li>This reduces the total number of active tasks. If there were 5 active tasks initially (1 for sugar, 4 for other products), after coalescing the 4 small ones, there might be only 2 active tasks: one for sugar, and one for the merged smaller products.</li> </ul> </li> <li>Benefits:<ul> <li>Reduced number of tasks: Fewer tasks mean less scheduling and monitoring overhead for Spark.</li> <li>Saved CPU Cores: If two tasks are merged into one, you save one CPU core.</li> <li>Balanced Workload (Partial): While the skewed \"sugar\" partition might still be large, other smaller partitions are balanced, leading to faster completion for those.</li> <li>Dynamic Partition Sizing: Instead of pre-defining a fixed number of partitions, AQE dynamically determines the optimal number based on actual data characteristics.</li> </ul> </li> </ul> <p>Handling Highly Skewed Data (Splitting):</p> <ul> <li>Even with coalescing, if one partition (like the \"sugar\" partition) remains extremely large (e.g., 80% of data), it can still cause \"1 out of 2 tasks\" to be very slow.</li> <li>In such cases, AQE can split the highly skewed large partition into multiple smaller ones.</li> <li>Conditions for Splitting Skewed Data: A partition will be split if both of these conditions are met:<ol> <li>The size of the skewed data partition is greater than 256 MB (e.g., 80% of data is very large).</li> <li>The size of the skewed data is more than 5 times the median size of all other partitions (e.g., if the median partition size is 5 MB, the skewed partition must be &gt; 25 MB and also &gt; 256 MB for splitting to occur).</li> </ol> </li> <li>Conceptual Example: The \"sugar\" partition (80% of data) could be split into four smaller partitions, each approximately 20% of the data, to balance the workload. This results in tasks that complete in similar times.</li> </ul>"},{"location":"spark/adaptive/#dynamically-switching-join-strategy","title":"Dynamically Switching Join Strategy","text":"<p>AQE can change the type of join executed at runtime based on the actual size of the tables after transformations.</p> <p>The Problem Before AQE: - When Spark initially creates the query plan (DAG), it might decide on a Sort-Merge Join if both tables involved in the join are large (e.g., Table 1 is 10 GB, Table 2 is 20 GB). Sort-Merge Join involves shuffling and sorting, which can be computationally expensive. - However, after applying multiple transformations, such as filters, to these tables, their sizes can significantly reduce. For example, Table 1 might become 8 GB, and Table 2 might become just 5 MB (less than 10 MB). - Without AQE, the initial plan would still execute a Sort-Merge Join, even though a much faster join strategy is now possible. How AQE Solves It (Dynamically Switching): - AQE continuously monitors the runtime statistics of tables during execution. - If, after transformations, one of the tables becomes small enough to fit into memory (typically less than a configured broadcast threshold, often around 10MB or more depending on configuration), AQE will dynamically switch the join strategy to a Broadcast Join. - Conceptual Example:     - Initial plan: Join Table 1 (10 GB) and Table 2 (20 GB) on product_name key. Spark defaults to Sort-Merge Join.     - Runtime transformation: Filters are applied, reducing Table 2 to 5 MB.     - AQE action: Since Table 2 is now small enough, AQE detects this at runtime and changes the join strategy from Sort-Merge Join to Broadcast Join. - Benefits:     - Significantly Faster Joins: Broadcast Join is generally the fastest join strategy as it avoids shuffling and sorting the larger table, instead broadcasting the smaller table to all executor nodes.     - Runtime Adaptability: AQE ensures that the most optimal join strategy is used based on the actual data sizes encountered during execution, not just initial estimates. - Note: While it optimizes join strategy, shuffling might still occur for the other (larger) table depending on the overall query plan, but the expensive Sort-Merge operation for the join itself is avoided.</p>"},{"location":"spark/adaptive/#dynamically-optimizing-skewed-joins","title":"Dynamically Optimizing Skewed Joins","text":"<p>This feature specifically tackles performance bottlenecks caused by data skew during join operations. The Problem Before AQE: - When joining two tables, even if they initially appear large, the data might be skewed on the join key. For example, if \"sugar\" product data is 80% of the sales in both tables, and these tables are being joined on the \"product name\" key. - After shuffling to bring matching keys together, a single partition corresponding to the skewed key (e.g., \"sugar\") can become extremely large. - This large partition can cause issues:     - \"Out Of Memory Exception\": Executors trying to process this single massive partition might run out of memory.     - Long-running Tasks: Similar to shuffle coalescing, this skewed partition will take a disproportionately long time to process, leading to the \"199 out of 200 tasks complete, but one task is stuck\" scenario.     - Query Failure: The query might eventually fail due to memory issues or timeouts. How AQE Solves It (Splitting Skewed Partitions and Duplicating Data): - AQE utilizes its \"AQE shuffle reader\" (also used for coalescing) to identify and analyze skewed partitions during the join process. - If a partition is identified as skewed (based on the same conditions as coalescing, i.e., &gt; 256MB and &gt; 5x median size), AQE will split this large, skewed partition into multiple smaller partitions. - Conceptual Example:     - You have two tables, Table 1 (left side) and Table 2 (right side), both with skewed data where \"sugar\" is the dominant key.     - After the initial shuffle and grouping by the join key, the \"sugar\" partition becomes excessively large on both sides of the join.     - AQE detects this skew in the \"sugar\" partition.     - AQE splits the large \"sugar\" partition on the left side into multiple smaller partitions (e.g., two parts: P1 and P2).     - To enable these new smaller partitions to join correctly, AQE duplicates the corresponding data for the skewed key from the right-hand side table. This means if the left-side sugar partition is split into two, the right-side sugar data will be duplicated so that each of the two new left-side partitions has a copy of the right-side sugar data to join with. - Benefits:     - Avoids Out Of Memory (OOM) Errors: By breaking down large partitions, the memory footprint per task is reduced, preventing OOM exceptions.     - Balanced Task Execution: All tasks, including those processing previously skewed data, complete in a more balanced and timely manner.     - Self-Sufficient Partitions: Each new, smaller partition becomes self-sufficient for performing the join, improving parallelism and overall performance.</p>"},{"location":"spark/bp/","title":"Best practices to design Spark application","text":""},{"location":"spark/bp/#understand-your-data-and-workload","title":"Understand Your Data and Workload","text":"<ol> <li> <p>Data Size: Understand the volume of data your application will process. This will influence the infrastructure needed and the parallelism level in your application.</p> </li> <li> <p>Data Skewness: Identify if your data is skewed, as this can cause certain tasks to take longer to complete and negatively impact performance. Techniques like key salting can be applied to handle skewness.</p> </li> <li> <p>Workload Type: Understand the type of operations your application will perform. For example, analytical workloads may benefit from columnar data formats like Parquet.</p> </li> </ol>"},{"location":"spark/bp/#application-design","title":"Application Design","text":"<ol> <li> <p>Transformations: Use transformations like map, filter, reduceByKey over actions as much as possible, as transformations are lazily evaluated and can benefit from Spark's optimization.</p> </li> <li> <p>Shuffling: Minimize operations that cause data shuffling across the network, as they are expensive. Avoid operations like groupByKey in favor of reduceByKey or aggregateByKey.</p> </li> <li> <p>Broadcasting: Small datasets that are used across transformations should be broadcasted to improve performance.</p> </li> <li> <p>Partitioning: Use the right level of partitioning. Too few partitions can lead to fewer concurrent tasks and underutilization of resources. Too many partitions might lead to excessive overhead</p> </li> </ol>"},{"location":"spark/bp/#resource-allocation-and-configuration","title":"Resource Allocation and Configuration","text":"<ol> <li> <p>Memory Allocation: Properly allocate memory to Spark executors, driver, and overhead. Monitor the memory usage of your application to fine-tune these settings.</p> </li> <li> <p>Dynamic Resource Allocation: Enable dynamic resource allocation if supported by your cluster manager. This allows Spark to adjust the resources based on workload.</p> </li> <li> <p>Parallelism: Configure the level of parallelism (number of partitions) based on the data volume and infrastructure. Infrastructure Consideration:</p> </li> <li> <p>Storage: Use fast storage (like SSDs) to store your data, as I/O operations can become a bottleneck in large data processing.</p> </li> <li> <p>Network: A high-speed network is important, especially if your workload involves data shuffling.</p> </li> <li> <p>Nodes and Cores: More nodes with multiple cores can increase parallelism and data processing speed.</p> </li> <li> <p>Data Locality: Aim for data locality, i.e., running tasks on nodes where the data is stored, to reduce network I/O</p> </li> </ol>"},{"location":"spark/bp/#monitor-and-iterate","title":"Monitor and Iterate","text":"<p>Use Spark's built-in web UI to monitor your applications. Look for stages that take a long time to complete, tasks that fail and are retried, storage and computation bottlenecks, and executor memory usage.</p> <p>Data skewness in Spark occurs when the data is not evenly distributed across partitions. This often happens when certain keys in your data have many more values than others. Consequently, tasks associated with these keys take much longer to run than others, which can lead to inefficient resource utilization and longer overall job execution time.</p> <p>Here are a few scenarios where data skewness can occur:</p> <ol> <li> <p>Join Operations: When you perform a join operation on two datasets based on a key, and some keys have significantly more values than others, these keys end up having larger partitions. The tasks processing these larger partitions will take longer to complete.</p> </li> <li> <p>GroupBy Operations: Similar to join operations, when you perform a groupByKey or reduceByKey operation, and some keys have many more values than others, data skewness can occur.</p> </li> <li> <p>Data Distribution: If the data distribution is not uniform, such that certain partitions get more data than others, then data skewness can occur. This could happen due to the nature of the data itself or the partitioning function not distributing the data evenly.</p> </li> </ol>"},{"location":"spark/bp/#how-to-deal-with-data-skewness","title":"How to deal with data skewness","text":"<p>Handling data skewness is a common challenge in distributed computing frameworks like Apache Spark. Here are some popular techniques to mitigate it: 1. Salting: Salting involves adding a random component to a skewed key to create additional unique keys. After performing the operation (like a join), the extra key can be dropped to get back to the original data.</p> <ol> <li> <p>Splitting skewed data: Identify the skewed keys and process them separately. For instance, you can filter out the skewed keys and perform a separate operation on them.</p> </li> <li> <p>Increasing the number of partitions: Increasing the number of partitions can distribute the data more evenly. However, this might increase the overhead of managing more partitions.</p> </li> <li> <p>Using reduceByKey instead of groupByKey: reduceByKey performs local aggregation before shuffling the data, which reduces the data transferred over the network.</p> </li> <li> <p>Using Broadcast Variables: When joining a large DataFrame with a small DataFrame, you can use broadcast variables to send a copy of the small DataFrame to all nodes. This avoids shuffling of the large DataFrame.</p> </li> </ol> <p>Total blocks - 128MB default block size (HDFS) - 1TB -&gt; 1 *1024 1024/128 = 8192.</p> <p>Let us take:</p> <ol> <li>20 Executor-machine</li> <li>5 Core processor in each executor node</li> <li>6 GB RAM in each executor node</li> </ol> <p>And the cluster can perform (20*5=100) Task parallel at a time, Here tasks mean block so 100 blocks can be processed parallelly at a time.</p> <p>100*128MB = 12800 MB / 1024GB = 12.5 GB (So, 12GB data will get processed in 1st set of a batch)</p> <p>Since the RAM size is 6GB in each executor, (20 executor x 6GB RAM =120GB Total RAM) So, at a time 12GB of RAM will occupy in a cluster (12gb/20node=0.6GB RAM In each executor).</p> <p>Now, Available RAM in each executor will be (6GB - 0.6GB = 5.4GB) RAM which will be reserved for other users' jobs and programs.</p> <p>So, 1TB = 1024 GB / 12GB = (Whole data will get processed in around 85 batches).</p> <p>Note :- Actual values may differ in comparison with real-time scenarios.</p>"},{"location":"spark/bp/#case-1-hardware-6-nodes-and-each-node-have-16-cores-64-gb-ram","title":"Case 1 Hardware \u2014 6 Nodes and each node have 16 cores, 64 GB RAM","text":"<p>We start with how to choose the number of cores:</p> <p>Number of cores = Concurrent tasks an executor can run</p> <p>So we might think, more concurrent tasks for each executor will give better performance. But research shows that any application with more than 5 concurrent tasks, would lead to a bad show. So the optimal value is 5.</p> <p>This number comes from the ability of an executor to run parallel tasks and not from how many cores a system has. So the number 5 stays the same even if we have double (32) cores in the CPU</p> <p></p> <p>Number of executors: Coming to the next step, with 5 as cores per executor, and 15 as total available cores in one node (CPU) \u2014 we come to 3 executors per node which is 15/5. We need to calculate the number of executors on each node and then get the total number for the job.</p> <p>So with 6 nodes and 3 executors per node \u2014 we get a total of 18 executors. Out of 18, we need 1 executor (java process) for Application  Master in YARN. So the final number is 17 executors. This 17 is the number we give to spark using \u2013num-executors while running from the spark-submit shell command.</p> <p>Memory for each executor:</p> <p>From the above step, we have 3 executors per node. And available RAM on each node is 63 GB</p> <p>So memory for each executor in each node is 63/3 = 21GB.</p> <p>However small overhead memory is also needed to determine the full memory request to YARN for each executor.</p> <p>The formula for that overhead is max(384, .07 * spark.executor.memory) , Calculating that overhead: .07 * 21 (Here 21 is calculated as above  63/3) = 1.47</p> <p>Since 1.47 GB &gt; 384 MB, the overhead is 1.47</p> <p>Take the above from each 21 above =&gt; 21\u20131.47 ~ 19 GB , So executor memory \u2014 19 GB</p> <p>Final numbers \u2014 Executors \u2014 17, Cores 5, Executor Memory \u2014 19 GB</p>"},{"location":"spark/bp/#case-2-hardware-6-nodes-and-each-node-have-32-cores-64-gb","title":"Case 2 Hardware \u2014 6 Nodes and Each node have 32 Cores, 64 GB","text":"<p>Number of cores of 5 is the same for good concurrency as explained above.</p> <p>Number of executors for each node = 32/5 ~ 6</p> <p>So total executors = 6 * 6 Nodes = 36. Then the final number is 36\u20131(for AM) = 35</p> <p>Executor memory: 6 executors for each node. 63/6 ~ 10. Overhead is .07 * 10 = 700 MB. So rounding to 1GB as overhead, we get 10\u20131 = 9 GB</p> <p>Final numbers \u2014 Executors \u2014 35, Cores 5, Executor Memory \u2014 9 GB</p>"},{"location":"spark/bp/#case-3-when-more-memory-is-not-required-for-the-executors","title":"Case 3 \u2014 When more memory is not required for the executors","text":"<p>The above scenarios start with accepting the number of cores as fixed and moving to the number of executors and memory.</p> <p>Now for the first case, if we think we do not need 19 GB, and just 10 GB is sufficient based on the data size and computations involved, then following are the numbers:</p> <p>Cores: 5</p> <p>Number of executors for each node = 3. Still, 15/5 as calculated above.</p> <p>At this stage, this would lead to 21 GB, and then 19 as per our first calculation. But since we thought 10 is ok (assume little overhead), then we cannot switch the number of executors per node to 6 (like 63/10). Because with 6 executors per node and 5 cores it comes down to 30 cores per node when we only have 16 cores. So we also need to change the number of cores for each executor.</p> <p>So calculating again, The magic number 5 comes to 3 (any number less than or equal to 5). So with 3 cores, and 15 available cores \u2014 we get 5 executors per node, 29 executors ( which is (5*6 -1)) and memory is 63/5 ~ 12.</p> <p>The overhead is 12*.07=.84. So executor memory is 12\u20131 GB = 11 GB</p> <p>Final Numbers are 29 executors, 3 cores, executor memory is 11 GB</p>"},{"location":"spark/bp/#driver-failure","title":"Driver Failure","text":"<p>The driver program runs the main() function of the application and creates a SparkContext. If the driver node fails, the entire application will be terminated, as it's the driver program that declares transformations and actions on data and submits such requests to the cluster.</p> <p>Impact:</p> <ol> <li>The driver node is a single point of failure for a Spark application.</li> <li>If the driver program fails due to an exception in user code, the entire Spark application is terminated, and all executors are released.</li> </ol> <p>Handling Driver Failure:</p> <ol> <li>Driver failure is usually fatal, causing the termination of the application.</li> <li>It's crucial to handle exceptions in your driver program to prevent such failures.</li> <li>Also, monitor the health of the machine hosting the driver program to prevent failures due to machine errors.</li> <li>In some cluster managers like Kubernetes, Spark supports mode like spark.driver.supervise to supervise and restart the driver on failure.</li> </ol>"},{"location":"spark/bp/#executor-failure","title":"Executor Failure","text":"<p>Executors in Spark are responsible for executing the tasks. When an executor fails, the tasks that were running will fail.</p> <p>Impact:</p> <ol> <li>Executors can fail for various reasons, such as machine errors or OOM errors in the user's application.</li> <li>If an executor fails, the tasks that were running on it are lost.</li> <li>The failure of an executor doesn't cause the failure of the Spark application, unless all executors fail.</li> </ol> <p>Handling Executor Failure:</p> <ol> <li>If an executor fails, Spark can reschedule the failed tasks on other executors.</li> <li>There is a certain threshold for task failures. If the same task fails more than 4 times (default), the application will be terminated.</li> <li>Make sure to tune the resources allocated for each executor, as an executor might fail due to insufficient resources.</li> <li>For resilience, you can also opt to replicate the data across different executor nodes.</li> </ol>"},{"location":"spark/bp/#failure-due-to-out-of-memory-in-spark","title":"Failure Due to Out Of Memory in Spark","text":"<p>Spark Driver OOM Scenarios:</p> <ol> <li> <p>Large Collect Operations: If the data collected from executors using actions such as collect() or take() is too large to fit into the driver's memory, an OutOfMemoryError will occur.     Solution: Be cautious with actions that pull large volumes of data into the driver program. Use actions like take(n), first(), collect() carefully, and only when the returned data is manageable by the driver.</p> </li> <li> <p>Large Broadcast Variables: If a broadcast variable is larger than the amount of free memory on the driver node, this will also cause an OOM error.    Solution: Avoid broadcasting large variables. If possible, consider broadcasting a common subset of the data, or use Spark's built-in broadcast join if joining with a large DataFrame.</p> </li> <li> <p>Improper Driver Memory Configuration: If spark.driver.memory is set to a high value, it can cause the driver to request more memory than what is available, leading to an OOM error.    Solution: Set the spark.driver.memory config based on your application's need and ensure it doesn't exceed the physical memory limits.</p> </li> </ol>"},{"location":"spark/bp/#spark-executor-oom-scenarios","title":"Spark Executor OOM Scenarios","text":"<ol> <li> <p>Large Task Results: If the result of a single task is larger than the amount of free memory on the executor node, an OutOfMemoryError will occur.    Solution: Avoid generating large task results. This is often due to a large map operation. Consider using reduceByKey or aggregateByKey instead of groupByKey when transforming data.</p> </li> <li> <p>Large RDD or DataFrame operations: Certain operations on RDDs or DataFrames, like join, groupByKey, reduceByKey, can cause data to be shuffled around, leading to a large amount of data being held in memory at once, potentially causing an OOM error.    Solution: Be cautious with operations that require shuffling large amounts of data. Use operations that reduce the volume of shuffled data, such as reduceByKey and aggregateByKey, instead of groupByKey.</p> </li> <li> <p>Persistent RDDs/DataFrames: If you're persisting many RDDs/DataFrames in memory and there isn't enough memory to store them, this will also cause an OOM error.    Solution: Unpersist unnecessary RDDs and DataFrames as soon as they are no longer needed. Tune the spark.memory.storageFraction to increase the amount of memory reserved for cached RDDs/DataFrames.</p> </li> <li> <p>Improper Executor Memory Configuration: Similar to the driver, if spark.executor.memory is set to a high value, it can cause the executor to request more memory than what is available, leading to an OOM error.    Solution: Set the spark.executor.memory config based on your application's need and ensure it doesn't exceed the physical memory limits of the executor nodes.</p> </li> </ol>"},{"location":"spark/bp/#code-level-optimization","title":"Code Level Optimization","text":"<ol> <li> <p>Use DataFrames/Datasets instead of RDDs: DataFrames and Datasets have optimized execution plans, leading to faster and more memory-efficient operations than RDDs. They also have more intuitive APIs for many operations.</p> </li> <li> <p>Leverage Broadcasting: If you're performing an operation like a join between a large DataFrame and a small DataFrame, consider broadcasting the smaller DataFrame. Broadcasting sends the smaller DataFrame to all worker nodes, so they have a local copy and don't need to fetch the data across the network.</p> </li> <li> <p>Avoid Shuffling: Operations like groupByKey cause shuffling, where data is transferred across the network, which can be slow. Operations like reduceByKey or aggregateByKey reduce the amount of data that needs to be shuffled, and can be faster.</p> </li> <li> <p>Avoid Collecting Large Data: Be careful with operations like collect() that bring a large amount of data into the driver program, which could cause an out of memory error.</p> </li> <li> <p>Repartitioning and Coalescing: Depending on your use case, you might want to increase or decrease the number of partitions. If you have too many small partitions, use coalesce to combine them. If you have too few large partitions, use repartition to split them.</p> </li> <li> <p>Persist/Cache Wisely: Persist or cache the DataFrames or RDDs that you'll reuse. However, keep in mind that these operations consume memory, so use them judiciously.</p> </li> </ol>"},{"location":"spark/bp/#resource-configuration-optimization","title":"Resource Configuration Optimization","text":"<ol> <li> <p>Tune Memory Parameters: Make sure to set spark.driver.memory, spark.executor.memory, spark.memory.fraction, and spark.memory.storageFraction based on the memory requirements of your application and the capacity of your hardware.</p> </li> <li> <p>Control Parallelism: Use spark.default.parallelism and spark.sql.shuffle.partitions to control the number of tasks during operations like join, reduceByKey, etc. Too many tasks can cause a lot of overhead, but too few tasks might not fully utilize your cluster.</p> </li> <li> <p>Dynamic Allocation: If your cluster manager supports it, use dynamic resource allocation, which allows Spark to dynamically adjust the resources your application occupies based on the workload. This means that if your application has stages that require lots of resources, they can be allocated dynamically.</p> <pre><code>spark.dynamicAllocation.enabled true \nspark.dynamicAllocation.initialExecutors 2 \nspark.dynamicAllocation.minExecutors 1 \nspark.dynamicAllocation.maxExecutors 20\nspark.dynamicAllocation.schedulerBacklogTimeout 1m \nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 2m \nspark.dynamicAllocation.executorIdleTimeout 2min\nspark.dynamicAllocation.enabled is set to true to enable dynamic allocation.\nspark.dynamicAllocation.initialExecutors is set to 2 to specify that initially, two executors will be allocated.\nspark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors control the minimum and maximum number of executors, respectively.\nspark.dynamicAllocation.schedulerBacklogTimeout and spark.dynamicAllocation.sustainedSchedulerBacklogTimeout control how long a backlog of tasks Spark will tolerate before adding more executors.\nspark.dynamicAllocation.executorIdleTimeout controls how long an executor can be idle before Spark removes it.\n</code></pre> </li> </ol>"},{"location":"spark/bp/#resource-configuration-optimization_1","title":"Resource Configuration Optimization","text":"<ol> <li> <p>Tune Garbage Collection: Spark uses the JVM, so the garbage collector can significantly affect performance. You can use spark.executor.extraJavaOptions to pass options to the JVM to tune the garbage collection.</p> </li> <li> <p>Use Appropriate Data Structures: Parquet and Avro are both columnar data formats that are great for analytical queries and schema evolution. If your data processing patterns match these, consider using these formats.</p> </li> </ol>"},{"location":"spark/cacheandpersist/","title":"Caching & Persist","text":"<p>The methods persist() and cache() in Apache Spark are used to save the RDD, DataFrame, or Dataset in memory for faster access during computation. They are effectively ways to optimize the execution of your Spark jobs, especially when you have repeated transformations on the same data. However, they differ in how they handle the storage:</p>"},{"location":"spark/cacheandpersist/#cache","title":"cache()","text":"<ul> <li>What is Caching in Spark? Caching is an optimization technique in Spark that allows you to store intermediate results in memory. This prevents Spark from re-calculating the same data repeatedly when it is used multiple times in subsequent transformations.</li> <li>Where is Cached Data Stored? When data is cached, it is stored within the Storage Memory Pool of a Spark Executor. An executor's memory is divided into three parts: User Memory, Spark Memory, and Reserved Memory. Spark Memory, in turn, contains two pools: Storage Memory Pool and Execution Memory Pool. Cached data specifically resides in the Storage Memory Pool. If the Storage Memory Pool fills up, Spark might evict (remove) data that is not frequently used (using an LRU - Least Recently Used - fashion) or spill it to disk.</li> <li> <p>Why Do We Need Caching? Spark operates with lazy evaluation, meaning transformations are not executed until an action is called. When an action is triggered, Spark builds a Directed Acyclic Graph (DAG) to determine the lineage of the data. If a DataFrame (DF) is used multiple times, Spark will re-calculate it from the beginning each time it's referenced, because DataFrames are immutable and executors' memories are short-lived</p> </li> <li> <p>How Caching Helps: By calling .cache() on df, its intermediate result is stored in the Storage Memory Pool. Now, whenever df is needed again, Spark directly retrieves it from memory instead of re-calculating it. This significantly reduces computation time and improves efficiency</p> </li> <li> <p>When Not to Cache? You should avoid caching data when the DataFrame is very small or when its re-calculation time is negligible. Caching consumes memory, and if the benefits of caching don't outweigh the memory consumption, it's better to avoid it.</p> </li> <li> <p>Limitations of Caching:</p> </li> <li>Memory Fit: If the cached data's partitions are larger than the available Storage Memory Pool, the excess partitions will not be stored in memory and will either be re-calculated on the fly or spilled to disk if using a storage level that supports it. Spark does not store partial partitions; a partition is stored entirely or not at all.</li> <li> <p>Partition Loss: If a cached partition is lost (e.g., due to an executor crash), Spark will re-calculate it using the DAG lineage</p> </li> <li> <p>How to Uncache Data() To remove data from the cache, you can use the .unpersist() method.</p> </li> </ul> <p>When you call df.cache(), it internally calls df.persist() with a default storage level of MEMORY_AND_DISK.</p> <p>persist() offers more flexibility because it allows you to specify the desired storage level as an argument</p>"},{"location":"spark/cacheandpersist/#persiststoragelevel","title":"persist(storageLevel)","text":"<p>Storage levels define where data is stored (memory, disk, or both) and how it is stored (serialized or deserialized, and with replication). These levels provide fine-grained control over how cached data is managed, balancing performance, fault tolerance, and memory usage. To use StorageLevel with persist(), you need to import it: from pyspark import StorageLevel Here are the different storage levels explained: \u2022 MEMORY_ONLY:     \u25e6 Stores data only in RAM (deserialized form).     \u25e6 If memory is insufficient, partitions will be re-calculated when needed.     \u25e6 Advantage: Fastest processing because data is in memory and readily accessible.     \u25e6 Disadvantage: High memory utilization, potentially limiting other operations.     \u25e6 Use case: For small to medium-sized datasets that fit entirely in memory and where re-calculation overhead is high. \u2022 MEMORY_AND_DISK:     \u25e6 Default for cache().     \u25e6 Attempts to store data in RAM first (deserialized form).     \u25e6 If RAM is full, excess partitions are spilled to disk (serialized form).     \u25e6 Advantage: Provides a good balance of speed and resilience; data is less likely to be re-calculated.     \u25e6 Disadvantage: Disk access is slower than memory. Data read from disk (serialized) requires CPU to deserialize it, leading to higher CPU utilization.     \u25e6 Use case: For larger datasets that might not fully fit in memory but where performance is still critical. \u2022 MEMORY_ONLY_SER:     \u25e6 Stores data in RAM only, but in a serialized form.     \u25e6 Serialization saves memory space, allowing more data to be stored in the same amount of RAM (e.g., 5GB uncompressed might become 8GB serialized).     \u25e6 Disadvantage: Data needs to be deserialized by the CPU when accessed, leading to higher CPU utilization and slightly slower access compared to MEMORY_ONLY.     \u25e6 Limitation: This serialization specifically works for Java and Scala objects, and not for Python objects (though Python has its own pickling mechanisms, the _SER storage levels in Spark are typically for JVM objects).     \u25e6 Use case: When memory is a major constraint and you can tolerate increased CPU usage for deserialization. \u2022 MEMORY_AND_DISK_SER:     \u25e6 Stores data first in RAM (serialized), then spills to disk (serialized) if memory is full.     \u25e6 Combines memory saving of serialization with resilience of disk storage.     \u25e6 Disadvantage: High CPU usage due to deserialization for both memory and disk reads.     \u25e6 Use case: For very large datasets where memory constraints are severe and some CPU overhead for deserialization is acceptable. \u2022 DISK_ONLY:     \u25e6 Stores data only on disk (serialized form).     \u25e6 Slowest storage level due to reliance on disk I/O.     \u25e6 Advantage: Good for extremely large datasets that don't fit in memory, or for fault tolerance where data needs to be durable across executor restarts.     \u25e6 Disadvantage: Significantly slower than memory-based storage levels.     \u25e6 Use case: When performance is less critical than fault tolerance or when datasets are too large for memory. \u2022 Replicated Storage Levels (e.g., MEMORY_ONLY_2, DISK_ONLY_2):     \u25e6 These levels store two copies (2x replicated) of each partition across different nodes.     \u25e6 For example, MEMORY_ONLY_2 stores two copies in RAM on different executors.     \u25e6 Advantage: Provides fault tolerance. If one executor or worker node goes down, the data can still be accessed from its replica, avoiding re-calculation from the DAG.     \u25e6 Disadvantage: Doubles memory/disk consumption compared to non-replicated versions.     \u25e6 Use case: For highly critical data that is complex to calculate and must be readily available even if a node fails. Generally, cache() (which is MEMORY_AND_DISK) is preferred unless specific fault tolerance is required</p>"},{"location":"spark/cacheandpersist/#choosing-the-right-storage-level","title":"Choosing the Right Storage Level","text":"<p>The choice of storage level depends on your specific needs: \u2022 Start with MEMORY_ONLY: If your data fits in memory and transformations are simple, this is the fastest. \u2022 Move to MEMORY_AND_DISK: If data is larger and might not fit entirely in memory, or if re-calculation is expensive. This is the most commonly used for general caching. \u2022 Consider _SER options: Only if memory is a severe bottleneck and you can tolerate increased CPU usage for serialization/deserialization. Note that in Python, direct serialization using _SER levels like in Java/Scala might not provide the same benefits. \u2022 Use _2 options: Only for critical, complex data where high fault tolerance is a must and you can afford the doubled storage cost. 4. Code Examples and Spark UI Demonstration The video demonstrates the effect of cache() and persist() using the Spark UI (specifically the Storage tab at localhost:4040/storage)</p>"},{"location":"spark/context/","title":"SparkSession vs SparkContext","text":"<p>Both Spark Session and Spark Context serve as the entry point into a Spark cluster, similar to how a <code>main</code> method serves as the entry point for code execution in languages like C++ or Java. This means that to run any Spark code, you first need to establish one of these entry points.</p>"},{"location":"spark/context/#spark-session","title":"Spark Session","text":"<p>The Spark Session is the unified entry point introduced in Spark 2.0. It is now the primary way to interact with Spark.</p> <ul> <li> <p>Unified Entry Point: Prior to Spark 2.0 (specifically up to Spark 1.4), if you wanted to work with different Spark functionalities like SQL, Hive, or Streaming, you had to create separate contexts for each (e.g., <code>SQLContext</code>, <code>HiveContext</code>, <code>StreamingContext</code>). The Spark Session encapsulates all these different contexts, providing a single object to access them. This simplifies development as you only need to create a Spark Session to gain access to all necessary functionalities.</p> </li> <li> <p>Resource Allocation: When you create a Spark Session, you can pass configurations for resources needed, such as the amount of memory or the number of executors. The Spark Session takes these values and communicates with the Resource Manager (like YARN or Mesos) to request and allocate the necessary driver memory and executors. Once these resources are secured, the Spark Session facilitates the execution of your Spark jobs within that allocated environment.</p> </li> <li> <p>Default in Databricks: If you've been using Databricks notebooks, you might have implicitly been using a Spark Session without realizing it. Databricks typically provides a default <code>spark</code> object, which is an instance of <code>SparkSession</code>, allowing you to directly write code like <code>spark.read.format(...)</code>. This is why the local setup is demonstrated in the source, as the default session is not automatically provided outside environments like Databricks.</p> </li> <li> <p>Configuration: You can configure properties like <code>spark.driver.memory</code> by using the <code>.config()</code> method when building the Spark Session.</p> </li> </ul>"},{"location":"spark/context/#spark-context","title":"Spark Context","text":"<p>The Spark Context (<code>SparkContext</code>) was the original entry point for Spark applications before Spark 2.0.</p> <ul> <li> <p>Historical Role: In earlier versions of Spark (up to Spark 1.4), <code>SparkContext</code> was the primary entry point for general Spark operations. However, for specific functionalities like SQL, you needed additional context objects like <code>SQLContext</code>.</p> </li> <li> <p>Current Role (RDD Level): While Spark Session has become the dominant entry point, <code>SparkContext</code> is still relevant for RDD (Resilient Distributed Dataset) level operations. If you need to perform low-level transformations directly on RDDs (e.g., <code>flatMap</code>, <code>map</code>), you would typically use the Spark Context. An example provided is writing a word count program using RDDs, where <code>SparkContext</code> comes into use.</p> </li> <li> <p>Access via Spark Session: With the advent of Spark Session, you do not create a <code>SparkContext</code> directly as a separate entry point anymore. Instead, you can access the <code>SparkContext</code> object through the <code>SparkSession</code> instance. This means that the <code>SparkContext</code> is now encapsulated within the <code>SparkSession</code>.</p> </li> </ul>"},{"location":"spark/context/#code-example","title":"Code Example","text":"<p>Here\u2019s an example demonstrating how to create a Spark Session and then obtain a Spark Context from it, based on the provided transcript:</p> <pre><code># First, import the SparkSession class from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create a SparkSession builder\n# The .builder() method is used to construct a SparkSession instance\nspark_builder = SparkSession.builder\n\n# Configure the SparkSession\n# .master(\"local\"): Specifies that Spark should run in local mode.\n#                  This means Spark will use your local machine's resources.\n# .appName(\"Testing\"): Sets the name of your Spark application.\n#                     This can be any descriptive name for your project.\n# .config(\"spark.driver.memory\", \"12g\"): An optional configuration to request specific resources,\n#                                       here requesting 12GB for the driver memory.\nspark_session_config = spark_builder.master(\"local\").appName(\"Testing\").config(\"spark.driver.memory\", \"12g\")\n\n# Get or Create the SparkSession\n# .getOrCreate(): This is a crucial method. If a SparkSession with the specified\n#                name and configuration already exists, it will retrieve it.\n#                Otherwise, it will create a new one.\nspark = spark_session_config.getOrCreate()\n\n# Print the SparkSession object to verify it's created\nprint(spark)\n\n# Access the SparkContext from the created SparkSession\n# The SparkContext is encapsulated within the SparkSession object.\n# This is how you get the sc (SparkContext) object in modern Spark applications.\nsc = spark.sparkContext\n\n# Print the SparkContext object\nprint(sc)\n\n# Example of using SparkSession to read data (common operation)\n# This is what you often do in Databricks without explicitly creating a session.\n# employee_df = spark.read.format(\"csv\").load(\"path/to/employee_data.csv\")\n</code></pre>"},{"location":"spark/dynamic/","title":"Dynamic resource allocation & partition pruning","text":"<p>Dynamic Resource Allocation is a crucial concept in Spark for optimizing resource utilization at the cluster level. It addresses challenges faced with static resource allocation, especially in multi-user or varied workload environments.</p>"},{"location":"spark/dynamic/#what-is-dynamic-resource-allocation-dra","title":"What is Dynamic Resource Allocation (DRA)?","text":"<p>Dynamic Resource Allocation refers to the ability of a Spark application to dynamically increase or decrease the number of executors it uses based on the workload. This means resources are added when tasks are queued or existing ones need more processing power, and released when they become idle. - Purpose: To make processes run faster and ensure other processes also get sufficient resources. - Context: DRA is a cluster-level optimization technique, contrasting with code-level optimizations like join tuning, caching, partitioning, and coalescing.</p>"},{"location":"spark/dynamic/#static-vs-dynamic-resource-allocation-techniques","title":"Static vs. Dynamic Resource Allocation Techniques","text":"<p>Spark offers two primary resource allocation techniques: - Static Resource Allocation:     - In this approach, the application requests a fixed amount of memory (e.g., 100 GB) at the start, and it retains that allocated memory for the entire duration of the application's run, regardless of whether the memory is actively used or idle.     - Default behavior in Spark if DRA is not explicitly enabled.     - Drawback: Can lead to resource wastage if the application doesn't constantly utilize all allocated resources, making them unavailable for other jobs. This is particularly problematic for smaller jobs that have to wait for large, static jobs to complete, even if the larger job is underutilizing its resources.     - Example Scenario: A heavy job requests 980 GB for executors and 20 GB for the driver (total 1 TB) on a 1 TB cluster. This saturates the entire cluster, leaving no resources for other users' jobs, even small ones. The resource manager, often operating on a First-In, First-Out (FIFO) policy, will make subsequent jobs wait until resources are freed. - Dynamic Resource Allocation:     - Dynamically adjusts resources by acquiring more executors when needed and releasing them when they become idle.     - Advantage: Optimizes cluster utilization by making resources available to other applications when they are not actively being used by a particular job.</p>"},{"location":"spark/dynamic/#how-dynamic-resource-allocation-works-in-detail","title":"How Dynamic Resource Allocation Works in Detail","text":"<ul> <li>Initial Resource Request and Configuration A Spark application typically requests resources using the spark-submit command. For DRA to work, specific configurations must be set. Example Spark Submit Command Parameters (Conceptual): spark-submit \\   --conf spark.dynamicAllocation.enabled=true \\   --conf spark.dynamicAllocation.minExecutors=5 \\   --conf spark.dynamicAllocation.maxExecutors=49 \\   --executor-memory 20G \\   --executor-cores 4 \\   --driver-memory 20G \\   --conf spark.dynamicAllocation.executorIdleTimeout=45s \\   --conf spark.dynamicAllocation.schedulerBacklogTimeout=2s \\   --conf spark.shuffle.service.enabled=true \\   --conf spark.dynamicAllocation.shuffleTracking.enabled=true \\   your_application.py</li> <li>spark.dynamicAllocation.enabled=true: This is the primary configuration to enable Dynamic Resource Allocation. By default, it is set to false (disabled).</li> <li>spark.dynamicAllocation.minExecutors: Specifies the minimum number of executors that the application will always retain, even if idle. This helps prevent the process from failing if it releases too many resources and cannot re-acquire them when needed later. For example, setting it to 20 ensures at least 20 executors are always available.</li> <li>spark.dynamicAllocation.maxExecutors: Specifies the maximum number of executors that the application can acquire. This acts as an upper limit for resource consumption.</li> <li>--executor-memory 20G: Sets the memory for each executor.</li> <li>--executor-cores 4: Sets the number of CPU cores for each executor, determining how many parallel tasks each executor can run.</li> <li>--driver-memory 20G: Sets the memory for the driver program.</li> </ul> <p>Resource Release Mechanism When an application no longer needs all its allocated resources, Spark can release them: \u2022 Trigger for Release: Resources are released when executors become idle, meaning they are not actively performing tasks. \u2022 Idle Timeout: By default, an executor will be released if it remains idle for 60 seconds. This can be configured using spark.dynamicAllocation.executorIdleTimeout.     \u25e6 Example Configuration: Setting spark.dynamicAllocation.executorIdleTimeout=45s will release idle executors after 45 seconds. \u2022 Spark's Role: Spark internally manages the release of resources. The resource manager (e.g., YARN, Mesos) does not directly request Spark applications to release resources. \u2022 Benefit: For instance, if a process initially needed 1000 GB for a join but then transitions to a filter operation requiring only 500 GB, the extra 500 GB can be released, making them available for other processes. 3.3. Resource Acquisition (Demand) Mechanism When an application's workload increases and it needs more resources, Spark will request them: \u2022 Trigger for Demand: The driver program identifies the need for more memory/executors (e.g., for a large join operation). \u2022 Backlog Timeout: Spark starts requesting more resources if it experiences a backlog of pending tasks for a certain duration. The default is 1 second. This can be configured using spark.dynamicAllocation.schedulerBacklogTimeout.     \u25e6 Example Configuration: Setting spark.dynamicAllocation.schedulerBacklogTimeout=2s will cause Spark to request resources after 2 seconds of a task backlog. \u2022 Incremental Acquisition (2-fold): Spark does not request all needed resources at once. Instead, it requests them in a 2-fold manner (doubling the requested executors each time):     \u25e6 Initially, it might request 1 additional executor.     \u25e6 If that's not enough, it will request 2 more.     \u25e6 Then 4, then 8, then 16, and so on, until the required resources are met or maxExecutors is reached.</p> <p>Challenges and Solutions in Dynamic Resource Allocation While DRA offers significant benefits, it also presents challenges that need to be addressed: \u2022 Challenge 1: Process Failure Due to Resource Unavailability     \u25e6 Problem: If an application releases too many resources, and other processes quickly acquire them, the original application might not be able to get back the needed resources on demand, potentially leading to process failure, especially for memory-intensive operations like joins.     \u25e6 Solution: Configure spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors.         \u25aa By setting a minExecutors value (e.g., 20 out of 49), you ensure that a baseline number of executors is always available to your application, preventing it from completely running out of resources and failing even if it releases others.         \u25aa maxExecutors prevents over-allocation and resource monopolization. \u2022 Challenge 2: Loss of Cached Data or Shuffle Output     \u25e6 Problem: When executors are released, any cached data or shuffle output written to the local disk of those executors would be lost. This would necessitate re-calculation of that data, negating the performance benefits of DRA.     \u25e6 Solution: External Shuffle Service and Shuffle Tracking.         \u25aa External Shuffle Service (spark.shuffle.service.enabled=true): This service runs independently on worker nodes and is responsible for storing shuffle data. This ensures that even if an executor or worker node is released, the shuffle data persists and can be retrieved later by other executors or if the original executor is re-acquired.         \u25aa Shuffle Tracking (spark.dynamicAllocation.shuffleTracking.enabled=true): This configuration ensures that shuffle output data is not deleted even when an executor is released. It works in conjunction with the external shuffle service to prevent the need for re-calculation of shuffled data throughout the Spark application's lifetime.</p> <p>When to Avoid Dynamic Resource Allocation While DRA is generally beneficial, there are specific scenarios where it should be avoided: - Critical Production Processes: For critical production jobs where any delay or potential failure due to resource fluctuation is unacceptable, it is advisable to use Static Memory Allocation. This ensures predictable resource availability and minimizes risk. - Non-Critical Processes / Development: For processes that have some bandwidth for resource fluctuations, or for development and testing environments, Dynamic Resource Allocation is highly recommended</p>"},{"location":"spark/dynamic/#dynamic-partition-pruning-dpp","title":"Dynamic Partition Pruning (DPP)","text":"<p>Dynamic Partition Pruning (DPP) is an optimization technique in Apache Spark that enhances query performance, especially when dealing with partitioned data and join operations. Here's a detailed explanation, including the underlying concepts and scenarios described in the sources: 1. Understanding Partition Pruning (The Foundation) Before diving into Dynamic Partition Pruning, it's essential to understand standard Partition Pruning. \u2022 What it is: Partition pruning is a mechanism where Spark avoids reading unnecessary data partitions based on filter conditions. It \"prunes\" or removes data that is not relevant to the query. \u2022 How it works: When data is partitioned on a specific column (e.g., sales_date), and a query applies a filter directly on that partitioning column, Spark can identify and read only the partitions that contain the relevant data. This significantly reduces the amount of data scanned. \u2022 Example (Scenario 1):     \u25e6 Imagine a large sales_data dataset partitioned by sales_date. Each date has its own partition.     \u25e6 If you run a query like SELECT * FROM sales_data WHERE sales_date = '2019-04-19', Spark, with partition pruning enabled, will only read the data for April 19, 2019.     \u25e6 Observed in Spark UI: In this example, if there are 123 total partitions, Spark will only read 1 file. The Spark UI's \"SQL\" tab details will show \"Partition Filter\" applied, indicating that the date has been cast and used for filtering. 2. The Issue: When Standard Partition Pruning Fails Standard partition pruning works efficiently when the filter condition is directly applied to the partitioning column of the table being queried. However, a common scenario where it fails is when: \u2022 You have two dataframes (or tables), say df1 and df2. \u2022 df1 is a partitioned table (e.g., partitioned by date). \u2022 You need to join df1 and df2. \u2022 The filter condition originates from df2 (the non-partitioned table or the table that is not the primary partitioned table being filtered). In such a case, because the filter is applied on df2 and not directly on df1's partitioning column, Spark's optimizer (without DPP) won't know which partitions of df1 to prune at the planning stage. \u2022 Example (Scenario 2 - DPP Disabled):     \u25e6 Data: df1 is sales_data (partitioned by sales_date) and df2 is a date_dimension table (containing date and week_of_year columns).     \u25e6 Goal: Find sales data for a specific week, e.g., week = 16.     \u25e6 Query Concept: df1 is joined with df2 (e.g., on date columns), and then df2 is filtered for week_of_year = 16.     \u25e6 Configuration for Demonstration: To observe this issue, Spark's default behavior needs to be overridden by explicitly disabling Dynamic Partition Pruning (spark.sql.set('spark.sql.optimizer.dynamicPartitionPruning.enabled', 'false')) and also potentially disabling broadcast joins.     \u25e6 Observed in Spark UI: When this query is run with DPP disabled, Spark will scan all 123 files of the sales_data table, even though only a few dates (and thus partitions) might be relevant for week 16. The \"Partition Filter\" section in the Spark UI for df1 will show no effective pruning related to the join condition. This leads to performance degradation. 3. Dynamic Partition Pruning (DPP): The Solution Dynamic Partition Pruning (DPP) addresses the performance issue described above by enabling Spark to prune partitions at runtime. \u2022 What it is: DPP is an optimization technique that allows Spark to update filter conditions dynamically at runtime. \u2022 How it works (Scenario 3 - DPP Enabled):     1. Filter Small Table: Spark first filters the smaller table (df2, e.g., date_dimension for week = 16) to identify the relevant values (e.g., specific dates that fall in week 16).     2. Broadcast: The relevant values (e.g., the list of specific dates) from the filtered smaller table are broadcasted to all executor nodes. Broadcasting makes this small dataset available on all nodes where the larger table is processed.     3. Subquery Injection: At runtime, Spark then uses these broadcasted values to create a subquery (similar to an IN clause) for the partitioned table (df1). For instance, it essentially transforms the query to look like: SELECT * FROM big_table WHERE sales_date IN (SELECT dates FROM small_table).     4. Dynamic Pruning: This subquery allows Spark to dynamically identify and prune the irrelevant partitions of the large table (df1), reading only the necessary ones. \u2022 Example (Scenario 3 - DPP Enabled):     \u25e6 Using the same sales_data (df1) and date_dimension (df2) tables, and the join with week = 16 filter.     \u25e6 Configuration: DPP is enabled (by default in Spark 3.0+ or explicitly enabled) and the broadcast mechanism is active.     \u25e6 Observed in Spark UI: When run with DPP enabled, Spark will only read a small subset of files (e.g., 3 files out of 123 total partitions), as only those files contain the dates relevant to week 16. The \"Partition Filter\" in the Spark UI will clearly show a \"Dynamic Pruning Expression\" applied to sales_date. You will also see \"Broadcast Exchange\" in the execution plan, indicating that the smaller table was broadcasted. 4. Key Conditions for Dynamic Partition Pruning For Dynamic Partition Pruning to work effectively, two primary conditions must be met: 1. Partitioned Data: The data in the larger table (df1 in our example) must be partitioned on the column used in the join and filter condition (e.g., sales_date). If the data is not partitioned, DPP cannot apply. 2. Broadcastable Second Table: The second table (df2), which provides the filter condition, must be broadcastable. This means it should be small enough to fit into memory and be efficiently broadcasted to all executor nodes. If it's too large, it won't be broadcasted, and DPP might not engage. You can also adjust Spark's broadcast threshold value if needed. 5. Spark Version and Default Behavior Dynamic Partition Pruning is a feature introduced in Spark 3.0 and newer versions. In these versions, it is enabled by default. This means that for modern Spark applications, this optimization will often kick in automatically if the conditions are met. Code Examples and Demonstrations The video transcript describes the process using various Spark configurations and demonstrations on the Spark UI, but it does not provide explicit code snippets in the text. \u2022 The speaker explains setting Spark configurations like spark.sql.set('spark.sql.optimizer.dynamicPartitionPruning.enabled', 'false') to disable DPP for demonstration purposes. \u2022 It also describes data being read (e.g., spark.read.parquet(...)) and joined, with filter conditions applied (e.g., df.filter(\"sales_date == '2019-04-19'\") for basic pruning or join conditions like df1.join(df2, ..., df2.filter(\"week == 16\")) for DPP scenarios). \u2022 The core of the explanation lies in how these operations manifest in the Spark UI's \"SQL\" and \"Jobs\" tabs, showing the \"Number of files read,\" \"Partition Filter,\" and \"Broadcast Exchange\" details. \u2022 The graphical representation of the subquery injection (WHERE sales_date IN (SELECT dates FROM small_table)) visually explains how the filter from the smaller table is passed to the larger one</p>"},{"location":"spark/overview/","title":"Overview & Architecture","text":""},{"location":"spark/overview/#problems-with-mapreduce","title":"Problems with MapReduce","text":"<ul> <li>Batch Processing: Hadoop and MapReduce are designed for batch processing, making them unfit for real-time or near real-time processing such as streaming data.</li> <li>Complexity: Hadoop has a steep learning curve and its setup, configuration, and maintenance can be complex and time-consuming.</li> <li>Data Movement: Hadoop's architecture can lead to inefficiencies and network congestion when dealing with smaller data sets.</li> <li>Fault Tolerance: While Hadoop has data replication for fault tolerance, it can lead to inefficient storage use and doesn't cover application-level failures.</li> <li>No Support for Interactive Processing: MapReduce doesn't support interactive processing, making it unsuitable for tasks needing back-and-forth communication.</li> <li>Not Optimal for Small Files: Hadoop is less effective with many small files, as it's designed to handle large data files.</li> </ul> Parameter Hadoop Spark Performance Hadoop is slower than Spark because it writes data back to disk and reads again from disk to memory. Spark is faster because it performs all computation in memory. Batch/Streaming Built for batch data processing. Built for batch as well as streaming data processing. Ease of Use Difficult to write code; Hive was built to make it easier. Easy to write and debug code, has interactive shell to develop and test. Spark provides high-level and low-level APIs. Security Kerberos authentication and ACL authorization. Doesn\u2019t have built-in security features. Fault Tolerance Uses block storage and replication factor to handle failure. Uses DAG (Directed Acyclic Graph) to provide fault tolerance. <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/overview/#apache-spark","title":"Apache spark","text":"<p>Apache spark is unified computing engine and set of libraries for parallel data processing on computed cluster.</p> <p>Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. </p> <p>Spark is known for its speed, ease of use, and versatility in handling multiple types of data workloads, including batch processing, real-time data streaming, machine learning, and interactive queries.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/overview/#features-of-spark","title":"Features of Spark","text":"<ul> <li>Speed: Compared to Hadoop MapReduce, Spark can execute large-scale data processing up to 100 times faster. This speed is achieved by leveraging controlled partitioning.</li> <li>Powerful Caching: Spark's user-friendly programming layer delivers impressive caching and disk persistence capabilities.</li> <li>Deployment: Spark offers versatile deployment options, including through Mesos, Hadoop via YARN, or its own cluster manager.</li> <li>Real-Time Processing: Thanks to in-memory computation, Spark facilitates real-time computation and offers low latency.</li> <li>Polyglot: Spark provides high-level APIs in several languages - Java, Scala, Python, and R, allowing code to be written in any of these. It also offers a shell in Scala and Python.</li> <li>Scalability: Spark's design is inherently scalable, capable of handling and processing large amounts of data by distributing tasks across multiple nodes in a cluster.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/overview/#spark-ecosystem","title":"Spark Ecosystem","text":"<ul> <li>Spark Core Engine: The foundation of the entire Spark ecosystem, the Spark Core, handles essential functions such as task scheduling, monitoring, and basic I/O operations. It also provides the core programming abstraction, Resilient Distributed Datasets (RDDs).</li> <li>Cluster Management: Spark's versatility allows for cluster management by multiple tools, including Hadoop YARN, Apache Mesos, or Spark's built-in standalone cluster manager. This flexibility accommodates varying requirements and operational contexts.</li> <li> <p>Library: The Spark ecosystem includes a rich set of libraries</p> <ul> <li> <p>Spark SQL allows SQL-like queries on RDDs or data from external sources, integrating relational processing with Spark's functional programming API.</p> </li> <li> <p>Spark MLlib is a library for machine learning that provides various algorithms and utilities.</p> </li> <li> <p>Spark GraphX allows for the construction and computation on graphs, facilitating advanced data visualization and graph computation.</p> </li> <li> <p>Spark Streaming makes it easy to build scalable, high-throughput, fault-tolerant streaming applications that can handle live data streams alongside batch processing.</p> </li> </ul> </li> <li> <p>Polyglot Programming: Spark supports programming in multiple languages including Python, Java, Scala, and R. This broad language support makes Spark accessible to a wide range of developers and data scientists.</p> </li> <li>Storage Flexibility: Spark can interface with a variety of storage systems, including HDFS, Amazon S3, local filesystems, and more. It also supports interfacing with both SQL and NoSQL databases, providing broad flexibility for various data storage and processing needs.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/overview/#spark-architecture","title":"Spark Architecture","text":"<ul> <li> <p>Driver Program: The driver program is the heart of a Spark application. It runs the main() function of an application and is the place where the SparkContext is created. SparkContext is responsible for coordinating and monitoring the execution of tasks. The driver program defines datasets and applies operations (transformations &amp; actions) on them.</p> </li> <li> <p>SparkContext: The SparkContext is the main entry point for Spark functionality. It represents the connection to a Spark cluster and can be used to create RDDs, accumulators, and broadcast variables on that cluster.</p> </li> <li> <p>Cluster Manager: SparkContext connects to the cluster manager, which is responsible for the allocation of resources (CPU, memory, etc.) in the cluster. The cluster manager can be Spark's standalone manager, Hadoop YARN, Mesos, or Kubernetes.</p> </li> <li> <p>Executors: Executors are worker nodes' processes in charge of running individual tasks in a given Spark job. They run concurrently across different nodes. Executors have two roles. Firstly, they run tasks that the driver sends. Secondly, they provide in-memory storage for RDDs.</p> </li> <li> <p>Tasks: Tasks are the smallest unit of work in Spark. They are transformations applied to partitions. Each task works on a separate partition and is executed in a separate thread in executors.</p> </li> <li> <p>RDD: Resilient Distributed Datasets (RDD) are the fundamental data structures of Spark. They are an immutable distributed collection of objects, which can be processed in parallel. RDDs can be stored in memory between queries without the necessity for serialization.</p> </li> <li> <p>DAG (Directed Acyclic Graph): Spark represents a series of transformations on data as a DAG, which helps it optimize the execution plan. DAG enables pipelining of operations and provides a clear plan for task scheduling. Spark Architecture &amp; Its components</p> </li> <li> <p>DAG Scheduler: The Directed Acyclic Graph (DAG) Scheduler is responsible for dividing operator graphs into stages and sending tasks to the Task Scheduler. It translates the data transformations from the logical plan (which represents a sequence of transformations) into a physical execution plan. It optimizes the plan by rearranging and combining operations where possible, groups them into stages, and then submits the stages to the Task Scheduler.</p> </li> <li> <p>Task Scheduler: The Task Scheduler launches tasks via cluster manager. Tasks are the smallest unit of work in Spark, sent by the DAG Scheduler to the Task Scheduler. The Task Scheduler then launches the tasks on executor JVMs. Tasks for each stage are launched in as many parallel operations as there are partitions for the dataset.</p> </li> <li> <p>Master: The Master is the base of a Spark Standalone cluster (specific to Spark's standalone mode, not applicable if Spark is running on YARN or Mesos). It's the central point and entry point of the Spark cluster. It is responsible for managing and distributing tasks to the workers. The Master communicates with each of the workers periodically to check if it is still alive and if it has completed tasks.</p> </li> <li> <p>Worker: The Worker is a node in the Spark Standalone cluster (specific to Spark's standalone mode). It receives tasks from the Master and executes them. Each worker has multiple executor JVMs running on it. It communicates with the Master and Executors to facilitate task execution.The worker is responsible for managing resources and providing an execution environment for the executor JVMs.</p> </li> <li> <p>What happens behind the scenes</p> <ol> <li>You launch the application.</li> <li>Spark creates a SparkContext in the Driver.</li> <li>Spark connects to the Cluster Manager (e.g., YARN, standalone, k8s).</li> <li>Cluster Manager allocates Workers and starts Executors.</li> <li>RDD transformations are converted into a DAG (Directed Acyclic Graph.</li> <li>Spark creates Stages, breaks them into Tasks (based on partitions).</li> <li>Tasks are shipped to Executors.</li> <li>Executors run the tasks and return results back to the Driver.</li> <li>Final results (e.g., word count) are written to HDFS.</li> </ol> </li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/overview/#spark-standalone","title":"Spark Standalone","text":"<p>Spark Standalone mode is a built-in cluster manager in Apache Spark that enables you to set up a dedicated Spark cluster without needing external resource managers like Hadoop YARN or Kubernetes.</p> <p>It is easy to deploy, suitable for development and testing, and supports distributed data processing across multiple nodes.</p> <p>Advantages</p> <ul> <li>Easy to set up and manage</li> <li>No need for external resource managers</li> <li>Built-in web UI for monitoring</li> <li>Supports HA (High Availability) with ZooKeeper</li> </ul> <p>Limitations</p> <ul> <li>Less fault-tolerant than YARN or Kubernetes</li> <li>Limited support for resource isolation and fairness</li> <li>Not recommended for large-scale production</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/overview/#spark-with-yarn","title":"Spark with YARN","text":"<p>Apache Spark on YARN means running Spark applications on top of Hadoop YARN (Yet Another Resource Negotiator) - the resource manager in Hadoop ecosystems. This setup allows Spark to share cluster resources with other big data tools (like Hive, HBase, MapReduce) in a multi-tenant environment.</p> <p>YARN handles resource management, job scheduling, and container allocation, while Spark focuses on data processing.</p> <ul> <li> <p>Resource Manager: It controls the allocation of system resources on all applications. A Scheduler and an Application Master are included. Applications receive resources from the Scheduler.</p> </li> <li> <p>Node Manager: Each job or application needs one or more containers, and the Node Manager monitors these containers and their usage. Node Manager consists of an Application Master and Container. The Node Manager monitors the containers and resource usage, and this is reported to the Resource Manager.</p> </li> <li> <p>Application Master: The ApplicationMaster (AM) is an instance of a framework-specific library and serves as the orchestrating process for an individual application in a distributed environment.</p> </li> </ul> <p>Advantages</p> <ul> <li>Leverages existing Hadoop cluster (no separate setup)</li> <li>Resource sharing across Hadoop ecosystem</li> <li>Supports HDFS, Hive, HBase integration natively</li> <li>Production-grade scalability and stability</li> </ul> <p>Considerations</p> <ul> <li>Slight overhead from YARN\u2019s container management</li> <li>Configuration tuning (memory, executor placement) is important</li> <li>YARN needs to be properly secured (Kerberos, ACLs)</li> </ul>"},{"location":"spark/partition/","title":"Partition","text":"<p>Partitioning and bucketing are two crucial optimization techniques in Apache Spark that are decided upon when writing data to ensure efficient data processing, especially for analytical workloads (OLAP) where data is written once and read many times (read many). These techniques help improve performance by reducing the amount of data Spark needs to scan and process for queries .</p>"},{"location":"spark/partition/#partitioning-in-spark","title":"Partitioning in Spark","text":"<p>Partitioning in Spark involves organizing data into separate subdirectories (folders) based on the distinct values of one or more specified columns . Each distinct value of the partitioning column(s) gets its own folder . </p> <p>Example</p> <p>If you partition by address (which contains countries), Spark will create separate folders for India, USA, Japan, etc., each containing only the records corresponding to that country .</p>"},{"location":"spark/partition/#how-it-works-advantages","title":"How it Works &amp; Advantages","text":"<ul> <li>Folder Creation: When you partition data, Spark creates a directory structure where each distinct value of the chosen column becomes a folder name. </li> </ul> <p>Example</p> <p>For instance, if you partition by address, you might see folders like address=India/, address=USA/, address=Japan/, etc., at the specified storage location .</p> <ul> <li> <p>Performance Optimization (Predicate Pushdown/Partition Pruning): This is the primary advantage. If you later query the data with a filter on the partitioned column (e.g., WHERE address = 'USA'), Spark can directly go to the address=USA folder and read only the data within that folder, completely ignoring data in other folders . This significantly reduces the amount of data read from disk, leading to faster query execution, especially for large datasets .</p> </li> <li> <p>Logical Organization: It provides a logical way to organize your data on the file system, making it easier to manage and understand .</p> </li> <li> <p>Persistence: The data written to these partitioned folders is permanently stored (persistent storage) and will not be deleted even if the Spark cluster is shut down .</p> </li> </ul>"},{"location":"spark/partition/#when-partitioning-fails-disadvantages","title":"When Partitioning Fails (Disadvantages)","text":"<ul> <li> <p>High Cardinality: Partitioning becomes inefficient and can even degrade performance if the chosen column has a very high number of distinct values (high cardinality) . If you partition by an ID column where every record has a unique ID, Spark will create a separate folder for each record .</p> </li> <li> <p>Small File Problem: Creating too many small folders (and thus, small files within them) is detrimental to performance in distributed file systems like HDFS (which Spark typically works with). Managing a large number of tiny files introduces overhead and can make operations slower rather than faster . The source indicates that for 1 billion records, creating 1 billion small partitions is not feasible .</p> </li> </ul>"},{"location":"spark/partition/#examples-for-partitioning","title":"Examples for Partitioning","text":"<ul> <li> <p>Partitioning by a Single Column: To partition your DataFrame (df) by the address column.    Explanation: This code will save the DataFrame df to the specified path. Within that path, Spark will create subdirectories for each unique value in the Address column (e.g., address=India, address=USA, address=Japan) . Each subdirectory will contain the data rows where Address matches that value.</p> </li> <li> <p>Partitioning by Multiple Columns: You can partition by multiple columns, which creates a nested folder structure. The order of columns in partitionBy matters . If you want folders for each country, and then inside each country folder, folders for Gender, you should specify Address first, then Gender.</p> </li> </ul> <p>Explaination</p> <p>The first example will create folders like address=India/gender=Female/, address=USA/gender=Male/, etc. . The second example will create gender=Female/address=India/, gender=Male/address=USA/, etc. . The order determines the hierarchy of folders on the file system and impacts how pruning works .</p>"},{"location":"spark/partition/#bucketing-in-spark","title":"Bucketing in Spark","text":"<p>Bucketing is a technique used to divide data into a fixed number of 'buckets' based on the hash value of one or more specified columns . Unlike partitioning, which creates a variable number of directories based on distinct values, bucketing creates a pre-defined, fixed number of files (buckets) . Bucketing is particularly useful when partitioning is not suitable due to high cardinality .</p>"},{"location":"spark/partition/#how-it-works-advantages_1","title":"How it Works &amp; Advantages","text":"<ul> <li> <p>Fixed Number of Buckets: You specify the exact number of buckets you want to create . Spark then hashes the values of the chosen bucketing column(s) and assigns rows to specific buckets based on these hash values . For instance, if you define 3 buckets, all records will be distributed among these 3 buckets .</p> </li> <li> <p>Requires saveAsTable(): Bucketing cannot be used with a direct save() operation to a file path. It requires the data to be saved as a table using saveAsTable(), as it interacts with Spark's metastore to manage bucket metadata .</p> </li> <li> <p>Optimizing Joins (Shuffle Avoidance): This is a key advantage of bucketing . If two tables are bucketed on the same columns and with the same number of buckets, when you join these two tables on those bucketing columns, Spark can perform a bucket-aware join . This means it can directly join corresponding buckets from each table without needing to shuffle the entire dataset across the network, which is an expensive operation . This significantly speeds up join operations .</p> </li> <li> <p>Faster Lookups (Bucket Pruning): Similar to partition pruning, bucketing can also enable bucket pruning . If you query data with a filter on the bucketing column, Spark can use the hash function to determine exactly which bucket(s) contain the relevant data . This means it only needs to scan a subset of files within the table, rather than the entire table, making lookups faster .</p> </li> <li> <p>Overcoming High Cardinality Issues: When partitioning fails due to high cardinality, bucketing provides a solution by distributing records with many unique values into a fixed, manageable number of buckets .</p> </li> </ul>"},{"location":"spark/partition/#important-considerations-for-bucketing","title":"Important Considerations for Bucketing","text":"<ul> <li> <p>repartition() before bucketBy(): A common issue with bucketing is that if you have many tasks running (e.g., 200 tasks), and you specify, say, 5 buckets, Spark might create 5 buckets per task, resulting in 200 * 5 = 1000 unwanted buckets . To avoid this, it's recommended to repartition your DataFrame to the desired number of buckets before applying bucketBy() . This ensures that only the specified number of buckets are created .</p> </li> <li> <p>Same Bucketing Strategy for Joins: For join optimization, both tables involved in the join must be bucketed on the same column(s) and with the same number of buckets .</p> </li> </ul> <p>Code Example for Bucketing</p> <p>Explaination</p> <p>repartition(3): This step ensures that the data is first divided into 3 partitions before bucketing, preventing the creation of an excessive number of bucket files if your Spark tasks are more than your desired buckets .</p> <p>bucketBy(3, \"ID\"): This specifies that the data should be divided into 3 buckets, using the ID column for hashing .</p> <p>saveAsTable(\"bucket_by_id_table\"): This command saves the DataFrame as a managed table in Spark's metastore, which is required for bucketing .</p> <p>Verification: After execution, you would typically use dbutils.fs.ls() to see the bucket files generated within the table's directory, often named like part-00000_0_0_C000.snappy.parquet where the _0_0 part might indicate bucket ID.</p>"},{"location":"spark/repart/","title":"Repartition vs Coalesce","text":""},{"location":"spark/repart/#understanding-the-core-problem-in-spark","title":"Understanding the Core Problem in Spark","text":"<p>The need for repartition and coalesce arises from issues faced when processing large datasets in Spark, particularly concerning data partitioning.</p> <p></p> <ul> <li> <p>Uneven Partition Size (Data Skew): When a DataFrame is created, it's often divided into multiple partitions. Sometimes, these partitions can be of uneven sizes (e.g., 10MB, 20MB, 40MB, 100MB).</p> </li> <li> <p>Impact on Processing:  Processing smaller partitions (e.g., 10MB) takes less time than larger ones (e.g., 100MB).</p> </li> </ul> <p>This leads to idle Spark executors: while one executor is busy with a large partition, others might finish their tasks quickly and then wait for the large partition to complete. This causes time delays and underutilization of allocated resources (e.g., RAM)</p> <ul> <li> <p>Origin of Data Skew: This situation often arises after operations like join transformations. For instance, if a join operation is performed on a product column, and one product is a \"best-selling product\" with a high number of records, all those records might get grouped into a single partition, making it very large. This phenomenon is called data skew.</p> </li> <li> <p>Observed Behavior: Users often see messages like \"199 out of 200 partitions processed,\" where the last remaining partition takes a significantly longer time to complete due to its large size.</p> </li> <li> <p>Solution: To deal with these scenarios and optimize performance, Spark provides repartition and coalesce methods</p> </li> </ul>"},{"location":"spark/repart/#repartition","title":"repartition","text":"<ul> <li> <p>Shuffling Data: repartition shuffles the entire dataset across the cluster. This means data from existing partitions can be moved to new partitions.</p> </li> <li> <p>Even Distribution: The primary goal of repartition is to evenly distribute data across the specified number of new partitions. For example, if you have 200MB of data across five uneven partitions and repartition it into five, it will aim for 40MB per partition.</p> </li> <li> <p>Flexibility in Partition Count: repartition can increase or decrease the number of partitions. If you initially have 5 partitions but need 10, repartition is the only choice.It can be used when you want to increase the number of partitions to allow for more concurrent tasks and increase parallelism when the cluster has more resources.</p> </li> <li> <p>Cost: Due to the shuffling operation, repartition is generally more expensive and involves more I/O operations compared to coalesce.</p> </li> <li> <p>Pros and Cons of Repartition: Evenly distributed data. More I/O (Input/Output) because of shuffling. More expensive.</p> </li> </ul> <p>In certain scenarios, you may want to partition based on a specific key to optimize your job. For example, if you frequently filter by a certain key, you might want all records with the same key to be on the same partition to minimize data shuffling. In such cases, you can use repartition() with a column name.</p>"},{"location":"spark/repart/#coalesce","title":"coalesce","text":"<ul> <li> <p>Merging Partitions: coalesce merges existing partitions to reduce the total number of partitions.</p> </li> <li> <p>No Shuffling (or Minimal): Crucially, coalesce tries to avoid full data shuffling. It achieves this by moving data from some partitions to existing ones, effectively merging them locally on the same executor if possible. This makes it less expensive than repartition.</p> </li> <li> <p>Uneven Distribution: Because it avoids full shuffling, coalesce does not guarantee an even distribution of data across the new partitions. It might result in an uneven distribution, especially if the original partitions were already skewed.</p> </li> <li> <p>Limited Partition Count: coalesce can only decrease the number of partitions. It cannot be used to increase the number of partitions. If you need more partitions, you must use repartition.</p> </li> <li> <p>Pros and Cons of Coalesce: No shuffling (or minimal shuffling). Not expensive (cost-effective). Uneven data distribution.</p> </li> </ul> <p>However, it can lead to  data skew if you have fewer partitions than before, because it combines existing partitions to reduce the total number.</p>"},{"location":"spark/repart/#when-to-choose-which","title":"When to Choose Which?","text":"<p>The choice between repartition and coalesce is use-case dependent</p> <ul> <li>Choose repartition when</li> </ul> <p>You need to evenly distribute data across partitions, which is crucial for balanced workload across executors.</p> <p>You need to increase the number of partitions (e.g., if you have too few partitions or want to process data in smaller chunks in parallel).</p> <p>You are okay with the overhead of a full shuffle, as the benefit of even distribution outweighs the cost.</p> <p>Dealing with severe data skew is a primary concern.</p> <ul> <li>Choose coalesce when</li> </ul> <p>You primarily need to decrease the number of partitions (e.g., after filter operations drastically reduce data, or before writing to a single file).</p> <p>You want to minimize shuffling and I/O costs.</p> <p>You can tolerate slightly uneven data distribution across partitions, or the data skew is minimal and won't significantly impact performance.</p> <p>You want to save processing time and cost by avoiding a full shuffle</p>"},{"location":"spark/repart/#why-doesnt-coalesce-explicitly-show-the-partitioning-scheme","title":"Why doesn't <code>.coalesce()</code> explicitly show the partitioning scheme?","text":"<p><code>.coalesce</code> doesn't show the partitioning scheme e.g. <code>RoundRobinPartitioning</code> because:  - The operation only minimizes data movement by merging into fewer partitions, it doesn't do any shuffling. - Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by <code>.coalesce</code></p>"},{"location":"spark/salting/","title":"Salting","text":"<p>Imagine we have two DataFrames, df1 and df2, that we want to join on a column named 'id'. Assume that the 'id' column is highly skewed.</p> <p>Firstly, without any handling of skewness, the join might look something like this:</p> <pre><code>result = df1.join(df2, on='id', how='inner')\n</code></pre> <p>Now, let's implement salting to handle the skewness: </p> <pre><code>import pyspark.sql.functions as F\n# Define the number of keys you'll use for salting \nnum_salting_keys = 100\n# Add a new column to df1 for salting \ndf1 = df1.withColumn('salted_key', (F.rand()*num_salting_keys).cast('int'))\n# Explode df2 into multiple rows by creating new rows with salted keys \ndf2_exploded = df2.crossJoin(F.spark.range(num_salting_keys).withColumnRenamed('id', 'salted_key'))\n# Now perform the join using both 'id' and 'salted_key' \nresult = df1.join(df2_exploded, on=['id', 'salted_key'], how='inner')\n# If you wish, you can drop the 'salted_key' column after the join\nresult = result.drop('salted_key')\n</code></pre> <p>In this code, we've added a \"salt\" to the 'id' column in df1 and created new rows in df2 for each salt value. We then perform the join operation on both 'id' and the salted key. This helps to distribute the computation for the skewed keys more evenly across the cluster.</p> <ol> <li>What is Data Skew Problem? \u2022 Definition: Data skew occurs when one partition contains a disproportionately large amount of data compared to other partitions. This happens when certain keys have a significantly higher frequency (e.g., ID_1 appears much more often than ID_2 or ID_3).     \u25e6 Example: A \"best-selling product\" might account for 90% of your data, leading to a large amount of data associated with that specific product ID. \u2022 Consequences: When performing operations like joins, if a skewed key lands on a single executor, it can slow down that executor significantly or even cause it to fail (out of memory errors or long-running tasks), leading to performance impact for the entire Spark job. \u2022 Challenging Scenario: The problem becomes particularly acute when you need to perform a join on skewed data, and the other table is too large to be broadcast (e.g., &gt;10 MB, &gt;100 MB, or &gt;200 MB), preventing the use of broadcast joins to avoid shuffles. In such situations, salting becomes a vital solution.</li> <li>Ways to Remove Skewness (Brief Mention) Before discussing salting, the video briefly mentions other methods: \u2022 Repartitioning: While useful for general data distribution, it's not effective for data skew because skewed data will still end up in a single partition after repartitioning if the key remains the same. \u2022 Adaptive Query Execution (AQE): Offers three types of optimizations that can help with skew. \u2022 Salting: This is the primary focus of the video and is used when other methods are insufficient.</li> <li>Understanding Joins and Skew Impact (Example) The video uses an example of two tables to illustrate the join process and how skew impacts it: \u2022 Table 1 (Skewed): Contains IDs with ID_1 being dominant (e.g., 5 records), ID_2 with fewer records (e.g., 2 records), and ID_3 with even fewer (e.g., 1 record). \u2022 Table 2 (Not Skewed): Contains IDs with a balanced distribution (e.g., 2 records for ID_1, 2 for ID_2, 1 for ID_3). \u2022 Inner Join Calculation: If an inner join is performed on these tables (e.g., table1.id == table2.id), the expected number of records would be 15 (10 from ID_1, 4 from ID_2, 1 from ID_3). The problem arises during computation, where the ID_1 data, being so large, would concentrate on one executor, slowing down the process.</li> <li>Salting: The Core Solution Salting involves modifying the join key to distribute the heavily skewed data across multiple partitions during the join operation. 4.1 Initial (Incorrect) Attempt at Salting The video first demonstrates a common, but incorrect, way to implement salting, often used to test understanding in interviews: \u2022 Left-Hand Side (Skewed Table): Create a salted_key by concatenating the original ID with a random number (e.g., from 1 to 10). This breaks down the ID_1 records into distinct keys like 1_7, 1_5, etc., distributing them into multiple partitions.     \u25e6 Example: If ID_1 appears 5 times, it might become 1_7, 1_5, 1_2, 1_9, 1_4. \u2022 Right-Hand Side (Non-Skewed Table): Similarly, create a salted_key by concatenating the original ID with a random number (1 to 10).     \u25e6 Example: ID_1 records might become 1_5, 1_8. \u2022 Problem: When these two salted tables are joined, the random numbers on both sides will likely not match, leading to a significant loss of expected records. In the example, instead of 15 records, only 3 records might be returned. This demonstrates that simply adding random numbers to both sides is not the correct approach for salting. 4.2 Correct Salting Implementation The correct implementation of salting addresses the problem of lost records while distributing the skewed data: \u2022 Left-Hand Side (Skewed Table - table1):     \u25e6 Add a new salted_key column by concatenating the original ID with a random number between 1 and N (e.g., 1 to 10). This ensures that the skewed ID is spread across N potential new keys/partitions.     \u25e6 Code Example (Conceptual for table1):     \u25e6 Note: The rand() function in Spark generates a random value between 0.0 and 1.0. rand() * 10 + 1 scales this to a range of 1.0 to 11.0, and casting to IntegerType results in integers from 1 to 10. \u2022 Right-Hand Side (Non-Skewed Table - table2):     \u25e6 This is the crucial step. For each record in the non-skewed table, you need to duplicate it N times (where N is the same range used on the left side, e.g., 10 times).     \u25e6 For each of these duplicated records, create a salted_key by concatenating the original ID with a sequential number from 1 to N.     \u25e6 Example: If ID_1 has two records in table2, each of those two records will be duplicated 10 times. So, the first ID_1 record will generate 1_1, 1_2, ..., 1_10. The second ID_1 record will also generate 1_1, 1_2, ..., 1_10.     \u25e6 Why this works: By replicating the right-hand side keys with all possible sequential suffixes (1 to 10), any salted_key generated on the left (e.g., 1_5) will find a matching key on the right-hand side, thus preserving all original join results while distributing the load.     \u25e6 Code Example (Conceptual for table2): \u2022 Joining Salted Tables: After both tables have their salted_key columns generated correctly, perform the join on these new keys.     \u25e6 Code Example (Conceptual for join):</li> <li>Implementation and Performance Demonstration The video provides a practical demonstration using Spark code: \u2022 Spark Session Configuration:     \u25e6 spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\"): This is set to 3 to match the 3 distinct IDs in the example, helping to visually demonstrate partitioning.     \u25e6 spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\"): Adaptive Query Execution (AQE) is disabled to prevent Spark from automatically changing join strategies or optimizing, allowing for a clear demonstration of the skew problem and salting solution. \u2022 Data Generation (Simulated for Demonstration):     \u25e6 table1 (skewed): Generated with 100,000 records, heavily skewed towards ID_1, then ID_2, then ID_3.     \u25e6 table2 (non-skewed, but needs replication): Generated with a smaller number of records (e.g., 5 records), intended to be replicated later. \u2022 Performance Before Salting:     \u25e6 A normal inner join is performed between the original table1 (skewed) and table2.     \u25e6 Spark UI Observation: When examining the Spark UI's Stages tab for the join, it clearly shows data skew.         \u25aa Some tasks finish very quickly (e.g., 0.2 seconds), while one task for the skewed data takes significantly longer (e.g., 6 seconds), even with small data. This wide disparity in task durations indicates the performance impact of data skew. \u2022 Performance After Salting:     \u25e6 The salting logic (adding random suffixes to table1, and replicating/adding sequential suffixes to table2) is applied.     \u25e6 The join is then performed on the newly created salted_key columns.     \u25e6 Spark UI Observation: After salting, the Spark UI shows a much more balanced distribution of task durations.         \u25aa The average task time (e.g., 9 seconds) and maximum task time (e.g., 12 seconds) are very close, indicating that the data skew problem has been resolved, and the load is evenly distributed across executors. The difference between min and max duration is drastically reduced (from 5.8 seconds to 3 seconds in the example)</li> </ol>"},{"location":"spark/sparkjoin/","title":"Sparkjoin","text":"<p>Spark joins are considered an expensive operation primarily because they involve data shuffling. Shuffling refers to the movement of data across the network.</p>"},{"location":"spark/sparkjoin/#join-types","title":"Join types","text":"<ul> <li> <p>Inner Join -     An inner join returns only the records that have matching keys in both the left and right tables.      If a key exists in one table but not the other, those records are excluded from the result</p> </li> <li> <p>Left Join (Left Outer Join) -      A left join returns all records from the left table and the matching records from the right table.      If there is no match for a record in the left table, the columns from the right table will have null values</p> </li> <li> <p>Right Join (Right Outer Join) -      A right join is symmetrical to a left join. It returns all records from the right table and the matchingrecords     from the left table.      If there is no match for a record in the right table, the columns from the left table will have null values</p> </li> <li> <p>Full Outer Join -      A full outer join returns all records when there is a match in either the left or the right table. This means       it includes: Records matching in both tables (like an inner join). Records unique to the left table (like a         left join, with nulls from the right).Records unique to the right table (like a right join, with nulls from the     left).</p> </li> <li> <p>Left Semi Join -      A left semi join returns only the records from the left table that have a match in the right table. It does not     include any columns from the right table.It acts like an inner join but only projects columns from the left         DataFrame.The presenter calls this a \"fictional join\" because the same result can be achieved by                   performing an inner join and then using dot select to pick only the required columns from the left table</p> </li> <li> <p>Left Anti Join -       left anti join returns only the records from the left table that do not have a match in the right table.       It is the inverse of a left semi join. Like left semi join, it only projects columns from the left DataFrame.</p> </li> <li> <p>Cross Join -      A cross join computes the Cartesian product of two tables. Every record from the left table is combined with       every record from the right table.This is an extremely expensive join and should generally be avoided unless       absolutely necessary.If Table A has M records and Table B has N records, a cross join will produce M * N           records.For example, if both tables have 1 million (10^6) records, the cross join will result in 1 trillion         (10^12) records, consuming massive computational resources.PySpark typically requires an explicit                   crossJoin() method or \"cross\" join type in join() to prevent accidental execution due to its high cost</p> </li> </ul>"},{"location":"spark/sparkjoin/#why-spark-joins-are-expensive-data-shuffling-wide-dependency-transformation","title":"Why Spark Joins are Expensive: Data Shuffling (Wide Dependency Transformation)","text":"<p>Necessity of Joins: Joins typically involve at least two DataFrames (e.g., <code>df1</code> and <code>df2</code>). The goal is to combine data based on a common key (e.g., <code>ID</code>).</p> <p>Initial Data Distribution: Data in Spark is broken down into partitions. For example, two 500MB DataFrames, <code>df1</code> and <code>df2</code>, would each be divided into four partitions if the default HDFS block size is 128MB (500MB / 128MB \u2248 4 partitions). These partitions are distributed across executors on worker nodes in a Spark cluster.</p> <p>The Problem: When performing a join, the matching keys from different DataFrames might reside on different executors or even different worker nodes. For instance, <code>ID=1</code> from <code>df1</code> (say, on Partition P1 of Executor 1) and <code>ID=1</code> from <code>df2</code> (say, on Partition P3 of Executor 2) cannot be joined directly because they are on separate machines.</p> <p>The Solution: Shuffling - To perform the join, all data with the same join key must be brought to the same executor (and partition). This process of moving data across the network is called shuffling.</p> <p>How Shuffling Works (with Example):</p> <p>Default Partitions: By default, Spark creates 200 partitions when a join (or any wide dependency transformation like <code>groupBy</code>) is performed.</p> <p>Key Hashing: Spark uses the join key (e.g., <code>ID</code>) and the total number of partitions (200) to determine which new partition the data should go to. It uses a hash function or modulo operation: <code>Key % Number_of_Partitions</code>.</p> <p>Example:     If <code>ID = 1</code>, then <code>1 % 200 = 1</code>. Both <code>df1</code> and <code>df2</code> records with <code>ID=1</code> will be sent to Partition 1.     If <code>ID = 7</code>, then <code>7 % 200 = 7</code>. Records with <code>ID=7</code> will go to Partition 7.     If <code>ID = 201</code>, then <code>201 % 200 = 1</code>. Records with <code>ID=201</code> will also go to Partition 1.     This ensures that all records with the same join key land on the same partition on the same executor, making the join possible.</p> <p>Impact of Shuffling:  Data movement across the network is slow and resource-intensive. Excessive shuffling can choke the cluster and degrade performance significantly, especially with large datasets. This is why optimizing joins is crucial.</p>"},{"location":"spark/sparkjoin/#spark-join-strategies","title":"Spark Join Strategies","text":"<p>Spark provides five main join strategies:</p> <ol> <li>Shuffle Sort Merge Join</li> <li>Shuffle Hash Join</li> <li>Broadcast Hash Join</li> <li>Cartesian Join</li> <li>Broadcast Nested Loop Join</li> </ol>"},{"location":"spark/sparkjoin/#shuffle-sort-merge-join-default-strategy","title":"Shuffle Sort Merge Join (Default Strategy)","text":"<p>Process:     This is Spark's default join strategy.</p> <ol> <li> <p>Shuffle: As explained above, data from both DataFrames is first shuffled so that records with the same join key land on the same partition of the same executor.</p> </li> <li> <p>Sort: After shuffling, the data within each partition is sorted by the join key.</p> </li> <li> <p>Merge: Once sorted, the two sorted streams of data (from <code>df1</code> and <code>df2</code> within the same partition) are merged by iterating through them and matching keys, similar to a merge sort algorithm.</p> </li> </ol> <p>Time Complexity: The sorting step has a time complexity of O(N log N), where N is the number of records in the partition.</p> <p>Resource Usage: This strategy primarily utilizes CPU for sorting.</p>"},{"location":"spark/sparkjoin/#shuffle-hash-join","title":"Shuffle Hash Join","text":"<p>Process:</p> <ol> <li> <p>Shuffle: Similar to Shuffle Sort Merge Join, data is first shuffled to bring matching keys to the same partition.</p> </li> <li> <p>Hash Table Creation: On each executor, Spark identifies the smaller of the two DataFrames for that partition. It then builds an in-memory hash table from the smaller DataFrame using the join key. The hash table stores the unique keys and their corresponding data.</p> </li> <li> <p>Probe: The larger DataFrame's records are then probed against this in-memory hash table. For each record in the larger DataFrame, its join key is hashed, and a lookup is performed in the hash table to find matching records.</p> </li> </ol> <p>Time Complexity: Once the hash table is built, lookup operations (probing) have an average time complexity of O(1).</p> <p>Resource Usage &amp; Trade-offs:    Building an in-memory hash table requires significant memory (RAM) on the executor. If the \"smaller\" table has too many unique keys or is too large, it can lead to an Out-Of-Memory (OOM) error.</p> <p>Compared to Shuffle Sort Merge Join (which uses CPU for sorting), Shuffle Hash Join uses less CPU for the actual join part once the hash  table is built.</p> <p>Comparison to Shuffle Sort Merge Join:</p> <p>Hash Joins can be faster (O(1) lookup vs. O(N log N) sort) if the hash table fits in memory.</p> <p>Sort Merge Join is generally more robust because it doesn't have the same risk of OOM errors as Hash Join. This is why Spark defaults to Shuffle Sort Merge Join.</p>"},{"location":"spark/sparkjoin/#broadcast-hash-join","title":"Broadcast Hash Join","text":"<p>Broadcast Hash Join is a join strategy in Spark that combines three terms: Broadcast, Hash, and Join. \u2022 Broadcast: Similar to how TV broadcasts spread information from one source to many receivers, in Spark, it means sending a small table from one location (the Driver) to all other locations (Executors). \u2022 Hash: This refers to hashing on the smaller table, as discussed in previous videos about hash joins. \u2022 Join: As learned previously, this means combining columns from two tables or DataFrames. Why it's needed: We need Broadcast Hash Join to avoid data shuffling (shuffle). In traditional join strategies like Shuffle Hash Join or Shuffle Sort Merge Join, data needs to be moved across different executors to bring matching keys together. This shuffling can lead to significant network overhead and slow down the cluster, making it \"choke\". Broadcast Hash Join optimizes this process by eliminating the need for shuffling for one of the tables</p> <p>How Broadcast Hash Join Works Broadcast Hash Join is applied when you have one large table and one small table. \u2022 The Driver is responsible for sending the small table to all Executors. \u2022 Each Executor then has a complete copy of the small table. \u2022 This allows each Executor to perform the join operation locally with its partition of the large table, without needing to exchange data with other Executors (i.e., no shuffling). This makes each executor \"self-sufficient\" for joining. Small vs. Large Tables: \u2022 By default, Spark considers a table smaller than 10 MB as a \"small table\" that can be broadcast. \u2022 Tables greater than 10 MB are generally considered \"large tables\". \u2022 For example, if you have a 1 GB table and a 5 MB table, the 5 MB table is considered small, and the 1 GB table is large. Process Example: 1. Assume a 5 MB small table and a 1 GB large table, with the large table partitioned across multiple executors. 2. The Driver takes the 5 MB small table. 3. The Driver then sends a copy of this 5 MB table to all Executors. 4. Each Executor now has its portion of the 1 GB large table and a complete copy of the 5 MB small table. 5. Executors perform the join internally, searching for matching keys within their own partitions of the large table and the broadcasted small table. 6. Result: This avoids data shuffling, as data does not need to move between Executors to find matching keys. This optimizes performance. 3. When Broadcast Hash Join Is Not Good (Failure Scenarios) While beneficial, Broadcast Hash Join has limitations: \u2022 Driver Memory: The Driver needs sufficient memory to store the small table before broadcasting it. If the broadcasted file is too large (e.g., trying to broadcast a 1 GB file with a 2 GB Driver), the Driver might run \"out of memory\". \u2022 Network Throughput: Broadcasting a very large file, even if the Driver has enough memory, will still involve sending the entire file over the network to all Executors. This can slow down the network. \u2022 Executor Memory: If the broadcasted table is too large, it can fill up the Executor's memory, leaving insufficient space for join operations, potentially leading to Executor \"out of memory\" errors when data is multiplied during the join. \u2022 Recommendation: It is advised to define the size of your small table based on your cluster's configuration (e.g., if an Executor has 16 GB of RAM, broadcasting a 100 MB file might be fine, but a 2 GB file could cause issues). 4. Changing the Broadcast Threshold Size Spark provides a configuration to change the default 10 MB broadcast threshold. \u2022 Default Threshold: The default automatic broadcast threshold is 10 MB. \u2022 Getting the current threshold: You can retrieve the current threshold using spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"). \u2022 Setting a new threshold: You can set a new threshold value using spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", value_in_bytes).     \u25e6 To disable auto-broadcasting, set the threshold to -1.     \u25e6 To increase the threshold, convert the desired MB value to bytes</p>"},{"location":"spark/sparkjoin/#broadcast-variables","title":"Broadcast Variables","text":"<p>In Spark, broadcast variables are read-only shared variables that are cached on each worker node rather than sent over the network with tasks. They're used to give every node a copy of a large input dataset in an efficient manner. Spark's actions are executed through a set of stages, separated by distributed \"shuffle\" operations. Spark automatically broadcasts the common data needed by tasks within each stage.</p> <pre><code>from pyspark.sql import SparkSession\n# initialize SparkSession \nspark = SparkSession.builder.getOrCreate()\n# broadcast a large read-only lookup table\nlarge_lookup_table = {\"apple\": \"fruit\", \"broccoli\": \"vegetable\", \"chicken\": \"meat\"}\nbroadcasted_table = spark.sparkContext.broadcast(large_lookup_table)\ndef classify(word):\n    # access the value of the broadcast variable  \n    lookup_table = broadcasted_table.value     \n    return lookup_table.get(word, \"unknown\")\ndata = [\"apple\", \"banana\", \"chicken\", \"potato\", \"broccoli\"] \nrdd = spark.sparkContext.parallelize(data)\nclassification = rdd.map(classify) \nprint(classification.collect())\n</code></pre>"},{"location":"spark/sparkjoin/#accumulators","title":"Accumulators","text":"<p>Accumulators are variables that are only \"added\" to through associative and commutative operations and are used to implement counters and sums efficiently. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.</p> <p>A simple use of accumulators is:</p> <pre><code>from pyspark.sql import SparkSession\n# initialize SparkSession \nspark = SparkSession.builder.getOrCreate()\n# create an Accumulator[Int] initialized to 0\naccum = spark.sparkContext.accumulator(0)\nrdd = spark.sparkContext.parallelize([1, 2, 3, 4]) \ndef add_to_accum(x):\n    global accum\n    accum += x rdd.foreach(add_to_accum)\n# get the current value of the accumulator \nprint(accum.value)\n</code></pre>"},{"location":"spark/sparkmemory/","title":"Memory management","text":"<p>When the Spark application is launched, the Spark cluster will start two processes \u2014 Driver and Executor.</p> <p>The driver is a master process responsible for creating the Spark context, submission of Spark jobs, and translation of the whole Spark pipeline into computational units \u2014 tasks. It also coordinates task scheduling and orchestration on each Executor.</p> <p>Driver memory management is not much different from the typical JVM process.</p> <p>The executor is responsible for performing specific computational tasks on the worker nodes and returning the results to the driver, as well as providing storage for RDDs. And its internal memory management is very interesting.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparkmemory/#executor-memory","title":"Executor memory","text":"<p>A Spark executor container has three major components of memory:</p>"},{"location":"spark/sparkmemory/#on-heap-memory","title":"On-Heap Memory","text":"<p>This occupies the largest block and is where most of Spark's operations run .The On-Heap memory is managed by the JVM (Java Virtual Machine). Even though Spark is written in Scala and you might write code in Python using PySpark (which uses a wrapper around Java APIs), the underlying execution still happens on the JVM.</p> <p>The On-Heap memory is further divided into four sections:</p> <ul> <li> <p>Execution Memory:      It is mainly used to store temporary data in the shuffle, join, sort, aggregation, etc. Most likely, if your pipeline runs too long, the problem lies in the lack of space here.</p> <p>Note</p> <p>Execution Memory = usableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction).</p> <p>As Storage Memory, Execution Memory is also equal to 30% of all system memory by default (1 * 0.6 * (1 - 0.5) = 0.3).</p> </li> <li> <p>Storage Memory:      This is where caching (for RDDs or DataFrames) occurs, and it's also used for storing broadcast variables.     Storage Memory is used for caching and broadcasting data. </p> <p>Note</p> <p>Storage Memory = usableMemory * spark.memory.fraction * spark.memory.storageFraction</p> <p>Storage Memory is 30% of all system memory by default (1 * 0.6 * 0.5 = 0.3).</p> </li> <li> <p>User Memory:     Used for storing user objects such as variables, collections (lists, sets, dictionaries) defined in your program, or User Defined Functions (UDFs).     It is mainly used to store data needed for RDD conversion operations, such as lineage. You can store your own data structures there that will be used inside transformations. It's up to you what would be stored in this memory and how. Spark makes completely no accounting on what you do there and whether you respect this boundary or not.</p> <p>Note</p> <p>User Memory = usableMemory * (1 - spark.memory.fraction)</p> <p>It is 1 * (1 - 0.6) = 0.4 or 40% of available memory by default.</p> </li> <li> <p>Reserved Memory:      This is the memory Spark needs for running itself and storing internal objects     The most boring part of the memory. Spark reserves this memory to store internal objects. It guarantees to reserve sufficient memory for the system even for small JVM heaps.</p> <p>Note</p> <p>Reserved Memory is hardcoded and equal to 300 MB (value RESERVED_SYSTEM_MEMORY_BYTES in source code). In the test environment (when spark.testing set) we can modify it with spark.testing.reservedMemory.</p> <p>usableMemory = spark.executor.memory - RESERVED_SYSTEM_MEMORY_BYTES</p> </li> <li> <p>Unified memory:</p> <p>Unified memory refers to the Execution memory and Storage memory combined.</p> <ul> <li>Why it's \"Unified\": It's due to Spark's dynamic memory management strategy.This means if execution memory needs more space, it can use some of the storage memory, and vice-versa. There is a priority given to execution memory because critical operations like joins, shuffles, sorting, and group by happen there. The division between execution and storage is represented as a movable \"slider\".</li> </ul> <p>Evolution of Unified Memory (Pre-Spark 1.6 vs. Post-Spark 1.6):</p> <ul> <li>Before Spark 1.6: The space allocated to execution and storage memory was fixed.     If execution needed more memory but its fixed allocation was full, it could not use available space in storage memory, leading to wasted memory.</li> <li>After Spark 1.6 (&gt;= Spark 1.6): The \"slider\" became movable, allowing dynamic allocation based on needs.</li> </ul> </li> <li> <p>Rules for Slider Movement (Dynamic Allocation):</p> <p>Execution needs more memory, and Storage has vacant space: If storage is not using all its allocated space, execution can simply use that vacant portion of memory.</p> <p>Execution needs more memory, and Storage is occupied: If storage is using its blocks, it will evict some of its blocks (least recently used or LRU algorithm) to make room for execution memory.</p> <p>Storage needs more memory: In this case, because execution has priority, none of the execution blocks will be evicted. Storage must evict its own blocks (based on LRU) to free up space for new cached data</p> </li> </ul>"},{"location":"spark/sparkmemory/#off-heap-memory","title":"Off-Heap Memory","text":"<p>Off-Heap memory is often the least talked about and least used, but it can be very useful in certain situations. - Default State: It is disabled by default (spark.memory.offHeap.enabled is set to zero).</p> <ul> <li>Enabling and Sizing: You can enable it by setting spark.memory.offHeap.enabled to true and specify its size using spark.memory.offHeap.size. A good starting point for its size is 10% to 20% of your executor memory.</li> <li>Structure: Similar to unified memory, off-heap memory also has two parts: execution and storage.</li> <li>Purpose/Use Case: It becomes useful when the on-heap memory is full.<ul> <li>When on-heap memory is full, a garbage collection (GC) cycle occurs, which pauses the program's operation to clean up unwanted objects. These GC pauses can negatively impact program performance.</li> <li>Off-Heap memory is managed by the Operating System, not the JVM. Therefore, it is not subject to the JVM's GC cycles.</li> </ul> </li> <li>Developer Responsibility: Since it's not subject to GC, the Spark developer is responsible for both the allocation and deallocation of memory in the off-heap space. This adds complexity and requires caution to avoid memory leaks.</li> <li>Performance: Off-heap memory is slower than on-heap memory. However, if Spark had to choose between spilling data to disk or using off-heap memory, using off-heap memory would be a better choice because writing to disk is several orders of magnitude slower</li> </ul>"},{"location":"spark/sparkmemory/#overhead-memory","title":"Overhead Memory","text":"<p>Used for internal system-level operations</p> <p>Example</p> <p>Calculation: The overhead memory is defined as the maximum of 384 MB or 10% of the spark.executor.memory. If spark.executor.memory is 10 GB, 10% of it is 1 GB. max(384 MB, 1 GB) = 1 GB. So, the overhead memory would be 1 GB.</p> <p>It's important to note that the spark.executor.memory parameter only allocates for on-heap memory. When Spark requests memory from a cluster manager (like YARN), it adds the executor memory and the overhead memory. If off-heap memory is enabled, it will also add that amount to the request.</p> <p>Example</p> <p>If spark.executor.memory is 10 GB, and overhead is 1 GB (and off-heap is disabled), Spark will request 10 GB + 1 GB = 11 GB from the cluster manager for that container</p>"},{"location":"spark/sparkmemory/#why-out-of-memory-occurs-even-when-spillage-is-possible","title":"Why Out of Memory Occurs Even When Spillage is Possible","text":"<p>Despite the ability to spill data to disk from the Execution Memory Pool, an Out of Memory Exception can still occur, especially during operations like joins or aggregations:</p> <ul> <li> <p>The Problem of Data Skew: If data for a single key (e.g., ID=1) becomes excessively large (e.g., 3GB), exceeding the available Execution Memory Pool (e.g., 2.9GB), it cannot be processed.</p> </li> <li> <p>Impossibility of Partial Spillage: During operations like joins, all data related to a specific key must be present on the same executor for the operation to complete correctly. If a 3GB chunk of data for a single ID has to be processed, and only 2.9GB is available, it's impossible to spill just a portion of that key's data. Spilling half of the 3GB data would mean the join would not yield the correct result for that key. Therefore, if a single partition or a single key's data exceeds the physical memory capacity of the executor's Execution Memory Pool (even with potential spill), an Out of Memory Exception is inevitable.</p> </li> </ul>"},{"location":"spark/sparkmemory/#solutions-to-out-of-memory-exception","title":"Solutions to Out of Memory Exception","text":"<ul> <li>Repartitioning: Redistributing data across more partitions.</li> <li>Salting: A technique to add a \"salt\" to skewed keys to distribute them more evenly during shuffles.</li> <li>Sorting: Pre-sorting data can sometimes help with certain types of joins (e.g., Sort-Merge Join) to reduce memory pressure.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparkmemory/#driver-memory","title":"Driver Memory","text":"<p>The Spark driver has its own dedicated memory. You can configure the driver's memory when starting a PySpark session.</p> <p>Requesting Driver Memory: To request a specific amount of driver memory, you can use the pyspark command with the --driver-memory flag: pyspark --driver-memory 1g</p> <p>This command requests 1 GB of driver memory from your local setup. After starting the session, you can verify the Spark driver memory configuration by navigating to localhost:4040/jobs in your web browser.</p> <p>Types of Driver Memory Within the Spark driver, there are two main types of memory that work together</p> <ul> <li> <p>JVM Heap Memory (spark.driver.memory):This is the primary memory allocated for the driver's Java Virtual Machine (JVM) processes. All JVM-related operations, such as scheduling tasks and handling responses from executors, primarily use this memory.This is what you configure using --driver-memory or spark.driver.memory.</p> </li> <li> <p>Memory Overload (spark.driver.memoryOverhead): This memory is dedicated to non-JVM processes.     It handles objects created by your application that are not part of the JVM heap.     It also accounts for the memory requirements of the application master container itself, which hosts the driver.</p> <p>Note</p> <p>By default, spark.driver.memoryOverhead is calculated as 10% of spark.driver.memory. However, there's a minimum threshold: if 10% of spark.driver.memory is less than 384 MB, then spark.driver.memoryOverhead will default to 384 MB. The system picks whichever value is higher.</p> <ul> <li>Example 1 (1GB driver memory): 10% of 1GB is 100 MB. Since 100 MB is less than 384 MB, the memoryOverhead will be 384 MB.</li> <li>Example 2 (4GB driver memory): 10% of 4GB is 400 MB. Since 400 MB is greater than 384 MB, the memoryOverhead will be 400 MB.</li> <li>Example 3 (20GB driver memory): 10% of 20GB is 2GB. In this case, the memoryOverhead would be 2GB memory**</li> </ul> </li> </ul>"},{"location":"spark/sparkmemory/#common-reasons-for-driver-out-of-memory","title":"Common Reasons for Driver Out of Memory","text":"<p>Besides the collect() method, several other common scenarios can lead to driver OOM:</p> <ul> <li> <p>Using collect() Method on Large Datasets: As demonstrated, attempting to pull all data to the driver's memory will cause an OOM if the data size exceeds the driver's capacity.</p> </li> <li> <p>Broadcasting Large DataFrames/Tables:     Broadcasting is a technique used in Spark to optimize joins by sending a smaller DataFrame or table to all executors so that the larger DataFrame can be joined locally without shuffling data.      When you broadcast data (e.g., df2 and df3 in the example), the driver first merges and holds this data in its memory.      Then, the driver sends this combined data to all executors.      If you broadcast multiple large DataFrames (e.g., five 50 MB DataFrames, totaling 250 MB) and the driver doesn't have enough memory to hold them before distributing them, it will lead to a driver OOM error. This is why broadcasting is recommended only for small tables/DataFrames.</p> </li> <li>Excessive Object Creation and Heavy Non-JVM Processing:      If your Spark application creates many objects or performs heavy processing that falls under non-JVM operations, it consumes the memoryOverhead.      If the memoryOverhead is insufficient, it can lead to OOM errors often indicated as being \"due to memory overhead\".</li> <li> <p>Incorrect Memory Configuration:     Manually setting spark.driver.memory or spark.driver.memoryOverhead to values that are too low for the workload can lead to OOM.</p> <p>Example</p> <p>If you have a 20 GB driver but incorrectly set spark.driver.memoryOverhead to 1 GB when it should ideally be 2 GB (10% of 20GB), you might encounter an OOM error related to memoryOverhead</p> </li> </ul>"},{"location":"spark/sparkmemory/#handling-and-solving-driver-out-of-memory","title":"Handling and Solving Driver Out of Memory","text":"<p>Based on the reasons for OOM, the solutions are often direct:</p> <ul> <li>Avoid collect() on Large Data:     For large datasets, never use df.collect() unless you are absolutely certain the data size is small enough to fit within the driver's memory.     Instead, use df.show() for quick inspection.     If you need to process all data, consider writing it to a file system (like HDFS or S3) or processing it in a distributed manner across executors.</li> <li>Manage Broadcasted Data Carefully:     Only broadcast DataFrames or tables that are genuinely small.     Before broadcasting, ensure the driver's memory (specifically the JVM heap) is large enough to hold the combined size of all dataframes you plan to broadcast.</li> <li>Increase Driver Memory and Memory Overhead:     If your application performs extensive non-JVM operations or creates many objects, you might need to increase spark.driver.memory and/or spark.driver.memoryOverhead.     If you observe \"due to memory overhead\" errors, explicitly increasing spark.driver.memoryOverhead beyond its default 10% (while respecting system limits) might resolve the issue</li> </ul>"},{"location":"spark/sparksqlengine/","title":"SparkQuery Plan","text":"<p>The Spark SQL Engine is fundamentally the Catalyst Optimizer. Its primary role is to convert user code (written in DataFrames, SQL, or Datasets) into Java bytecode for execution. This conversion and optimization process occurs in four distinct phases. It's considered a compiler because it transforms your code into Java bytecode. It plays a key role in optimizing code leveraging concepts like lazy evaluation</p> <p></p>"},{"location":"spark/sparksqlengine/#phase-1-unresolved-logical-plan","title":"Phase 1: Unresolved Logical Plan","text":"<ul> <li> <p>User Code Input: This is the initial stage where you write your code using DataFrames, SQL, or Datasets APIs.</p> </li> <li> <p>Action: When you write transformations (e.g., select, filter, join), Spark creates an \"unresolved logical plan\". This plan is like a blueprint or a \"log\" of transformations, indicating what operations need to be performed in what order.</p> </li> <li> <p>Nature: At this stage, the plan is \"unresolved\" because Spark has not yet checked if the tables, columns, or files referenced actually exist</p> </li> </ul>"},{"location":"spark/sparksqlengine/#phase-2-analysis","title":"Phase 2: Analysis","text":"<ul> <li> <p>Purpose: To resolve the logical plan by checking the existence and validity of all referenced entities.</p> </li> <li> <p>Role of Catalog: This phase heavily relies on the Catalog. The Catalog is where Spark stores metadata (data about data). It contains information about tables, files, databases, their names, creation times, sizes, column names, and data types. For example, if you read a CSV file, the Catalog knows its path, name, and column headers.</p> </li> <li> <p>Validation: The Analysis phase queries the Catalog to verify if the files, columns, or tables specified in the unresolved logical plan actually exist.</p> </li> <li> <p>Output: If everything is found and validated, the plan becomes a \"Resolved Logical Plan\". If any entity is not found (e.g., a non-existent file path or a misspelled column name), Spark throws an AnalysisException</p> </li> </ul>"},{"location":"spark/sparksqlengine/#phase-3-logical-optimization","title":"Phase 3: Logical Optimization","text":"<ul> <li> <p>Purpose: To optimize the \"Resolved Logical Plan\" without considering the physical execution aspects. It focuses on making the logical operations more efficient.</p> </li> <li> <p>Optimization Examples:</p> <p>Note</p> <p>Predicate Pushdown: If you apply multiple filters, the optimizer might combine them or push them down closer to the data source to reduce the amount of data processed early.</p> <p>Column Pruning: If you select all columns (SELECT *) but then only use a few specific columns in subsequent operations, the optimizer will realize this and modify the plan to only fetch the necessary columns from the start, saving network I/O and processing.</p> </li> <li> <p>Leveraging Lazy Evaluation: This phase benefits from Spark's lazy evaluation, allowing it to perform these optimizations before any actual computation begins.</p> </li> <li> <p>Output: An \"Optimized Logical Plan\"</p> </li> </ul>"},{"location":"spark/sparksqlengine/#phase-4-physical-planning","title":"Phase 4: Physical Planning","text":"<ul> <li> <p>Conversion: The \"Optimized Logical Plan\" is converted into multiple possible \"Physical Plans\". Each physical plan represents a different strategy for executing the logical operations (e.g., different join algorithms).</p> </li> <li> <p>Cost-Based Model: Spark applies a \"Cost-Based Model\" to evaluate these physical plans. It estimates the resources (memory, CPU, network I/O) each plan would consume if executed.</p> </li> <li> <p>Strategy Selection: The plan that offers the best resource utilization and lowest estimated cost (e.g., least data shuffling, fastest execution time) is selected as the \"Best Physical Plan\".</p> <p>Example</p> <p>For joins, if one table is significantly smaller than the other, Spark might choose a Broadcast Join. This involves sending the smaller table to all executor nodes where the larger table's partitions reside. This avoids data shuffling (expensive network operations) of the larger table across the cluster, leading to significant performance gains.</p> </li> <li> <p>Output: The Best Physical Plan, which is essentially a set of RDDs (Resilient Distributed Datasets) ready to be executed on the cluster.</p> </li> </ul>"},{"location":"spark/sparksqlengine/#phase-5whole-stage-code-generation","title":"Phase 5:Whole-Stage Code Generation","text":"<ul> <li> <p>Final Step: This is the final step where the \"Best Physical Plan\" (the RDD operations) is translated into Java bytecode.</p> </li> <li> <p>Execution: This bytecode is then sent to the individual executors on the cluster to be executed. This direct bytecode generation improves performance by eliminating interpretation overhead and allowing the JVM to further optimize the code.</p> </li> </ul>"},{"location":"spark/sparksqlengine/#in-what-cases-will-predicate-pushdown-not-work","title":"In what cases will predicate pushdown not work?","text":"<ul> <li>Complex Data Types</li> </ul> <p>Spark's Parquet data source does not push down filters that involve complex types, such as arrays, maps, and struct. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.</p> <p>Here's an example:</p> <pre><code>root\n |-- Name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|Name      |properties                   |\n+----------+-----------------------------+\n|Afaque    |[eye -&gt; black, hair -&gt; black]|\n|Naved     |[eye -&gt;, hair -&gt; brown]      |\n|Ali       |[eye -&gt; black, hair -&gt; red]  |\n|Amaan     |[eye -&gt; grey, hair -&gt; grey]  |\n|Omaira    |[eye -&gt; , hair -&gt; brown]     |\n+----------+-----------------------------+\n</code></pre> <pre><code>df.filter(df.properties.getItem(\"eye\") == \"brown\").show()\n</code></pre> <pre><code>== Physical Plan ==\n*(1) Filter (metadata#123[key] = value)\n+- *(1) ColumnarToRow\n   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...\n</code></pre> <ul> <li>Unsupported Expressions </li> </ul> <p>In Spark, <code>Parquet</code> data source does not support pushdown for filters involving a <code>.cast</code> operation.</p> <p>The reason for this behaviour is as follows: <code>.cast</code> changes the datatype of the column, and the Parquet data source may not be able to perform the filter operation correctly on the cast data.</p> <p>Note</p> <p>This behavior may vary based on the data source. For example, if you're working with a JDBC data source connected to a database that supports SQL-like operations, the <code>.cast</code> filter could potentially be pushed down to the database.</p>"},{"location":"spark/sparksubmit/","title":"Spark-Submit & Deployment modes","text":""},{"location":"spark/sparksubmit/#what-is-spark-submit","title":"What is Spark Submit?","text":"<p>Spark Submit is a command-line tool that allows you to trigger or run your Spark applications on a Spark cluster. It packages all the required files and JARs (Java Archive files) and deploys them to the Spark cluster for execution. It is used to run jobs on various types of Spark clusters</p>"},{"location":"spark/sparksubmit/#where-is-your-spark-cluster-located","title":"Where is Your Spark Cluster Located?","text":"<p>Spark clusters can be deployed in multiple environments. When using Spark Submit, you specify the location of your master node.</p> <p>Common cluster types include:</p> <ul> <li>Standalone Cluster: A simple, self-contained Spark cluster. An example master configuration for a standalone cluster could look like spark://10.160.78.10:7077, where 7077 is the default port.</li> <li>Local Mode: For running Spark applications on your local machine, typically for development or testing. The master configuration is simply local.</li> <li>YARN (Yet Another Resource Negotiator): A popular resource management system in the Hadoop ecosystem. The master configuration is yarn.</li> <li>Kubernetes: A container orchestration system.</li> <li>Mesos: Another cluster management platform</li> </ul>"},{"location":"spark/sparksubmit/#spark-submit-command","title":"spark-submit command","text":"<p>Example</p> <p>spark-submit --master {stanadlone,yarn.mesos,kubernetes} --deploy-mode {client/cluster} --class mainclass.scala  --jars mysql-connector.jar  --conf spark.dynamicAllocation.enabled=true   --conf spark.dynamicAllocation.minExecutors=1   --conf spark.dynamicAllocation.maxExecutors=10   --conf spark.sql.broadcastTimeout=3600   --conf spark.sql.autobroadcastJoinThreshold=100000   --conf spark.executor.cores=2   --conf spark.executor.instances=5   --conf spark.default.parallelism=20   --conf spark.driver.maxResultSize=1G   --conf spark.network.timeout=800  --conf spark.driver.maxResultSize=1G   --conf spark.network.timeout=800   --driver-memory 1G   --executor-memory 2G   --num-executors 5   --executor-cores 2   --py-files /path/to/other/python/files.zip  /path/to/your/python/wordcount.py    /path/to/input/textfile.txt </p> <ul> <li> <p>master: This is the master URL for the cluster. It can be a URL for any Spark-supported cluster manager. For example, local for local mode, spark://HOST:PORT for standalone mode, mesos://HOST:PORT for Mesos, or yarn for YARN.</p> </li> <li> <p>deploy-mode: This can be either client (default) or cluster. In client mode, the driver runs on the machine from which the job is submitted. In cluster mode, the framework launches the driver inside the cluster.</p> </li> <li> <p>class: This is the entry point for your application, i.e., where your main method runs. For Java and Scala, this would be a fully qualified class name.</p> </li> <li> <p>jars: This argument allows you to provide paths to external JAR files that your Spark application depends on. You can provide multiple JAR files as a comma-separated list. It's recommended to use absolute paths for JAR files to prevent future issues, even if they are in the same directory</p> </li> <li> <p>conf: This is used to set any Spark property. For example, you can set Spark properties like spark.executor.memory, spark.driver.memory, etc.</p> <ul> <li>spark.dynamicAllocation.enabled true: Enables dynamic memory allocation.</li> <li>spark.dynamicAllocation.minExecutors 1: Sets the minimum number of executors to 1.</li> <li>spark.dynamicAllocation.maxExecutors 10: Sets the maximum number of executors to 10. This prevents a single process from hogging all resources. This is beneficial because if a process reserves memory but doesn't use it, dynamic allocation can free up that idle memory for other processes.</li> <li>Broadcast Threshold: This configuration determines the maximum size of data that Spark will automatically broadcast to all worker nodes when performing a join. The default is 10MB.</li> <li>Broadcast Timeout: This sets the maximum time (in seconds) that a broadcast operation is allowed to take before timing out. A common general setting might be 600 seconds (10 minutes) or 1200 seconds (20 minutes), while 3600 seconds (1 hour) is considered very long and can significantly delay job completion</li> <li>spark.executor.cores=2 sets the number of cores to use on each executor.</li> <li>spark.executor.instances=5: sets the number of executor instances</li> <li>spark.default.parallelism=20: sets the default number of partitions in RDDs returned by transformations like join(), reduceByKey(), and parallelize() when not set by user.</li> <li>spark.driver.maxResultSize=1G:  limits the total size of the serialized results of all partitions for each Spark action (e.g., collect). This should be at least as large as the largest object you want to collect.</li> <li>spark.network.timeout=800:  sets the default network timeout value to 800 seconds. This configuration plays a vital role in cases where you deal with large shuffles.</li> </ul> </li> <li> <p>driver-memory: Specifies the amount of memory allocated to the Spark Driver program.</p> </li> <li> <p>executor-memory: Specifies the amount of memory allocated to each Spark Executor.</p> </li> <li> <p>num-executors: Specifies the total number of executors to launch for the application. Combined with --executor-memory, this implies the total executor memory required (e.g., 2 GB/executor * 5 executors = 10 GB total executor memory).</p> </li> <li> <p>executor-cores: Specifies the number of CPU cores allocated to each executor. This determines how many parallel tasks an executor can run</p> </li> <li> <p>files: This argument is used to specify non-Python files (e.g., configuration files like .ini, .json, .csv) that your Spark application needs. Similar to --py-files, these are bundled and distributed to all worker nodes</p> </li> </ul> <p>After all the Spark Submit configurations, you provide the path to your main application script (e.g., main.py). Any values provided after the main script are treated as command-line arguments that can be accessed within your script.</p> <ul> <li> <p>main.py: This is typically accessed as sys.argv in Python.</p> </li> <li> <p>Subsequent arguments: These are sys.argv, sys.argv, and so on. They are useful for passing dynamic parameters like environment names (e.g., dev, qa, prod) to control execution flow within your script without changing the script itself</p> </li> <li> <p>application-jar: This is a path to your compiled Spark application.</p> </li> <li> <p>application-arguments: These are arguments that you need to pass to your Spark application</p> </li> </ul> <p></p>"},{"location":"spark/sparksubmit/#client-mode","title":"Client Mode","text":"<p>In Client mode, the Spark Driver runs directly on the edge node (or the machine from which the spark-submit command is executed).</p> <p>The Executors, however, still run on the worker nodes within the cluster.</p> <ul> <li>Advantages:<ul> <li>Easy Debugging and Real-time Logs: Logs (STD OUT and STD ERR) are generated directly on the client machine (edge node). This makes it very easy for developers to monitor the process, see real-time output, debug issues, and observe errors as they occur. This mode is highly suitable for development and testing of small code snippets.</li> </ul> </li> <li>Disadvantages:<ul> <li>Vulnerability to Edge Node Shutdown: If the edge node is shut down, either accidentally or intentionally, the Spark Driver (running on it) will be terminated. Since the Driver coordinates the entire application, its termination will cause all associated Executors to be killed, leading to the entire Spark job stopping abruptly and incompletely.</li> <li>High Network Latency: Communication between the Driver (on the edge node) and the Executors (on worker nodes in the cluster) involves two-way communication across the network. This can introduce network latency, especially for operations like Broadcaster, where data needs to be first sent to the Driver and then distributed to Executors.</li> <li>Potential for Driver Out of Memory (OOM) Errors: If multiple users submit jobs in Client mode from the same edge node, and their collective Driver memory requirements exceed the edge node's physical memory capacity (which is typically lower than worker nodes), processes may fail to start or encounter Driver OOM errors</li> </ul> </li> </ul> <p>When u start a spark shell, application driver creates the spark session in your local machine which request to Resource Manager present in cluster to create Yarn application. YARN Resource Manager start an Application Master (AM container). For client mode Application Master acts as the Executor launcher. Application Master will reach to Resource Manager and request for further containers.  Resource manager will allocate new containers. These executors will directly communicate with Drivers which is present in the system in which you have submitted the spark application.</p>"},{"location":"spark/sparksubmit/#cluster-mode","title":"Cluster Mode","text":"<p>For cluster mode, there\u2019s a small difference compare to client mode in place of driver. Here Application Master will create driver in it and driver will reach to Resource Manager.</p> <p>In Cluster mode, the Spark Driver (Application Master container) is launched and runs on one of the worker nodes within the Spark cluster. The Executors also run on other worker nodes in the cluster.</p> <ul> <li>Advantages:<ul> <li>Resilience and Disconnect-ability: Once a Spark job is submitted in Cluster mode, the Driver runs independently within the cluster. This means the user can disconnect from or even shut down their edge node machine without affecting the running Spark application. This makes it ideal for long-running jobs.</li> <li>Low Network Latency: Both the Driver and the Executors are running within the same cluster. This proximity significantly reduces network latency between them, leading to more efficient data transfer and communication.</li> <li>Scalability and Resource Utilization: Worker nodes are provisioned with significant memory and processing capabilities. By running the Driver on a worker node, the application can leverage the cluster's robust resources, reducing the likelihood of Driver OOM issues, even with many concurrent jobs.</li> <li>Suitable for Production Workloads: Cluster mode is the recommended deployment mode for production workloads, especially for scheduled jobs that run automatically and do not require constant real-time monitoring on the client side.</li> </ul> </li> <li>Disadvantages:     Indirect Log Access: Logs and output are not directly displayed on the client machine. When a job is submitted in Cluster mode, an Application ID is generated. Users must use this Application ID to access the Spark Web UI (User Interface) to track the job's status, progress, and logs. This adds an extra step for monitoring compared to Client mode</li> </ul>"},{"location":"spark/sparksubmit/#local-mode","title":"Local Mode","text":"<p>In local mode, Spark runs on a single machine, using all the cores of the machine. It is the simplest mode of deployment and is mostly used for testing and debugging.</p>"},{"location":"spark/sparksubmit/#comparison","title":"Comparison","text":"Feature Client Mode Cluster Mode Driver Location Edge Node (or client machine) Worker Node within the cluster Log Generation On client machine (STD OUT, STD ERR) Application ID generated; view via Spark Web UI Debugging Easy, real-time feedback Requires checking Spark Web UI Network Latency High (Driver &lt;-&gt; Executors across network) Low (Driver &lt;-&gt; Executors within cluster) Edge Node Shutdown Application stops (Driver killed) Application continues to run Driver Out of Memory Higher chance if many users/low edge node memory Lower chance (cluster has more resources) Use Case Development, small code snippets, debugging Production workloads, long-running jobs"},{"location":"spark/sparktrans/","title":"DAG, Transformations & Actions","text":"<p>Spark represents a sequence of transformations on data as a DAG, a concept borrowed from mathematics and computer science. A DAG is a directed graph with no cycles, and it represents a finite set of transformations on data with multiple stages. The nodes of the graph represent the RDDs or DataFrames/Datasets, and the edges represent the transformations or operations applied.</p> <p>Each action on an RDD (or DataFrame/Dataset) triggers the creation of a new DAG. The DAG is optimized by the Catalyst optimizer (in case of DataFrame/Dataset) and then it is sent to the DAG scheduler, which splits the graph into stages of tasks.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#job-stage-and-task-in-spark","title":"Job, Stage and Task in Spark","text":"<ul> <li> <p>Application An application in Spark refers to any command or program that you submit to your Spark cluster for execution. Typically, one spark-submit command creates one Spark application. You can submit multiple applications, but each spark-submit initiates a distinct application.</p> </li> <li> <p>Job Within an application, jobs are created based on \"actions\" in your Spark code. An action is an operation that triggers the computation of a result, such as collect(), count(), write(), show(), or save(). If your application contains five actions, then five separate jobs will be created. Every job will have a minimum of one stage and one task associated with it.</p> </li> <li> <p>Stage A job is further divided into smaller parts called stages. Stages represent a set of operations that can be executed together without shuffling data across the network. Think of them as logical steps in a job's execution plan. Stages are primarily defined by \"wide dependency transformations\".</p> <p>Note</p> <p>Wide Dependency Transformations (e.g., repartition(), groupBy(), join()) require shuffling data across partitions, meaning data from one partition might be needed by another. Each wide dependency transformation typically marks the end of one stage and the beginning of a new one.</p> <p>Narrow Dependency Transformations (e.g., filter(), select(), map()) do not require data shuffling; an output partition can be computed from only one input partition. Multiple narrow transformations can be grouped into a single stage.</p> </li> <li> <p>Task A task is the actual unit of work that is executed on an executor. It performs the computations defined within a stage on a specific partition of data. The number of tasks within a stage is directly determined by the number of partitions the data has at that point in the execution. If a stage operates on 200 partitions, it will typically launch 200 tasks.</p> </li> </ul> <p>Relationship Summary:</p> <ul> <li>One Application can contain Multiple Jobs.</li> <li>One Job can contain Multiple Stages.</li> <li>One Stage can contain Multiple Tasks</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#example-spark-code-snippet","title":"Example Spark Code Snippet","text":"<pre><code># Action (read is an action sometimes)\n# The source treats 'read' as an action that triggers a job.\ndf = spark_session.read.csv(\"path/to/data.csv\") \n# Wide Dependency Transformation\ndf_repartitioned = df.repartition(2) \n# Narrow Dependency Transformation\ndf_filtered = df_repartitioned.filter(\"salary &gt; 90000\") \n# Narrow Dependency Transformation\ndf_selected = df_filtered.select(\"age\", \"salary\")\n# Wide Dependency Transformation\ndf_grouped = df_selected.groupBy(\"age\").count()\n# Action\ndf_grouped.collect() \n</code></pre> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#step-by-step-analysis-of-execution","title":"Step-by-Step Analysis of Execution","text":"<ol> <li> <p>Job Creation:     The source explicitly states that read and collect are actions that create jobs.     Therefore, this code snippet will trigger two jobs.</p> <ul> <li> <p>Job 1: Triggered by the spark_session.read.csv() operation.</p> </li> <li> <p>Job 2: Triggered by the df_grouped.collect() operation.</p> </li> </ul> </li> <li> <p>Stage Creation (within Job 2, as it's more complex):     Remember, stages are split at wide dependency transformations.     Initial Data State: When df is read, the source assumes it's small (e.g., less than 128MB), so it initially fits into one partition.</p> <p>Stage 1 (triggered by repartition):</p> <ul> <li> <p>df.repartition(2): This is a wide dependency transformation. It means the single initial partition will be repartitioned into two partitions.</p> </li> <li> <p>This repartition operation will mark the end of one stage and the beginning of a new one. It becomes Stage 1 of Job 2.</p> </li> </ul> <p>Stage 2 (triggered by filter and select):</p> <ul> <li> <p>df_repartitioned.filter(\"salary &gt; 90000\"): This is a narrow dependency transformation.</p> </li> <li> <p>df_filtered.select(\"age\", \"salary\"): This is also a narrow dependency transformation.</p> </li> <li> <p>Since both are narrow transformations and follow repartition without another wide dependency, they will be executed within the same stage, which is Stage 2 of Job 2.</p> </li> </ul> <p>Stage 3 (triggered by groupBy):</p> <ul> <li> <p>df_selected.groupBy(\"age\").count(): This is a wide dependency transformation. Grouping by a key requires data shuffling to bring all records with the same key to the same partition.</p> </li> <li> <p>This groupBy operation will trigger a new stage, becoming Stage 3 of Job 2.</p> </li> </ul> <p>Total Stages:</p> <ul> <li> <p>Job 1 (from read) would have a minimum of one stage.</p> </li> <li> <p>Job 2 (from collect) would have three stages (one for repartition, one for filter/select, and one for groupBy).</p> </li> <li> <p>Thus, the total number of stages for this entire application would be 1 + 3 = 4 stages.</p> </li> </ul> </li> <li> <p>Task Creation (within stages of Job 2):</p> <p>The number of tasks in a stage depends on the number of partitions.</p> <p>Job 1 Stage 1 (from read):</p> <ul> <li> <p>Initially, the data is in one partition (assuming less than 128MB).</p> </li> <li> <p>Therefore, 1 task will be created for this stage.</p> </li> </ul> <p>Job 2 Stage 1 (from repartition):</p> <ul> <li> <p>After repartition(2), the data is now in two partitions.</p> </li> <li> <p>Therefore, 2 tasks will be created in this stage to handle the two partitions.</p> </li> </ul> <p>Job 2 Stage 2 (from filter and select):</p> <ul> <li> <p>These operations are on the two partitions created by repartition.</p> </li> <li> <p>Thus, 2 tasks will run in parallel for this stage, one for each partition.</p> </li> </ul> <p>Job 2 Stage 3 (from groupBy):</p> <ul> <li> <p>When groupBy is performed, Spark, by default, creates 200 partitions for shuffle operations (like groupBy or join). Even if the input data had fewer partitions, the output of a shuffle stage will default to 200 partitions.</p> </li> <li> <p>Therefore, 200 tasks will be created for this stage, one for each of the 200 output partitions.</p> </li> </ul> <p>Total Tasks:</p> <ul> <li> <p>Job 1: 1 task.</p> </li> <li> <p>Job 2: (Stage 1: 2 tasks) + (Stage 2: 2 tasks) + (Stage 3: 200 tasks) = 204 tasks.</p> </li> <li> <p>The source shows 203 tasks as a total. This discrepancy might arise from the initial read task not being explicitly counted in the later sum, or some tasks being optimized away. However, the explanation for 2 tasks (from repartition) and 200 tasks (from groupBy default) is consistent. The sum presented in the source for the latter part of the job is 200+2 = 202, and adding the initial 1 task for the read gives 203.</p> </li> </ul> </li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#what-if-our-cluster-capacity-is-less-than-the-size-of-data-to-be-processed","title":"What if our cluster capacity is less than the size of data to be processed?","text":"<p>If your cluster memory capacity is less than the size of the data to be processed, Spark can still handle it by leveraging its ability to perform computations on disk and spilling data from memory to disk when necessary.</p> <p>Let's break down how Spark will handle a 60 GB data load with a 30 GB memory cluster:</p> <ol> <li> <p>Data Partitioning: When Spark reads a 60 GB file from HDFS, it partitions the data into manageable blocks, according to the Hadoop configuration parameter dfs.blocksize or manually specified partitions. These partitions can be processed independently.</p> </li> <li> <p>Loading Data into Memory: Spark will load as many partitions as it can fit into memory. It starts processing these partitions. The size of these partitions is much smaller than the total size of your data (60 GB), allowing Spark to work within the confines of your total memory capacity (30 GB in this case).</p> </li> <li> <p>Spill to Disk: When the memory is full, and Spark needs to load new partitions for processing, it uses a mechanism called \"spilling\" to free up memory. Spilling means writing data to disk. The spilled data is the intermediate data generated during shuffling operations, which needs to be stored for further stages.</p> </li> <li> <p>On-Disk Computation: Spark has the capability to perform computations on data that is stored on disk, not just in memory. Although computations on disk are slower than in memory, it allows Spark to handle datasets that are larger than the total memory capacity.</p> </li> <li> <p>Sequential Processing: The stages of the job are processed sequentially, meaning Spark doesn't need to load the entire dataset into memory at once. Only the data required for the current stage needs to be in memory or disk.</p> </li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#how-spark-perform-data-partitioning","title":"How spark perform data partitioning","text":"<ul> <li>Data Partitioning: Apache Spark partitions data into logical chunks during reading from sources like HDFS, S3, etc.</li> <li>Data Distribution: These partitions are distributed across the Spark cluster nodes, allowing for parallel processing.</li> <li>Custom Partitioning: Users can control data partitioning using Spark's repartition(), coalesce() and partitionBy() methods, optimizing data locality or skewness.</li> </ul> <p>When Apache Spark reads data from a file on HDFS or S3, the number of partitions is determined by the size of the data and the default block size of the file system. In general, each partition corresponds to a block in HDFS or an object in S3.</p> <p>Example</p> <p>If HDFS is configured with a block size of 128MB and you have a 1GB file, it would be divided into 8 blocks in HDFS. Therefore, when Spark reads this file, it would create 8 partitions, each corresponding to a block.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#transformations","title":"Transformations","text":"<p>In Spark, a transformation is an operation applied on an RDD (Resilient Distributed Dataset) or DataFrame/Dataset to create a new RDD or DataFrame/Dataset. </p> <p>Transformations refer to any processing done on data. They are operations that create a new DataFrame (or RDD) from an existing one, but they do not execute immediately. Spark is based on lazy evaluation, meaning transformations are only executed when an action is triggered</p> <p>Transformations in Spark are categorized into two types: narrow and wide transformations.</p> <p></p>"},{"location":"spark/sparktrans/#narrow-transformations","title":"Narrow Transformations","text":"<p>In these transformations, all elements that are required to compute the records in a single partition live in the same partition of the parent RDD. Data doesn't need to be shuffled across partitions.</p> <p>These are transformations that do not require data movement between partitions. In a distributed setup, each executor can process its partition of data independently without needing to communicate with other executors</p> <p>Example</p> <ul> <li>map(): Applies a function to each element in the RDD and outputs a new RDD.</li> <li>filter(): Creates a new RDD by selecting only the elements of the original RDD that pass a function's condition.</li> <li>flatMap(): Function in Spark applies a function to each element of an RDD, then flattens the multiple outputs into single RDD.</li> <li>sample(): Create a sample dataset from the original data.</li> </ul>"},{"location":"spark/sparktrans/#wide-transformations","title":"Wide Transformations","text":"<p>These transformations will have input data from multiple partitions. This typically involves shuffling all the data across multiple partitions. </p> <p>These transformations require data movement or \"shuffling\" between partitions. This means an executor might need data from another executor's partition to complete its computation. This data movement makes wide transformations expensive operations</p> <p>Example</p> <ul> <li>groupByKey(): Groups all the values of each key in the RDD into a single sequence.</li> <li>reduceByKey(): Performs a reduction operation for each key in the RDD.</li> <li>join(): Joins two RDDs based on a common key, similar to the SQL JOIN operation.</li> <li>distinct(): Remove duplicates in the RDD.</li> <li>coalesce(): Decreases the number of partitions in the RDD.</li> <li>repartition(): Increases the number of partitions in the RDD.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#actions","title":"Actions","text":"<p>Actions in Apache Spark are operations that provide non-RDD values; they return a final value to the driver program or write data to an external system. Actions trigger the execution of the transformation operations accumulated in the Directed Acyclic Graph (DAG).</p> <p>Actions are operations that trigger the execution of all previous transformations and produce a result. When an action is hit, Spark creates a job.</p> <p>Example</p> <ul> <li>Collect: collect() returns all the elements of the RDD as an array to the driver program. This can be useful for testing and debugging, but be careful with large datasets to avoid out-of-memory errors.</li> <li>Count: count() returns the number of elements in the RDD.</li> <li>First: first() returns the first element of the RDD.</li> <li>Take: take(n) returns the first n elements of the RDD.</li> <li>foreach: foreach() is used for performing computations on each element in the RDD.</li> <li>SaveAsTextFile: saveAsTextFile() writes the elements of the dataset to a text file (or set of text files) in a specified directory in the local filesystem, HDFS, or any other Hadoop-supported file system.</li> <li>SaveAsSequenceFile: This action is used to save RDDs, which consist of key/value pairs, in SequenceFile format.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#read-write-operation-in-spark-are-transformationaction","title":"Read &amp; Write operation in Spark are Transformation/Action?","text":"<p>Reading and writing operations in Spark are often viewed as actions, but they're a bit unique. </p> <p>Read Operation:Transformations , especially read operations can behave in two ways according to the arguments you provide</p> <p>Note</p> <ul> <li>Lazily evaluated - It will be performed only when an action is called.</li> <li>Eagerly evaluated - A job will be triggered to do some initial evaluations. In case of read.csv()</li> </ul> <p>If it is called without defining the schema and inferSchema is disabled, it determines the columns as string types and it reads only the first line to determine the names (if header=True, otherwise it gives default column names) and the number of fields.  Basically it performs a collect operation with limit 1, which means one new job is created instantly</p> <p>Now if you specify inferSchema=True, Here above job will be triggered first as well as one more job will be triggered which will scan through entire record to determine the schema, that's why you are able to see 2 jobs in spark UI</p> <p>Now If you specify schema explicitly by providing StructType() schema object to 'schema' argument of read.csv(), then you can see no jobs will be triggered here. This is because, we have provided the number of columns and type explicitly and catalogue of spark will store that information and now it doesn't need to scan the file to get that information and this will be validated lazily at the time of calling action.</p> <p>Write Operation: Writing or saving data in Spark, on the other hand, is considered an action. Functions like saveAsTextFile(), saveAsSequenceFile(), saveAsObjectFile(), or DataFrame write options trigger computation and result in data being written to an external system.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"spark/sparktrans/#lazy-evaluation-in-spark","title":"Lazy Evaluation in Spark","text":"<p>Lazy evaluation in Spark means that the execution doesn't start until an action is triggered. In Spark, transformations are lazily evaluated, meaning that the system records how to compute the new RDD (or DataFrame/Dataset) from the existing one without performing any transformation. The transformations are only actually computed when an action is called and the data is required. </p> <p>Example</p> <p>spark.read.csv()  will not actually read the data until an action like .show() or .count() is performed</p>"},{"location":"spark/sparkwindow/","title":"Sparkwindow","text":"<p>ROW_NUMBER():     \u25e6 Assigns a unique, sequential integer to each row within its partition, starting from 1.     \u25e6 It does not consider ties in the ordering column; if two rows have the same value, they will still receive different, consecutive row numbers.     \u25e6 Analogy: Like giving a unique ID to each student in a class based on their height, even if some have the same height. \u2022 RANK():     \u25e6 Assigns a rank to each row within its partition.     \u25e6 If there are ties (multiple rows have the same value in the ordering column), they receive the same rank.     \u25e6 However, RANK() leaves gaps in the ranking sequence for subsequent rows after a tie. For example, if two employees are rank 1, the next unique salary will get rank 3 (skipping rank 2).     \u25e6 Analogy: If two students are the tallest, they both get rank 1, and the next tallest student gets rank 3, as if rank 2 was \"taken\" by the tied students. \u2022 DENSE_RANK():     \u25e6 Similar to RANK(), it assigns the same rank to rows with tied values in the ordering column.     \u25e6 The crucial difference is that DENSE_RANK() does not leave gaps in the ranking sequence. If two employees are rank 1, the next unique salary will get rank 2.     \u25e6 Analogy: If two students are the tallest, they both get rank 1, and the next tallest student gets rank 2, effectively \"compressing\" the ranks</p> Salary Row Number Rank Dense Rank 5000 1 1 1 6000 2 2 2 7000 3 3 3 8000 4 4 4 8000 5 4 4 8000 6 4 4 9000 7 7 5 <p>LEAD and LAG are window functions that allow you to access data from a preceding (LAG) or succeeding (LEAD) row within the same result set, often within a defined \"window\" or partition. \u2022 LAG: Used to access data from a previous record. If you are currently in the January record, LAG would let you access the December record (or the record immediately preceding January based on your orderBy clause). \u2022 LEAD: Used to access data from a subsequent record. If you are in the January record, LEAD would let you access the February record (or the record immediately following January). Both lag and lead functions take three arguments: 1. Column Name: The name of the column from which you want to retrieve the value (e.g., \"Sales\"). 2. Offset: An integer specifying how many records back (for LAG) or forward (for LEAD) you want to look. This is not about months but about the number of rows/records. For example, 1 means one record back/forward, 2 means two records back/forward. 3. Default Value: A value to be returned if there is no record available at the specified offset (e.g., if you ask for a previous record for the very first record in a partition). By default, this value is null. You can set it to 0, 100, or anything else. Interaction with Window Functions: LEAD and LAG must be used with a window specification. A window function defines a \"frame\" or \"partition\" of rows on which the function will operate. This is crucial because LEAD and LAG only operate within their designated window and cannot cross partition boundaries. A typical window specification for these functions involves: \u2022 partitionBy: This divides your data into logical groups (e.g., by product_id or product_name). LEAD and LAG operations will reset for each new partition. For example, if you partition by product, an iPhone's previous month sales will only look at previous iPhone sales, not Samsung sales. \u2022 orderBy: This specifies the order of rows within each partition (e.g., by sales_data or month). This ordering is critical for lag and lead to correctly identify \"previous\" or \"next\" records</p> <p>Window Frame Components:         \u25aa UNBOUNDED PRECEDING: Refers to all rows from the start of the window.         \u25aa UNBOUNDED FOLLOWING: Refers to all rows from the current row to the end of the window.         \u25aa CURRENT ROW: Refers to the row currently being processed.         \u25aa ROWS BETWEEN: This function defines the start and end boundaries of the window frame based on physical row offsets.             \u2022 It takes two arguments: start and end.             \u2022 0 represents the CURRENT ROW.             \u2022 Negative numbers (e.g., -1, -2) represent rows preceding (above) the current row.             \u2022 Positive numbers (e.g., 1, 2) represent rows following (below) the current row.             \u2022 For example, rowsBetween(-2, Window.currentRow) includes the current row and the two preceding rows.         \u25aa RANGE BETWEEN: Similar to rows between but defines the window frame based on logical offset values (e.g., time or value difference) rather than physical row counts. The video mentions that range between was not fully demonstrated due to a type mismatch error encountered during the recording</p>"},{"location":"sql/Mongo/","title":"Mongo","text":""},{"location":"sql/Mongo/#what-is-nosql-database","title":"What is NoSQL Database?","text":"<p>NoSQL databases, also known as \"non-SQL\" or \"not only SQL\", are databases that provide a mechanism to store and retrieve data modeled in ways other than the tabular format used in relational databases. They are typically used in large-scale or real-time web applications where the ability to scale quickly and handle large, diverse types of data is critical.</p> <p>Here are some key characteristics and features of NoSQL databases:</p> <ol> <li>Schema-less: NoSQL databases do not require a fixed schema, which gives you the flexibility to store different types of data entities together.</li> <li>Scalability: NoSQL databases are designed to expand easily to handle more traffic. They are horizontally scalable, meaning you can add more servers to handle larger amounts of data and higher loads.</li> <li>Diverse Data Models: NoSQL databases support a variety of data models including key-value pairs, wide-column, graph, or document. This flexibility allows them to handle diverse types of data and complex data structures.</li> <li>Distributed Architecture: Many NoSQL databases are designed with a distributed architecture, which can improve fault tolerance and data availability.</li> <li>Performance: Without the need for data relationships and joins as in relational databases, NoSQL databases can offer high-performance reads and writes.</li> </ol>"},{"location":"sql/Mongo/#types-of-nosql-databases","title":"Types of NoSQL Databases","text":"<p>NoSQL databases are categorized into four basic types based on the way they organize data. Let's go through each type and provide examples:</p> <ol> <li>Document Databases: These store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values, and the values can typically be a variety of types including strings, numbers, booleans, arrays, or objects. Each document is unique and can contain different data from other documents in the collection. This structure makes document databases flexible and adaptable to various data models.     Example: MongoDB, CouchDB.</li> <li>Key-Value Databases: These are the simplest type of NoSQL databases. Every single item in the database is stored as  an attribute name (or 'key') and its value. The main advantage of a key-value store is the ability to read and write operations using a simple key. This type of NoSQL database is typically used for caching and session management.     Example: Redis, Amazon DynamoDB</li> <li>Wide-Column Stores: These databases store data in tables, rows, and dynamic columns. Wide-column stores offer high performance and a highly scalable architecture. They're ideal for analyzing large datasets and are capable of storing vast amounts of data (Big Data).     Example: Apache Cassandra, Google BigTable.</li> <li>Graph Databases: These are used to store data whose relations are best represented as a graph. Each node of the graph represents an entity, and the relationship between nodes is stored directly, which allows the data to be retrieved in one operation. They're ideal for storing data with complex relationships, like social networks or a network of IoT devices.     Example: Neo4j, Amazon Neptune</li> </ol>"},{"location":"sql/Mongo/#difference-between-transactional-nosql-database","title":"Difference between Transactional &amp; NoSQL Database","text":""},{"location":"sql/Mongo/#nosql-databases-are-good-fit-for-analytical-queries","title":"NoSQL Databases are Good fit for Analytical Queries?","text":"<p>While NoSQL databases can handle certain analytical tasks, their primary purpose is not for heavy analytical queries. Traditional relational databases and data warehousing solutions, such as Hive, Redshift, Snowflake or BigQuery, are often better suited for complex analytical queries due to their ability to handle operations like joins and aggregations more efficiently</p>"},{"location":"sql/Mongo/#nosql-databases-in-bigdata-ecosystem","title":"NoSQL Databases in BigData Ecosystem","text":"<p>The strength of NoSQL databases lies in their flexibility, scalability, and speed for certain types of workloads, making them ideal for specific use-cases in the Big Data ecosystem:</p> <ol> <li>Handling Large Volumes of Data at Speed: NoSQL databases are designed to scale horizontally across many servers, which enables them to handle large volumes of data at high speed. This is particularly useful for applications that need real-time read/write operations on Big Data.</li> <li>Variety of Data Formats: NoSQL databases can handle a wide variety of data types (structured, semi-structured, unstructured), making them ideal for Big Data scenarios where data formats are diverse and evolving.</li> <li>Fault Tolerance and Geographic Distribution: NoSQL databases have a distributed architecture that provides high availability and fault tolerance, critical for applications operating on Big Data.</li> <li>Real-time Applications: Many Big Data applications require real-time or near-real-time functionality. NoSQL databases, with their high-speed read/write capabilities and ability to handle high volumes of data, are often used for real-time analytics, IoT data, and other similar scenarios. That said, the choice between SQL, NoSQL, or other database technologies should be based on the specific needs of the use-case at hand.</li> </ol>"},{"location":"sql/Mongo/#cap-theorem","title":"CAP Theorem","text":"<p>The CAP theorem is a concept that a distributed computing system is unable to simultaneously provide all three of the following guarantees:</p> <ol> <li>Consistency (C): Every read from the system receives the most recent write or an error. This implies that all nodes see the same data at the same time. It's the idea that you're always reading fresh data.</li> <li>Availability (A): Every request receives a (non-error) response, without the guarantee that it contains the most recent write. It's the idea that you can always read or write data, even if it's not the most current data.</li> <li>Partition Tolerance (P): The system continues to operate despite an arbitrary number of network or message failures (dropped, delayed, scrambled messages). It's the idea that the system continues to function even when network failures occur between nodes. Now, the key aspect of the CAP theorem, proposed by computer scientist Eric Brewer, is that a distributed system can satisfy any two of these three guarantees at the same time, but not all three. Hence the term \"CAP\" - Consistency, Availability, and Partition tolerance.</li> </ol> <p></p> <p>Here's how the three dichotomies look like:</p> <ol> <li>CA (Consistent and Available) systems prioritize data consistency and system availability but cannot tolerate network partitions. In such a system, if there is a partition between nodes, the system won't work as it doesn't support partition tolerance.</li> <li>CP (Consistent and Partition-tolerant) systems prioritize data consistency and partition tolerance. If a network partition occurs, the system sacrifices availability to ensure data consistency across all nodes.</li> <li> <p>AP (Available and Partition-tolerant) systems prioritize system availability and partition tolerance. If a network partition occurs, all nodes may not immediately reflect the same data, but the system remains available. Remember, real-world systems must tolerate network partitions (P), so the practical choice is between consistency (C) and availability (A) when partitions occur.</p> </li> <li> <p>NoSQL Database: MongoDB is a NoSQL database, meaning it does not use traditional table-based relational database structures. It's designed for large scale data storage and for handling diverse data types.</p> </li> <li>Document-Oriented: It stores data in JSON-like documents (BSON format), which allows for varied, dynamic schemas. This is in contrast to SQL databases which use a fixed table schema.</li> <li>Schema-less: MongoDB is schema-less, meaning that the documents in the same collection (equivalent to tables in SQL) do not need to have the same set of fields or structure, and the common field in different documents can hold different types of data.</li> <li>Scalability: It offers high scalability through sharding, which distributes data across multiple machines.</li> <li>Replication: MongoDB provides high availability with replica sets. A replica set consists of two or more copies of the data. Each replica set member may act in the role of primary or secondary replica at any time. The primary replica performs all write operations, while secondary replicas maintain a copy of the data of the primary using built-in replication.</li> <li>Querying: Supports a rich set of querying capabilities, including document-based queries, range queries, regular expression searches, and more.</li> <li>Indexing: Any field in a MongoDB document can be indexed, which improves the performance of search operations.</li> </ol>"},{"location":"sql/Mongo/#mongodb-architecture","title":"MongoDB Architecture","text":"<ol> <li> <p>Document Model: BSON Format: MongoDB stores data in BSON (Binary JSON) documents, which are JSON-like structures. This format supports a rich variety and complexity of data types. Unlike relational databases, MongoDB does not require a predefined schema. The structure of documents can change over time.</p> </li> <li> <p>Collections: Similar to Tables: Collections in MongoDB are analogous to tables in relational databases. They hold sets of documents. Schema-less: Each document in a collection can have a completely different structure.</p> </li> <li> <p>Database: Multiple Collections: A MongoDB instance can host multiple databases, each containing their own collections.</p> </li> <li> <p>Sharding: a shard typically refers to a group of multiple nodes (machines), especially in production environments.  Each shard is often a replica set, which is a group of mongod instances that hold the same data set. In this setup, each shard consists of multiple machines - one primary and multiple secondaries.</p> <p>Shard Key: The distribution of data across shards is determined by a shard key. MongoDB partitions data in the collection based on this  key, and different partitions (or chunks of data) are stored on different shards.</p> <p>Primary Node: Within each shard (replica set), there is one primary node that handles all write operations. All data changes are first written to the primary.</p> <p>Secondary Nodes: The secondary nodes replicate data from the primary node, providing redundancy and increasing data availability. They can also serve read operations to distribute the read load.</p> </li> <li> <p>Query Router Role in Sharded Clusters:The Query Router is typically a mongos instance in MongoDB. It acts as an intermediary between client applications and the MongoDB sharded cluster.</p> <p>Query Distribution: The Query Router receives queries from client applications and determines the appropriate shard(s) that hold the relevant data. It routes the query to the correct shard(s) based on the shard key and the cluster\u2019s current configuration.</p> <p>Aggregation of Results: After receiving responses from the shards, the Query Router aggregates these results and returns them to the client application. This process is transparent to the client, which interacts with the Query Router as if it were a single MongoDB server.</p> <p>Load Balancing: Query Routers can help distribute read and write loads across the shards, enhancing the overall performance of the database system.In larger deployments, multiple Query Routers can be used to balance the load and provide redundancy.</p> <p>Shard Management: The Query Router communicates with the cluster\u2019s config servers to keep track of the metadata about the cluster's current state, including the distribution of data across shards.</p> <p>Simplifies Client Interaction: By abstracting the complexity of the sharded cluster, Query Routers simplify how clients interact with the database. Clients do not need to know the details of data distribution across shards.</p> <p>Write Operations: For write operations, the Query Router forwards the request to the primary replica set member of the appropriate shard.</p> <p>Caching: Query Routers cache the cluster\u2019s metadata to quickly route queries without needing to frequently access config servers.</p> </li> </ol>"},{"location":"sql/Mongo/#mongodb-indexes","title":"MongoDB Indexes","text":"<p>Indexing in MongoDB is a critical feature that improves the performance of database operations, particularly in querying and sorting data. </p> <p>Purpose: Indexes in MongoDB are used to efficiently fetch data from a database. Without indexes, MongoDB must perform a full scan of a collection to select those documents that match the query statement.     Default Index: Every MongoDB collection has an automatic index created on the _id field. The _id index is the primary key and ensures the uniqueness of each document in the collection.     Index Types: MongoDB supports various types of indexes, catering to different types of data and queries.</p> <p>Types of Indexes</p> <ol> <li>Single Field Index: Indexes a single field of a document in either ascending or descending order. Besides the default _id index, you can create custom single field indexes.</li> <li>Compound Index: Indexes multiple fields within a document. The order of fields listed in a compound index is significant. It determines the sort order and query capability of the index.</li> <li>Multikey Index: Created automatically for fields that hold an array. If you index a field that contains an array, MongoDB creates separate index entries for each element of the array.</li> <li>Text Index: Used for searching text strings. A text index stores the content of a field tokenized as words, optimized for text search operations.</li> <li>Hashed Index: Stores the hash of the value of a field. These are primarily used in sharding scenarios to evenly distribute data across shards.</li> <li>Partial Index: Indexes only the documents in a collection that meet a specified filter expression. This can be more efficient and consume less space than indexing all documents.</li> <li>TTL (Time-To-Live) Index: Automatically deletes documents from a collection after a certain amount of time. This is useful for data that needs to expire, like sessions or logs.</li> </ol>"},{"location":"sql/Mongo/#use-cases-of-mongodb","title":"Use cases of MongoDB","text":"<ol> <li> <p>Content Management Systems:     Flexible schema accommodates various content types and changing data structures.     Efficiently stores and retrieves diverse and complex data sets.</p> </li> <li> <p>Mobile Apps:     Scales easily with user growth.     Offers real-time data synchronization and integration capabilities.</p> </li> <li> <p>Internet of Things (IoT):     Handles high volumes of time-series data from sensors and devices. Supports geospatial queries and real-time analytics.</p> </li> <li> <p>E-commerce Applications:     Manages diverse and evolving product catalogs.     Offers personalized customer experiences through robust data handling.</p> </li> <li> <p>Gaming Industry:     Provides high performance for real-time analytics.     Scales dynamically to handle fluctuating user loads.</p> </li> <li> <p>Real-Time Analytics:     Facilitates real-time data processing and aggregation. Offers quick insights from live data.</p> </li> <li> <p>Catalogs and Inventory Management:     Easily manages complex and varied product data.     Supports fast queries for efficient inventory tracking.</p> </li> <li> <p>Log Data Storage and Analysis:     Stores large volumes of log data for analysis.     Offers time-to-live (TTL) indexes for expiring old logs.</p> </li> <li> <p>Document and Asset Management:     Ideal for storing, retrieving, and managing document-based information. Supports rich document structures and metadata.</p> </li> <li> <p>Social Networks:     Manages dynamic and large-scale user-generated data.     Handles complex friend networks and social graph data efficiently.</p> </li> </ol>"},{"location":"sql/mongoiq/","title":"MongoDB","text":"<p>What is MongoDB?</p> <p>MongoDB is a NoSQL database that stores data in flexible,JSON-like documents, meaning fields can vary from document to document and data structure can be changed over time.</p> <p>What are Collections in MongoDB?</p> <p>Collections in MongoDB are analogous to tables in relational databases and are used to store a set of documents.</p> <p>What is a Document in MongoDB?</p> <p>A document is the basic unit of data in MongoDB and is similar to a JSON object, consisting of field-value pairs.</p> <p>How does MongoDB differ from SQL databases?</p> <p>MongoDB is a NoSQL database that does not require a fixed schema, allows horizontal scaling, and uses a document-based data model, unlike structured, table-based SQL databases.</p> <p>What is the role of _id in MongoDB?</p> <p>The _id field acts as a primary key in MongoDB documents, uniquely identifying each document in a collection.</p> <p>What are Indexes in MongoDB?</p> <p>Indexes support the efficient execution of queries in MongoDB. Without indexes, MongoDB must scan every document in a collection to select those that match the query statement.</p> <p>Can you change an _id field of a document?</p> <p>No, the _id field of a document is immutable and cannot be changed once set.</p> <p>What is a Replica Set in MongoDB?</p> <p>A replica set in MongoDB is a group of mongod instances that maintain the same data set, providing redundancy and high availability.</p> <p>What is Sharding in MongoDB?</p> <p>Sharding is the process of storing data records across multiple machines and is MongoDB\u2019s approach to meeting the demands of data growth.</p> <p>What are Aggregations in MongoDB?</p> <p>Aggregations in MongoDB process data records and return computed results, similar to GROUP BY in SQL. They provide a way to perform complex data processing and transformations.</p> <p>How do you back up a MongoDB database?</p> <p>MongoDB can be backed up using mongodump, a utility for creating binary export of the contents of a database.</p> <p>What is BSON in MongoDB?</p> <p>BSON (Binary JSON) is a binary-encoded serialization of JSON-like documents used by MongoDB.</p> <p>What is a Namespace in MongoDB?</p> <p>A namespace in MongoDB is the concatenation of the database name and the collection name, used to uniquely identify collections across databases.</p> <p>What is Mongoose in the context of MongoDB?</p> <p>Mongoose is an Object Data Modeling (ODM) library for MongoDB and Node.js, managing relationships between data and providing schema validation.</p> <p>How does MongoDB provide concurrency?</p> <p>MongoDB uses reader-writer locks that allow concurrent readers shared access to a resource, such as a database or collection, but give exclusive access to a single write operation.</p> <p>What are some common commands in MongoDB?</p> <p>Common commands include find() for retrieving documents, insert() for adding new documents, update() for modifying existing documents, and delete() for removing documents.</p> <p>What is Journaling in MongoDB?</p> <p>Journaling in MongoDB is used to safeguard data in case of a crash by recording changes before they are written to the database.</p> <p>What is GridFS and when is it used?</p> <p>GridFS is used in MongoDB for storing and retrieving large files like images, audio files, or video files.</p> <p>How do you scale MongoDB?</p> <p>MongoDB can be scaled horizontally by sharding, distributing data across multiple servers, or vertically by adding more resources to the existing machines.</p> <p>What is the default port for MongoDB?</p> <p>The default port for MongoDB is 27017</p> <p>How does MongoDB handle transaction management?</p> <p>MongoDB supports multi-document ACID transactions, similar to relational databases. Transactions in MongoDB can be used to perform a series of read and write operations atomically.</p> <p>Explain the concept of 'upsert' in MongoDB.</p> <p>'Upsert' is a combination of 'update' and 'insert'. If the specified document exists, MongoDB updates it with the new values; if it does not exist, MongoDB inserts it as a new document.</p> <p>What are the differences between embedded documents and references in MongoDB?</p> <p>Embedded documents are stored directly within a parent document, providing fast read access. References are links to documents stored in another collection, requiring an additional query to retrieve but are better for data normalization and avoiding data duplication.</p> <p>How do you ensure data integrity in MongoDB?</p> <p>Data integrity in MongoDB can be ensured through proper schema design, using transactions for complex operations, and implementing validation rules in the database layer.</p> <p>Scenario: How would you design a MongoDB schema for a blogging platform?</p> <p>A blogging platform schema might involve collections for users, posts, and comments. Posts can have embedded comments or reference them. User documents can reference their posts.</p> <p>What is MapReduce in MongoDB and when would you use it?</p> <p>MapReduce is a data processing paradigm in MongoDB used for batch processing of data and aggregation operations. It's useful for large datasets and complex aggregations.</p> <p>How does MongoDB ensure high availability?</p> <p>MongoDB ensures high availability through replica sets, which provide redundancy and automatic failover in case of primary node failure.</p> <p>Can you change the shard key after sharding a collection?</p> <p>No, once a shard key is chosen and sharding is implemented, you cannot change the shard key of a collection.</p> <p>What is a Covered Query in MongoDB?</p> <p>A covered query is a query in which all the fields in the query, including the sort and projection fields, are part of an index. Covered queries can be much faster as they avoid fetching documents.</p> <p>Explain Write Concern in MongoDB.</p> <p>Write concern in MongoDB describes the level of acknowledgment requested from MongoDB for write operations, determining the guarantee of writing data to the database.</p> <p>What is the Aggregation Pipeline in MongoDB?</p> <p>The Aggregation Pipeline is a framework in MongoDB for data aggregation, modeled as a pipeline through which documents pass and are transformed into aggregated results.</p> <p>Scenario: How would you optimize a slow query in MongoDB?</p> <p>To optimize a slow query, first identify the query, examine the execution plan, create appropriate indexes, and consider redesigning the schema for more efficient querying.</p> <p>What is the role of the Profiler in MongoDB?</p> <p>The Profiler in MongoDB is used to monitor database operations, helping in identifying slow queries and performance bottlenecks.</p> <p>How can you achieve pagination in MongoDB queries?</p> <p>Pagination can be achieved using the skip() and limit() methods in MongoDB. However, for large datasets, a range-based pagination using _id or another indexed field is more efficient.</p> <p>Explain the role of the WiredTiger storage engine in MongoDB.</p> <p>WiredTiger, the default storage engine in MongoDB, offers advantages like support for transactions, compression, and cache management, leading to improved performance and storage efficiency.</p> <p>Scenario: How would you handle a scenario where your MongoDB database is hitting memory limits?</p> <p>Addressing memory limits involves optimizing indexes, queries, and schema design; considering sharding for horizontal scaling; and potentially increasing the server's memory capacity.</p> <p>What are TTL Indexes in MongoDB?</p> <p>Time-To-Live (TTL) indexes are used to automatically remove documents from a collection after a certain amount of time, useful for data that only needs to be stored temporarily.</p> <p>What is the difference between $lookup and DBRef in MongoDB?</p> <p>$lookup is an aggregation pipeline stage that lets you join documents from different collections, while DBRef is a convention for representing a reference to another document.</p> <p>Scenario: Describe how you would migrate data from a SQL database to MongoDB.</p> <p>Migrating data involves exporting data from the SQL database, transforming it into a format suitable for MongoDB (like JSON), and then importing it using tools like mongoimport.</p> <p>What is a Capped Collection in MongoDB?</p> <p>Capped collections are fixed-size collections that automatically overwrite their oldest entries when they reach their maximum size. They are ideal for logging and caching purposes.</p> <p>Explain the use of the $facet stage in the aggregation pipeline.</p> <p>The $facet stage allows for performing multiple aggregation pipelines within a single stage. It is useful for creating multi-faceted aggregations that categorize data into different metrics in a single query.</p> <p>How does MongoDB handle large-scale join operations?</p> <p>MongoDB isn't designed for large-scale join operations like traditional RDBMS. However, it can perform left outer joins using the $lookup stage in the aggregation pipeline.</p> <p>What are the limitations of using transactions in MongoDB?</p> <p>Transactions in MongoDB can affect performance due to increased latency and have limitations like a 60-second transaction timeout and increased storage space requirements for the WiredTiger engine.</p> <p>Scenario: Design a MongoDB schema for a real-time analytics dashboard.</p> <p>For a real-time analytics dashboard, a schema that supports fast reads is crucial. This might involve denormalizing data, using pre-aggregated metrics, and optimizing indexes for common queries.</p> <p>Explain the role of oplog in MongoDB Replica Sets.</p> <p>The oplog (operations log) is a special capped collection that keeps a rolling record of all operations that modify the data storedin databases. It's used in replica sets for replication purposes.</p> <p>How does MongoDB ensure data durability?</p> <p>MongoDB ensures data durability through journaling and replication. Journaling writes operations to disk to prevent data loss, and replication ensures data is copied across multiple servers.</p> <p>What are the strategies for handling hotspots in sharded clusters?</p> <p>Handling hotspots involves choosing an effective shard key, ensuring even data distribution, using compound shard keys if necessary, and monitoring and redistributing chunks when needed.</p> <p>Scenario: How would you optimize a sharded MongoDB cluster with uneven shard loads?</p> <p>To optimize an unevenly loaded sharded cluster, analyze the shard key and chunk distribution, use zone sharding for better control, and possibly reshard the data with a more appropriate shard key.</p> <p>Discuss the impact of document size on MongoDB's performance.</p> <p>Larger documents consume more memory and can lead to slower read/write operations. Optimizing document size by avoiding large, complex documents or unnecessary fields can improve performance.</p> <p>How does MongoDB handle network partitions in a sharded environment?</p> <p>In the event of a network partition, MongoDB maintains consistency by ensuring that each shard remains independently consistent. However, the cluster might not be able to achieve overall consistency until the partition is resolved.</p> <p>Explain how the WiredTiger cache works in MongoDB.</p> <p>The WiredTiger cache holds recently read and written data in memory. When the cache becomes full, older or less frequently accessed data is written to disk. Proper cache management is crucial for performance.</p> <p>What are the best practices for securing a MongoDB database?</p> <p>Best practices include enabling authentication, using role-based access control, encrypting data at rest and in transit, regularly updating MongoDB, and securing the underlying server.</p> <p>Scenario: Implement a strategy to handle time-series data in MongoDB.</p> <p>For time-series data, use a schema that groups data into buckets (documents) based on time intervals. This approach optimizes storage and query efficiency for time-based data.</p> <p>Discuss the use of Change Streams in MongoDB.</p> <p>Change streams allow applications to access real-time data changes without polling the database. They are useful for triggering actions, updating caches, and synchronizing data with external systems.</p> <p>Explain how MongoDB\u2019s query planner selects indexes for executing queries.</p> <p>MongoDB's query planner evaluates various query plans using available indexes and chooses the one with the lowest estimated cost based on factors like index selectivity and document counts.</p> <p>What are the considerations for selecting a shard key in MongoDB?</p> <p>A good shard key should provide even distribution of data, support the query patterns of your application, and minimize the need for resharding.</p> <p>Scenario: How do you migrate a large collection in MongoDB with minimal downtime?</p> <p>For minimal downtime, perform the migration in stages: start by syncing data to the new collection or database, redirect read/write traffic, and then finalize the migration.</p> <p>How does MongoDB handle write operations in sharded clusters?</p> <p>Write operations in sharded clusters are directed to the primary shard responsible for the data's shard key. The primary shard then applies the write locally and replicates it to secondary members.</p> <p>What is a Write Amplification in MongoDB and how can it be minimized?</p> <p>Write amplification occurs when more writes are performed than necessary, often due to updates that cause document relocations. It can be minimized by schema design that reduces document growth and by using update operators efficiently.</p> <p>How do you monitor and tune the performance of a MongoDB cluster?</p> <p>Performance can be monitored using tools like MongoDB Atlas, mongostat, and mongotop. Tuning involves analyzing query patterns, optimizing indexes, adjusting server configurations, and ensuring adequate resources.</p>"},{"location":"sql/sql/","title":"SQL","text":""},{"location":"sql/sql/#what-is-a-database","title":"What is a database?","text":"<p>Database is structured, organised set of data.Think of it as a filecabinet whre you store data in different sections called tables.</p>"},{"location":"sql/sql/#types-of-databases","title":"Types of Databases","text":""},{"location":"sql/sql/#what-is-dbms","title":"What is DBMS?","text":"<p>A software which allows users to interact with data. Stores data in structured format. A schema defines the structure of data.</p>"},{"location":"sql/sql/#acid-properties","title":"Acid Properties","text":"<p>Atomicity enforces the \"all or nothing\" principle, ensuring that an entire transaction either completes successfully or is rolled back, preventing partial, inconsistent states. For example, in a financial transfer, if a crash occurs after deducting funds from one account but before adding them to another, an atomic system would detect the failure before the final COMMIT and roll back the entire transaction. </p> <p>Consistency guarantees that a transaction moves the database from one valid state to another, enforcing rules (like checking for sufficient funds) to prevent inconsistent results. </p> <p>Isolation is the \"no interference policy,\" meaning multiple concurrent transactions operate independently without seeing each other's uncommitted changes; Delta achieves this using  Optimistic Concurrency Control (OCC). </p> <p>Durability ensures that once a transaction is committed, it permanently remains recorded in the system, even in the event of crashes or power outages. This is achieved by logging the start of the operation and the final COMMIT status in a transaction log (ledger); if the system restarts and the log shows no COMMIT, the changes are rolled back</p>"},{"location":"sql/sql/#sql-data-types","title":"SQL Data Types","text":""},{"location":"sql/sql/#types-of-commands-in-sql","title":"Types of commands in SQL","text":"<ol> <li>Data Definition language</li> <li>Data Manipulation language</li> <li>Data Query language</li> <li>Data Control language</li> </ol>"},{"location":"sql/sql/#sql-constraints","title":"SQL Constraints","text":"<p>SQL constraints are used to specify rules for the data in a table. Constraints are used to limit the type of data that can go into a table. This ensures the accuracy and reliability of the data in the table. If there is any violation between the constraint and the data action, the action is aborted. Constraints can be column level or table level.</p> <p>The following constraints are commonly used in SQL:</p> <ol> <li>NOT NULL - Ensures that a column cannot have a NULL value</li> <li>UNIQUE - Ensures that all values in a column are different</li> <li>PRIMARY KEY - A combination of a NOT NULL and UNIQUE. Uniquely identifies each row in a table</li> <li>FOREIGN KEY - Prevents actions that would destroy links between tables</li> <li>CHECK - Ensures that the values in a column satisfies a specific condition</li> <li>DEFAULT - Sets a default value for a column if no value is specified</li> </ol> <p>Primary Key</p> <ol> <li>Uniqueness: Each primary key value must be unique per table row.</li> <li>Immutable: Primary keys should not change once set.</li> <li>Simplicity: Ideal to keep primary keys as simple as possible.</li> <li>Non-Intelligent: They shouldn't contain meaningful information.</li> <li>Indexed: Primary keys are automatically indexed for faster data retrieval.</li> <li>Referential Integrity: They serve as the basis for foreign keys in other tables.</li> <li>Data Type: Common types are integer or string.</li> </ol> <p>Foreign Key</p> <ol> <li>Referential Integrity: Foreign keys link records between tables, maintaining data consistency.</li> <li>Nullable: Foreign keys can contain null values unless specifically restricted.</li> <li>Match Primary Keys: Each foreign key value must match a primary key value in the parent table, or be null.</li> <li>Ensure Relationships: They define the relationship between tables in a database.</li> <li>No Uniqueness: Foreign keys don't need to be unique.</li> </ol> Feature DROP TRUNCATE DELETE Purpose Completely removes the entire table structure from the database Removes all rows from a table, but the table structure remains Removes specific rows based on a condition or all rows from a table, but the table structure remains Transaction Control Cannot be rolled back Cannot be rolled back Can be rolled back Space Reclaiming Releases the object and its space Frees the space containing the table Doesn't free up space, but leaves empty space for future use Speed Fastest as it removes all data and structure Faster than DELETE as it doesn't log individual row deletions Slowest as it logs individual row deletions Referential Integrity Not checked Checked Checked Where Clause Not applicable Not applicable Applicable Command Type DDL (Data Definition Language) DDL (Data Definition Language) DML (Data Manipulation Language)"},{"location":"sql/sql/#functions","title":"Functions","text":"<p>Functions in MySQL are reusable blocks of code that perform a specific task and return a single value.</p> <p>Purpose: Simplify complex calculations, enhance code reusability, and improve query performance.</p> <p>Types: Built-in Functions and User-Defined Functions (UDFs).</p> <ol> <li> <p>Built-in Functions</p> <p>a. String Functions (e.g., CONCAT, LENGTH, SUBSTRING)</p> <p>b. Numeric Functions (e.g., ABS, ROUND, CEIL)</p> <p>c. Date and Time Functions (e.g., NOW, DATE_FORMAT, DATEDIFF)</p> <p>d. Aggregate Functions (e.g., COUNT, SUM, AVG)</p> </li> <li> <p>User-Defined Functions (UDFs) </p> </li> </ol> <p>Custom functions created by users to perform specific operations. It is customizable, reusable, and encapsulate complex logic.</p> <pre><code>    DELIMITER $$\n    CREATE FUNCTION function_name(parameter(s))\n    RETURNS data_type\n    DETERMINISTIC\n    BEGIN\n    -- function body\n    RETURN value;\n    END $$\n    DELIMITER ;\n</code></pre>"},{"location":"sql/sql/#joins","title":"JOINS","text":"<p>SQL joins are used to combine rows from two or more tables, based on a related column between them.</p> <p>Here are the main types of SQL joins:</p> <p></p> <p>Inner Join</p> <p>Returns records that have matching values in both tables. </p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount \nFROM Customers\nINNER JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Left Join</p> <p>Returns all records from the left table (table1), and the matched records from the right table(table2). If no match, the result is NULL on the right side. </p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount\nFROM Customers\nLEFT JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Right Join</p> <p>Returns all records from the right table (table2), and the matched records from the left table(table1). If no match, the result is NULL on the left side.</p> <p></p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount\nFROM Customers\nRIGHT JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Full Join</p> <p>Returns all records when there is a match in either left (table1) or right (table2) table records.</p> <p></p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount\nFROM Customers\nFULL OUTER JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Cross Join</p> <p>Returns the Cartesian product of the sets of records from the two or more joined tables when no WHERE clause is used with CROSS JOIN. </p> <p></p> <pre><code>SELECT Model.car_model,\nColor.color_name\nFROM Model\nCross JOIN Color;\n</code></pre> <p>Self Join</p> <p>A regular join, but the table is joined with itself. </p> <p>Now, to show the name of the manager for each employee in the same row, we can run the following query:</p> <pre><code>SELECT\nemployee.Id,\nemployee.FullName,\nemployee.ManagerId,\nmanager.FullName as ManagerName\nFROM Employees employee\nJOIN Employees manager\nON employee.ManagerId = manager.Id;\n</code></pre> <p>Group By WITH ROLLUP</p> <p>The GROUP BY clause in MySQL is used to group rows that have the same values in specified columns into aggregated data. The WITH ROLLUP option allows you to include extra rows that represent subtotals and grand totals.</p> <p></p> <pre><code>SELECT\nSUM(payment_amount),\nYEAR(payment_date) AS 'Payment Year',\nstore_id AS 'Store'\nFROM payment\nGROUP BY YEAR(payment_date), store_id WITH ROLLUP\nORDER BY YEAR(payment_date), store_id;\n</code></pre>"},{"location":"sql/sql/#views","title":"Views","text":"<p>A view in SQL is a virtual table based on the result-set of an SQL statement. It contains rows and columns, just like a real table. The fields in a view are fields from one or more real tables in the database.</p> <p>Here are some key points about views:</p> <ol> <li>You can add SQL functions, WHERE, and JOIN statements to a view and display the data as if the data were coming from one single table.</li> <li>A view always shows up-to-date data. The database engine recreates the data every time a user queries a view.</li> <li>Views can be used to encapsulate complex queries, presenting users with a simpler interface to the data.</li> <li>They can be used to restrict access to sensitive data in the underlying tables, presenting only non-sensitive data to users.</li> </ol> <p></p> <pre><code>CREATE VIEW View_Products AS SELECT ProductName, Price FROM Products\nWHERE Price &gt; 30;\n</code></pre>"},{"location":"sql/sql/#window-functions","title":"Window Functions","text":"<ol> <li>Window functions: These are special SQL functions that perform a calculation across a set of related rows.</li> <li>How it works: Instead of operating on individual rows, a window function operates on a group or 'window' of rows that are somehow related to the current row. This allows for complex calculations based on these related rows.</li> <li>Window definition: The 'window' in window functions refers to a set of rows. The window can be defined using different criteria depending on the requirements of your operation.</li> <li>Partitions: By using the PARTITION BY clause, you can divide your data into smaller sets or 'partitions'. The window function will then be applied individually to each partition.</li> <li>Order of rows: You can specify the order of rows in each partition using the ORDER BY clause. This order influences how some window functions calculate their result.</li> <li>Frames: The ROWS/RANGE clause lets you further narrow down the window by defining a 'frame' or subset of rows within each partition.</li> <li>Comparison with Aggregate Functions: Unlike aggregate functions that return a single result per group, window functions return a single result for each row of the table based on the group of rows defined in the window.</li> <li>Advantage: Window functions allow for more complex operations that need to take into account not just the current row, but also its 'neighbours' in some way.</li> </ol>"},{"location":"sql/sql/#syntax","title":"syntax","text":"<pre><code>function_name (column) OVER (\n[PARTITION BY column_name_1, ..., column_name_n]\n[ORDER BY column_name_1 [ASC | DESC], ..., column_name_n [ASC | DESC]]\n)\n</code></pre> <ol> <li>function_name: This is the window function you want to use. Examples include ROW_NUMBER(), RANK(), DENSE_RANK(), SUM(), AVG(), and many others.</li> <li>(column): This is the column that the window function will operate on. For some functions like SUM(salary)</li> <li>OVER (): This is where you define the window. The parentheses after OVER contain the specifications for the window.</li> <li>PARTITION BY column_name_1, ..., column_name_n: This clause divides the result set into partitions upon which the window function will operate independently. For example, if you have PARTITION BY salesperson_id, the window function will calculate a result for each salesperson independently.</li> <li>ORDER BY column_name_1 [ASC | DESC], ..., column_name_n [ASC | DESC]: This clause specifies the order of the rows in each partition. The window function operates on these rows in the order specified. For example, ORDERBY sales_date DESC will make the window function operate on rows with more recent dates first.</li> </ol>"},{"location":"sql/sql/#types-of-windows-function","title":"Types of Windows Function","text":"<p>There are three main categories of window functions in SQL: Ranking functions, Value functions, and Aggregate functions. Here's a brief description and example for each:</p>"},{"location":"sql/sql/#ranking-functions","title":"Ranking Functions:","text":"<ol> <li>ROW_NUMBER(): Assigns a unique row number to each row, ranking start from 1 and keep increasing till the end of last row</li> </ol> <pre><code>SELECT Studentname,\nSubject,\nMarks,\nROW_NUMBER() OVER(ORDER BY Marks desc)\nRowNumber\nFROM ExamResult;\n</code></pre> <ol> <li>RANK(): Assigns a rank to each row. Rows with equal values receive the same rank, with the next row receiving a rank which skips the duplicate rankings.</li> </ol> <pre><code>SELECT Studentname,\nSubject,\nMarks,\nRANK() OVER(ORDER BY Marks DESC) Rank\nFROM ExamResult\nORDER BY Rank;\n</code></pre> <ol> <li>DENSE_RANK(): Similar to RANK(), but does not skip rankings if there are duplicates.</li> </ol> <pre><code>SELECT Studentname,\nSubject,\nMarks,\nDENSE_RANK() OVER(ORDER BY Marks DESC) Rank\nFROM ExamResult\n</code></pre>"},{"location":"sql/sql/#value-functions","title":"Value Functions:","text":"<p>These functions perform calculations on the values of the window rows.</p> <ol> <li>FIRST_VALUE(): Returns the first value in the window.</li> </ol> <p></p> <pre><code>SELECT\nemployee_name,\ndepartment,\nhours,\nFIRST_VALUE(employee_name) OVER (\nPARTITION BY department\nORDER BY hours\n) least_over_time\nFROM\novertime;\n</code></pre> <ol> <li>LAST_VALUE(): Returns the last value in the window.</li> </ol> <p></p> <pre><code>SELECT employee_name, department,salary,\nLAST_VALUE(employee_name)\nOVER (\nPARTITION BY department ORDER BY\nsalary\n) as max_salary\nFROM Employee;\n</code></pre> <ol> <li>LAG(): Returns the value of the previous row.</li> </ol> <p></p> <pre><code>SELECT\nYear,\nQuarter,\nSales,\nLAG(Sales, 1, 0) OVER(\nPARTITION BY Year\nORDER BY Year,Quarter ASC)\nAS NextQuarterSales\nFROM ProductSales;\n</code></pre> <ol> <li>LEAD(): Returns the value of the next row.</li> </ol> <p></p> <pre><code>SELECT Year,\nQuarter,\nSales,\nLEAD(Sales, 1, 0) OVER(\nPARTITION BY Year\nORDER BY Year,Quarter ASC)\nAS NextQuarterSales\nFROM ProductSales;\n</code></pre>"},{"location":"sql/sql/#aggregation-functions","title":"Aggregation Functions:","text":"<p>These functions perform calculations on the values of the window rows.</p> <ol> <li>SUM()</li> <li>MIN()</li> <li>MAX()</li> <li>AVG()</li> </ol>"},{"location":"sql/sql/#frame-clause","title":"Frame Clause","text":"<p>The frame clause in window functions defines the subset of rows ('frame') used for calculating the result of the function for the current row.</p> <p>It's specified within the OVER() clause after PARTITION BY and ORDER BY.</p> <p>The frame is defined by two parts: a start and an end, each relative to the current row.</p> <p>Generic syntax for a window function with a frame clause:     function_name (expression) OVER (     [PARTITION BY column_name_1, ..., column_name_n]     [ORDER BY column_name_1 [ASC | DESC], ..., column_name_n [ASC | DESC]]     [ROWS|RANGE frame_start TO frame_end]     )</p> <p>The frame start can be:  1. UNBOUNDED PRECEDING (starts at the first row of the partition)  2. N PRECEDING (starts N rows before the current row)  3. CURRENT ROW (starts at the current row)</p> <p>The frame end can be:  1. UNBOUNDED FOLLOWING (ends at the last row of the partition)  2. N FOLLOWING (ends N rows after the current row)  3. CURRENT ROW (ends at the current row)</p> <p>For ROWS, the frame consists of N rows coming before or after the current row.</p> <p>For RANGE, the frame consists of rows within a certain value range relative to the value in the current row.</p> <p></p> <p>ROWS BETWEEN Example:</p> <p></p> <pre><code>SELECT date, revenue,\nSUM(revenue) OVER (\nORDER BY date\nROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) running_total\nFROM sales\nORDER BY date;\n</code></pre> <p>RANGE BETWEEN Example:</p> <p></p> <pre><code>SELECT\nshop,\ndate,\nrevenue_amount,\nMAX(revenue_amount) OVER (\nORDER BY DATE\nRANGE BETWEEN INTERVAL '3' DAY PRECEDING\nAND INTERVAL '1' DAY FOLLOWING\n) AS max_revenue\nFROM revenue_per_shop;\n</code></pre>"},{"location":"sql/sql/#common-table-expression","title":"Common Table Expression","text":"<p>A Common Table Expression (CTE) in SQL is a named temporary result set that exists only within the execution scope of a single SQL statement. Here are some important points to note about CTEs:</p> <p>CTEs can be thought of as alternatives to derived tables, inline views, or subqueries.</p> <p>They can be used in SELECT, INSERT, UPDATE, or DELETE statements.</p> <p>CTEs help to simplify complex queries, particularly those involving multiple subqueries or recursive queries.</p> <p>They make your query more readable and easier to maintain.</p> <p>A CTE is defined using the WITH keyword, followed by the CTE name and a query. The CTE can then be referred to by its name elsewhere in the query.</p> <p>Here's a basic example of a CTE:     WITH sales_cte AS (     SELECT sales_person, SUM(sales_amount) as total_sales     FROM sales_table     GROUP BY sales_person     )     SELECT sales_person, total_sales     FROM sales_cte     WHERE total_sales &gt; 1000;</p> <p>Recursive CTE: </p> <p>This is a CTE that references itself. In other words, the CTE query definition refers back to the CTE name, creating a loop that ends when a certain condition is met. Recursive CTEs are useful for working with hierarchical or tree-structured data.</p> <p></p> <p>Example: </p> <pre><code>WITH RECURSIVE number_sequence AS (\nSELECT 1 AS number\nUNION ALL\nSELECT number + 1\nFROM number_sequence\nWHERE number &lt; 10\n)\nSELECT * FROM number_sequence;\n</code></pre>"},{"location":"sql/sql/#indexing","title":"Indexing","text":"<p>Indexing in databases involves creating a data structure that improves the speed of data retrieval operations on a database table.</p> <p>Indexes are used to quickly locate data without having to search every row in a table each time a database table is accessed.</p> <p></p>"},{"location":"sql/sql/#why-is-indexing-important","title":"Why is Indexing Important?","text":"<p>Indexes are crucial for enhancing the performance of a database by:</p> <ol> <li>Speeding up Query Execution: Indexes reduce the amount of data that needs to be scanned for a query, significantly speeding up data retrieval operations.</li> <li>Optimizing Search Operations: Indexes help in efficiently searching for records based on the indexed columns.</li> <li>Improving Sorting and Filtering: Indexes assist in sorting and filtering operations by providing a structured way to access data.</li> <li>Enhancing Join Performance: Indexes on join columns improve the performance of join operations between tables.</li> </ol>"},{"location":"sql/sql/#advantages-of-indexing","title":"Advantages of Indexing","text":"<ol> <li>Faster Data Retrieval: Indexes make search queries faster by providing a quick way to locate rows in a table.</li> <li>Efficient Use of Resources: Reduced query execution time translates to more efficient use of CPU and memory resources.</li> <li>Improved Performance for Large Tables: Indexes are particularly beneficial for large tables where full table scans would be time-consuming.</li> <li>Better Sorting and Filtering: Indexes can improve the performance of ORDER BY, GROUP BY, and WHERE clauses.</li> </ol>"},{"location":"sql/sql/#how-to-choose-the-right-indexing-column","title":"How to Choose the Right Indexing Column","text":"<ol> <li>Primary Key and Unique Constraints: Always index columns that are primary keys or have unique constraints, as they uniquely identify rows.</li> <li>Frequently Used Columns in WHERE Clauses: Index columns that are frequently used in WHERE clauses to filter data.</li> <li>Columns Used in Joins: Index columns that are used in join conditions to speed up join operations.</li> <li>Columns Used in ORDER BY and GROUP BY: Index columns that are used in ORDER BY and GROUP BY clauses for faster sorting and grouping.</li> <li>Selectivity of the Column: Choose columns with high selectivity (columns with many unique values) to maximize the performance benefits of the index.</li> </ol>"},{"location":"sql/sql/#query-optimizations","title":"Query Optimizations","text":"<ol> <li>Use Column Names Instead of * in a SELECT Statement</li> </ol> <p>Avoid including a HAVING clause in SELECT statements</p> <p>The HAVING clause is used to filter the rows after all the rows are selected and it is used like a filter. It is quite useless in a SELECT statement. It works by going through the final result table of the query parsing out the rows that don\u2019t meet the HAVING condition.</p> <p>Example:</p> <pre><code>Original query:\nSELECT s.cust_id,count(s.cust_id)\nFROM SH.sales s\nGROUP BY s.cust_id\nHAVING s.cust_id != '1660' AND s.cust_id != '2';\n\nImproved query:\nSELECT s.cust_id,count(cust_id)\nFROM SH.sales s\nWHERE s.cust_id != '1660'\nAND s.cust_id !='2'\nGROUP BY s.cust_id;\n</code></pre> <ol> <li>Eliminate Unnecessary DISTINCT Conditions</li> </ol> <p>Considering the case of the following example, the DISTINCT keyword in the original query is unnecessary because the table_name contains the primary key p.ID, which is part of the result set.</p> <p>Example:</p> <pre><code>Original query:\nSELECT DISTINCT * FROM SH.sales s\nJOIN SH.customers c\nON s.cust_id= c.cust_id\nWHERE c.cust_marital_status = 'single';\n\nImproved query:\nSELECT * FROM SH.sales s JOIN\nSH.customers c\nON s.cust_id = c.cust_id\nWHERE c.cust_marital_status='single';\n</code></pre> <ol> <li>Consider using an IN predicate when querying an indexed column</li> </ol> <p>The IN-list predicate can be exploited for indexed retrieval and also, the optimizer can sort the IN-list to match the sort sequence of the index, leading to more efficient retrieval. Example:</p> <pre><code>Original query:\nSELECT s.*\nFROM SH.sales s\nWHERE s.prod_id = 14\nOR s.prod_id = 17;\n\nImproved query:\nSELECT s.*\nFROM SH.sales s\nWHERE s.prod_id IN (14, 17);\n</code></pre> <ol> <li>Try to use UNION ALL in place of UNION</li> </ol> <p>The UNION ALL statement is faster than UNION, because UNION ALL statement does not consider duplicate s, and UNION statement does look for duplicates in a table while selection of rows, whether or not they exist. Example:</p> <pre><code>Original query:\nSELECT cust_id\nFROM SH.sales\nUNION\nSELECT cust_id\nFROM customers;\n\nImproved query:\nSELECT cust_id\nFROM SH.sales\nUNION ALL\nSELECT cust_id\nFROM customers;\n</code></pre>"},{"location":"sql/subq/","title":"Subq","text":""},{"location":"sql/subq/#subqueries-in-sql","title":"Subqueries in SQL","text":"<p>IN: The IN operator allows you to specify multiple values in a WHERE clause. It returns true if a value matches any value in a list.</p> <pre><code>SELECT * FROM Orders WHERE ProductName IN ('Apple', 'Banana');\n</code></pre> <p>NOT IN: The NOT IN operator excludes the values in the list. It returns true if a value does not match any value in the list.</p> <pre><code>SELECT * FROM Orders WHERE ProductName NOT IN ('Apple', 'Banana');\n</code></pre> <p>ANY: The ANY operator returns true if any subquery value meets the condition.</p> <p>ALL: The ALL operator returns true if all subquery value meets the condition.</p> <p>EXISTS: The EXISTS operator returns true if the subquery returns one or more records.</p> <p>NOT EXISTS: The NOT EXISTS operator returns true if the subquery returns no records.</p>"}]}